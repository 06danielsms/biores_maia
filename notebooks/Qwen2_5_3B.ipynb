{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GsP7Fazij3fl",
        "outputId": "937d3c13-b236-4fdd-9f99-e5e632bb56e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.43.3\n",
            "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.11.1\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting accelerate==0.30.1\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (0.7.0)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.43.3)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.3) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.43.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.43.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.43.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.43.3) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (3.0.3)\n",
            "Downloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m364.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m292.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m200.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, accelerate, peft\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.11.0\n",
            "    Uninstalling accelerate-1.11.0:\n",
            "      Successfully uninstalled accelerate-1.11.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.18.0\n",
            "    Uninstalling peft-0.18.0:\n",
            "      Successfully uninstalled peft-0.18.0\n",
            "Successfully installed accelerate-0.30.1 peft-0.11.1 tokenizers-0.19.1 transformers-4.43.3\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.11-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.43.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.19.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textstat-0.7.11-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m375.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat, bert-score\n",
            "Successfully installed bert-score-0.3.13 pyphen-0.17.2 textstat-0.7.11\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting alignscore-SpeedOfMagic\n",
            "  Downloading alignscore_speedofmagic-0.1.4-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (4.0.0)\n",
            "Collecting jsonlines<3,>=2.0.0 (from alignscore-SpeedOfMagic)\n",
            "  Downloading jsonlines-2.0.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting numpy<2,>=1.23.1 (from alignscore-SpeedOfMagic)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m231.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<=3.20.3 (from alignscore-SpeedOfMagic)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting pytorch-lightning>=1.7.7 (from alignscore-SpeedOfMagic)\n",
            "  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (1.6.1)\n",
            "Requirement already satisfied: scipy<2,>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (1.16.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (2.19.0)\n",
            "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm<5,>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (4.67.1)\n",
            "Requirement already satisfied: transformers<5,>=4.20.1 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (4.43.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning>=1.7.7->alignscore-SpeedOfMagic)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=1.7.7->alignscore-SpeedOfMagic)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2,>=1.1.2->alignscore-SpeedOfMagic) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (3.10)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (3.1.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (3.5.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.20.1->alignscore-SpeedOfMagic) (0.7.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.20.1->alignscore-SpeedOfMagic) (0.19.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (1.2.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12.1->alignscore-SpeedOfMagic) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.3.2->alignscore-SpeedOfMagic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.3.2->alignscore-SpeedOfMagic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.3.2->alignscore-SpeedOfMagic) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (1.22.0)\n",
            "Downloading alignscore_speedofmagic-0.1.4-py3-none-any.whl (18 kB)\n",
            "Downloading jsonlines-2.0.0-py3-none-any.whl (6.3 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m349.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m409.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m407.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, lightning-utilities, jsonlines, torchmetrics, pytorch-lightning, alignscore-SpeedOfMagic\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alignscore-SpeedOfMagic-0.1.4 jsonlines-2.0.0 lightning-utilities-0.15.2 numpy-1.26.4 protobuf-3.20.3 pytorch-lightning-2.5.6 torchmetrics-1.8.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "95993fefecee4825ae42ecfbd23d092d",
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m\u2714 Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m\u26a0 Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip uninstall -y transformers tokenizers -q\n",
        "\n",
        "!pip install --no-cache-dir \"transformers==4.43.3\" \"peft==0.11.1\" \"accelerate==0.30.1\"\n",
        "!pip install --no-cache-dir datasets bert-score textstat\n",
        "!pip install --no-cache-dir sentencepiece\n",
        "!pip install --no-cache-dir alignscore-SpeedOfMagic spacy nltk\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7HXjS1tvzUH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "os.environ[\"BITSANDBYTES_DISABLE\"] = \"1\"\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ewZfFeBunZj",
        "outputId": "8b2ecc68-c20e-4180-e42e-2484d9613cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos extra\u00eddos en: /content/training_data\n",
            "content\n",
            "  - training_data.zip\n",
            "  .config\n",
            "    - .last_opt_in_prompt.yaml\n",
            "    - .last_survey_prompt.yaml\n",
            "    - .last_update_check.json\n",
            "    - hidden_gcloud_config_universe_descriptor_data_cache_configs.db\n",
            "    - config_sentinel\n",
            "    - active_config\n",
            "    - gce\n",
            "    - default_configs.db\n",
            "    configurations\n",
            "      - config_default\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "from bert_score import score as bert_score\n",
        "import textstat\n",
        "from alignscore import AlignScore\n",
        "\n",
        "\n",
        "ZIP_PATH = \"/content/training_data.zip\"\n",
        "ROOT_DATA_DIR = \"/content/training_data\"\n",
        "TRAINING_DATA_PATH = \"/content/training_data/training_data\"\n",
        "\n",
        "BASE_MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "ADAPTER_DIR = \"/content/qwen2.5-3b-pls\"\n",
        "\n",
        "os.makedirs(ROOT_DATA_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "        z.extractall(ROOT_DATA_DIR)\n",
        "    print(\"Datos extra\u00eddos en:\", ROOT_DATA_DIR)\n",
        "else:\n",
        "    print(\"No encontr\u00e9 training_data.zip\")\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content\"):\n",
        "    level = root.replace(\"/content\", \"\").count(os.sep)\n",
        "    indent = \" \" * (2 * level)\n",
        "    print(f\"{indent}{os.path.basename(root)}\")\n",
        "    for f in files:\n",
        "        print(f\"{indent}  - {f}\")\n",
        "    if level >= 2:\n",
        "        break\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUgtpu6UkRzT",
        "outputId": "0d023f45-f00d-4c00-f80d-be1fb3d9f709"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== DEBUG RUTA DE DATOS ===\n",
            "training_data_path: /content/training_data/training_data\n",
            "Contenido de la carpeta: ['no_pls_clean.parquet', 'main_train.parquet', 'augmented_test.parquet', 'augmented_train.parquet', 'main_test.parquet', 'pls_clean.parquet']\n",
            "\n",
            "Leyendo archivos y agrupando por doc_id normalizado...\n",
            " - [OK] main_train.parquet: 7408 filas\n",
            " - [OK] main_test.parquet: 1851 filas\n",
            " - [OK] augmented_train.parquet: 16469 filas\n",
            " - [OK] augmented_test.parquet: 4120 filas\n",
            " - [OK] no_pls_clean.parquet: 8401 filas\n",
            " - [OK] pls_clean.parquet: 6778 filas\n",
            "\n",
            "Total de doc_id (normalizados) con al menos 1 medical y 1 plain: 5688\n",
            "Total de pares construidos: 5688\n",
            "Split:\n",
            " - Train: 3981\n",
            " - Val:   853\n",
            " - Test:  854\n",
            "\n",
            "Total de pares preparados para RL / LoRA: 5688\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class PromptPair:\n",
        "    medical: str\n",
        "    plain: str\n",
        "    context: str | None = None\n",
        "    pair_id: str | None = None\n",
        "    flesch_score: float | None = None\n",
        "\n",
        "\n",
        "def normalize_doc_id(doc_id: str) -> str:\n",
        "    s = str(doc_id)\n",
        "    if \"_\" in s:\n",
        "        s = s.split(\"_\")[00]\n",
        "    s = s.replace(\"-abstract\", \"\").replace(\"-pls\", \"\")\n",
        "    return s\n",
        "\n",
        "\n",
        "def load_cochrane_pairs(training_data_path: str | Path):\n",
        "    training_data_path = Path(training_data_path)\n",
        "\n",
        "    print(\"\\n=== DEBUG RUTA DE DATOS ===\")\n",
        "    print(\"training_data_path:\", training_data_path)\n",
        "    if training_data_path.exists():\n",
        "        print(\"Contenido de la carpeta:\", os.listdir(training_data_path))\n",
        "    else:\n",
        "        print(\"Ruta no existe\")\n",
        "\n",
        "    parquet_files = [\n",
        "        \"main_train.parquet\",\n",
        "        \"main_test.parquet\",\n",
        "        \"augmented_train.parquet\",\n",
        "        \"augmented_test.parquet\",\n",
        "        \"no_pls_clean.parquet\",\n",
        "        \"pls_clean.parquet\",\n",
        "    ]\n",
        "\n",
        "    all_rows = []\n",
        "\n",
        "    print(\"\\nLeyendo archivos y agrupando por doc_id normalizado...\")\n",
        "    for fname in parquet_files:\n",
        "        fp = training_data_path / fname\n",
        "        if not fp.exists():\n",
        "            print(f\" - [OMITIDO] {fname} (no existe en {training_data_path})\")\n",
        "            continue\n",
        "\n",
        "        df = pd.read_parquet(fp)\n",
        "        print(f\" - [OK] {fname}: {df.shape[0]} filas\")\n",
        "        for col in [\"doc_id\", \"text\", \"label\"]:\n",
        "            if col not in df.columns:\n",
        "                raise ValueError(f\"{fname} no tiene columna {col}\")\n",
        "        df = df[[\"doc_id\", \"text\", \"label\"]].copy()\n",
        "        df[\"doc_id_norm\"] = df[\"doc_id\"].apply(normalize_doc_id)\n",
        "        all_rows.append(df)\n",
        "\n",
        "    if not all_rows:\n",
        "        raise RuntimeError(\"No se encontraron datos parquet v\u00e1lidos en la ruta.\")\n",
        "\n",
        "    full_df = pd.concat(all_rows, ignore_index=True)\n",
        "\n",
        "    grouped = full_df.groupby(\"doc_id_norm\")\n",
        "\n",
        "    pairs: list[PromptPair] = []\n",
        "    for doc_id_norm, g in grouped:\n",
        "        med_candidates = g[g[\"label\"] == \"no_pls\"]\n",
        "        plain_candidates = g[g[\"label\"] == \"pls\"]\n",
        "        if med_candidates.empty or plain_candidates.empty:\n",
        "            continue\n",
        "\n",
        "        med_row = med_candidates.loc[med_candidates[\"text\"].str.len().idxmax()]\n",
        "        plain_row = plain_candidates.loc[plain_candidates[\"text\"].str.len().idxmax()]\n",
        "\n",
        "        pairs.append(\n",
        "            PromptPair(\n",
        "                medical=str(med_row[\"text\"]),\n",
        "                plain=str(plain_row[\"text\"]),\n",
        "                context=None,\n",
        "                pair_id=doc_id_norm,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    print(f\"\\nTotal de doc_id (normalizados) con al menos 1 medical y 1 plain: {len(pairs)}\")\n",
        "\n",
        "    import random\n",
        "    random.seed(42)\n",
        "    random.shuffle(pairs)\n",
        "\n",
        "    n_total = len(pairs)\n",
        "    n_train = int(n_total * 0.70)\n",
        "    n_val   = int(n_total * 0.15)\n",
        "    n_test  = n_total - n_train - n_val\n",
        "\n",
        "    train_pairs = pairs[:n_train]\n",
        "    val_pairs   = pairs[n_train:n_train + n_val]\n",
        "    eval_pairs  = pairs[n_train + n_val:]\n",
        "\n",
        "    print(f\"Total de pares construidos: {n_total}\")\n",
        "    print(\"Split:\")\n",
        "    print(f\" - Train: {len(train_pairs)}\")\n",
        "    print(f\" - Val:   {len(val_pairs)}\")\n",
        "    print(f\" - Test:  {len(eval_pairs)}\")\n",
        "\n",
        "    return train_pairs, val_pairs, eval_pairs\n",
        "\n",
        "\n",
        "train_pairs, val_pairs, eval_pairs = load_cochrane_pairs(TRAINING_DATA_PATH)\n",
        "all_pairs = train_pairs + val_pairs + eval_pairs\n",
        "print(f\"\\nTotal de pares preparados para RL / LoRA: {len(all_pairs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_EWSXNYvB_o"
      },
      "outputs": [],
      "source": [
        "INSTRUCTION = (\n",
        "    \"You are a specialist in healthcare communication. \"\n",
        "    \"Use the context to transform the following medical text into a clear, concise, \"\n",
        "    \"and easy-to-understand summary for a patient and their family. \"\n",
        "    \"Retain all relevant clinical data, but explain technical terms using simple language and short sentences.\\n\\n\"\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PLSDataset(Dataset):\n",
        "    def __init__(self, pairs, tokenizer, max_length=2048):\n",
        "        \"\"\"\n",
        "        pairs: lista de PromptPair (medical, plain, ...)\n",
        "        tokenizer: tokenizer del modelo (Qwen 2.5-3B)\n",
        "        max_length: longitud m\u00e1xima del input\n",
        "        \"\"\"\n",
        "        self.pairs = pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "\n",
        "        # Prompt de instrucci\u00f3n\n",
        "        prompt = (\n",
        "            \"You are a specialist in healthcare communication. \"\n",
        "            \"Use the context to transform the following medical text into a clear, concise, \"\n",
        "            \"and easy-to-understand summary for a patient and their family. \"\n",
        "            \"Retain all relevant clinical data, but explain technical terms using simple \"\n",
        "            \"language and short sentences.\\n\\n\"\n",
        "            \"### Medical text:\\n\"\n",
        "            f\"{pair.medical}\\n\\n\"\n",
        "            \"### Simplified summary:\\n\"\n",
        "            f\"{pair.plain}\"\n",
        "        )\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            prompt,\n",
        "            max_length=self.max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        input_ids = enc[\"input_ids\"][0]\n",
        "        attention_mask = enc[\"attention_mask\"][0]\n",
        "\n",
        "        # Etiquetas = input_ids\n",
        "        labels = input_ids.clone()\n",
        "        labels[attention_mask == 0] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456,
          "referenced_widgets": [
            "468d15dd8432418395533fcf4353c302",
            "1376fa08ebb84bd6b21829ba3c9bff08",
            "8cb7150a05d941179816b14dcc34cbe5",
            "75be2079bdb74e6a91d1e0c6e2c2539a",
            "d5136afca3c145e2a076809ab5f618cd",
            "792a91b23f7d494591d6ec3cca90f47b",
            "954ba5fe868c4ce796aad339ffd0ece1",
            "59c089bae3b3408c8e4ad0582c4cfb81",
            "6a92d25ef4524309974775bcd0cd1799",
            "4b6c8ced78fc47d7b0d02d98cbc5f299",
            "fcfc8b5e39244a0f8251a6dc0cbe241f",
            "0f9ffc00babf4fae8178d371f7c31067",
            "944847b42677469387c7da9d8d525793",
            "09c88a77325c413d8a033d1d10a9c5aa",
            "8642d211d927490e8bfe447f0028e060",
            "779a36cba8c64b3eb4a41f56796c74e6",
            "f69a41f141f04ebc95be2ccd14422c29",
            "e039eb9330f2490c996635e87b03a9c8",
            "84ee119bc6424c1cb376991f2b008bb0",
            "e8c198649b374233aa21db6a62689cbf",
            "57d78ea87a8843df853e2c749c6c33b5",
            "20d0b88bb1d24d2792dc7528cc03061a",
            "35208873ab0c418b9e215d9fd1b9f8fe",
            "7c1dbeb8afff425f884e1477616f8b70",
            "c6219f73b78546d597baeb4c1bac0497",
            "ff98c36a3b6d4fb3ad211aff61ed5686",
            "d8407ad312b94915874eb11bf3546bf9",
            "5b4def119f854fe79f041fb66033d622",
            "659b376885a1492d844b2bd5d34fd127",
            "a50cea71cbb0419cbbdf888b84087839",
            "c90337189a674c2baaf6077826611dfb",
            "58b5709a37cd49278f918e2b45c883a7",
            "6fb6614387454f8491caf9ffb25ffaef",
            "a19a52a8f1c543828723b3b18e46dd0b",
            "60e290756ffe49f8a64da614ae49e308",
            "a4256050477a4e239aee981c2906a0ad",
            "9fc0186b4436424abbc9200685b37d59",
            "060630d69199411fbcd42f7e339177dd",
            "eb479bbdc8654214b8d1225ea26e2e4d",
            "b88a1019fbc840079461aecde19f9fe6",
            "52b359ada41e4326b67f21de5144aabb",
            "83ecc016a3c8421189b602d5b2c34b3a",
            "bd9f6e4fe8fd4849880ca86c9f9c6b3d",
            "09083042ab8648a8903f91e17e5db847",
            "a2333bb0816645b58661bd7520e512b3",
            "b31e56bea8184d19a285b55a43fdc898",
            "188125db2e534b4190c9f4b94dca83c5",
            "a84ae136e2dd4aa48b4591ac31194a55",
            "050cb9af551146eeba198d542350bb0f",
            "ab799586cc604d8d8e4cee06b5e276c0",
            "6d7ccd666c0e41d3bd9bbeb3872e5d65",
            "8ef1be039efa4ae193f2aba4eea69ff7",
            "beb7bed3cbdc4fb8a8ba608aabebc848",
            "4b5e70cf9001473fbd36c4dae1e1855b",
            "7efc40328e764bd4a96855da60082614",
            "a3ae2595ce0e42289c9cc1a626aa2d3c",
            "44b9e968feb34f4da511eeb6ccaaceab",
            "a76081de5fbe4f70b245658ae77fe977",
            "0739e74e4b7f4e29bdd69231a1057fa2",
            "f12a731d157b4882aeb8a491a0c6b0cd",
            "469851d88d2843f0b4bf41ea307c3221",
            "0a154c941d5348efa3da6182549d5047",
            "3654d21b8224455a9519ae050964b592",
            "6e4e3395e06a469a8349d102b6d86a71",
            "6f7d115b113848f4bd2b9230650923c1",
            "46400371c3e2486782b9a6710f81e2e9",
            "ed0e35d14194476cbfbe677d01a84123",
            "b35c7a7ec4c2405594d8143cca95a196",
            "4bad6528fd2c457bbef80685385f9a74",
            "1a6b48badd5c489e959aca0460668082",
            "30bd5377744c472d890bf4f3fde9a4d7",
            "922eda5b18af4cb2ad8a4fd0646aa0ce",
            "f3506ccb89924e58b03e0abbb1767b31",
            "ea46f060fdee481698088da6fb239d14",
            "dcf48e57405448dcba184a40314973b6",
            "ebf836b0cf0344d394f199422c2cccb0",
            "e49799933fff4585960c90c9aca51348",
            "9ea0169bc796492db8048bcc913b63e4",
            "1d52badeafd04ff7a52ee11a6d4874b3",
            "ea62e7c4c5d94a5aa493711c1a4af337",
            "c9e8c8bf9dde42d2bcb68cd20f98cbea",
            "db133efee17046afb7c072995f83325f",
            "09144ae49c194dbd91979d63c7d11fa3",
            "acadbfa1ca8e4e89b72261a69279f5d8",
            "7d61fe04b37541f69ddd5d7a48a55623",
            "59f3dd0218f547ed801349af30615fbe",
            "eff9d46a8c2d412683ac5d48ed9f011e",
            "8f7c4791a6304d1087fa839dc9d0b786",
            "3dff29714de9457dbb43135b4d756146",
            "f396855b328748eab75001775ee3597a",
            "4c30a7e544cd4072b8b172be4a0debef",
            "6678138313a84e45bf8bb67e96b30031",
            "219e64b60026470299e921d215e872c5",
            "cc07100307f14670aded2e66efc89cd3",
            "f8bb3824b2ab4ef8884fac79b01e0bb6",
            "50dffbaaa8ec4a4888d74b920d6d84df",
            "4f054620618e49e89010170728b76e1a",
            "bf1763d5fd284a019243723688211cfd",
            "c3492d843ffb42c4ba9ca769eb11ba6d",
            "6bbdd484e4ac4aa5b3db82c18b1ec892",
            "a0935c9d15274436addcd8d712548aff",
            "6eda16a4a2a54a1f91070136d7e9c377",
            "ac0dbe009d3a4a9c916b928f6213c84f",
            "945e69b1db1b467f9bf7a31b4add360e",
            "4937860d64d848189018d0743eb9558a",
            "6885be5935fa4baa9045fdb9ddd5be3e",
            "7a94e7f4156a43ce813153e954bcf7d9",
            "b723fae11af84fd2948bc4effcc9349c",
            "aae3fd2a8437425d84f3d18c2ccb43cb",
            "60b3cae3800d45a9babb37ba43a0dcb2",
            "b9acad05a4db43458f2e26c57bafa217",
            "d17bf1fef6b24f319ad8baf5b7ea9047",
            "6b4a9ab8de9a4880acacd892621bb326",
            "5201011f58674f4a9d4e6d05b0c93c8a",
            "608094e476ee46c3a9e6992a496de3ad",
            "fa3146be457140cb99fc3f3a428c5424",
            "0305a4441ff8402fb9d4983d1f2055e5",
            "55b2648040ff46ae97643073f9b1b9e2",
            "4a5286d1c5644efbbe58ff11110495e8",
            "dc6b08ccd3214192a02fe4ee67a121ab",
            "48790b3bbfad419bbc1f61ee5f26d298"
          ]
        },
        "id": "8H0fDx7tGnTo",
        "outputId": "0bffa61f-e6ed-4c27-db6f-365eabb77ad5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CARGANDO MODELO BASE Qwen/Qwen2.5-3B-Instruct\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "468d15dd8432418395533fcf4353c302",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f9ffc00babf4fae8178d371f7c31067",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35208873ab0c418b9e215d9fd1b9f8fe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a19a52a8f1c543828723b3b18e46dd0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a2333bb0816645b58661bd7520e512b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3ae2595ce0e42289c9cc1a626aa2d3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed0e35d14194476cbfbe677d01a84123",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ea0169bc796492db8048bcc913b63e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3dff29714de9457dbb43135b4d756146",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bbdd484e4ac4aa5b3db82c18b1ec892",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9acad05a4db43458f2e26c57bafa217",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparando modelo para LoRA...\n",
            "trainable params: 7,372,800 || all params: 3,093,311,488 || trainable%: 0.2383\n"
          ]
        }
      ],
      "source": [
        "def setup_quantization_config():\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=False,\n",
        "        load_in_8bit=False,\n",
        "    )\n",
        "\n",
        "def setup_lora_config():\n",
        "    return LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"CARGANDO MODELO BASE {BASE_MODEL_ID}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "if base_tokenizer.pad_token is None:\n",
        "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=None,\n",
        ")\n",
        "\n",
        "print(\"Preparando modelo para LoRA...\")\n",
        "lora_config = setup_lora_config()\n",
        "model_lora = get_peft_model(base_model, lora_config)\n",
        "model_lora.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOZ_5quLkT38",
        "outputId": "d908eb79-c9e1-45f9-e29d-700951eb2738"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tama\u00f1o train_dataset: 3981\n",
            "Tama\u00f1o val_dataset: 853\n",
            "input_ids torch.Size([2048]) torch.int64\n",
            "attention_mask torch.Size([2048]) torch.int64\n",
            "labels torch.Size([2048]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "train_dataset = PLSDataset(train_pairs, base_tokenizer, max_length=2048)\n",
        "val_dataset   = PLSDataset(val_pairs,   base_tokenizer, max_length=2048)\n",
        "\n",
        "print(\"Tama\u00f1o train_dataset:\", len(train_dataset))\n",
        "print(\"Tama\u00f1o val_dataset:\", len(val_dataset))\n",
        "\n",
        "# Verificamos un ejemplo\n",
        "sample = train_dataset[0]\n",
        "for k, v in sample.items():\n",
        "    print(k, v.shape, v.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XyNJmsUwvQvY",
        "outputId": "bb33a7e5-ff86-4d45-c22c-c85cd68d4bdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando fine-tuning...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1990' max='1990' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1990/1990 36:52, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.509400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.355200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.379200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.377600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.351300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.344000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.363000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.303900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.315500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.332600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.356300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.355600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.302300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.377200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.323800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.324500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.353700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.338000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.345000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.342300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.334000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.334400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.294200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.315600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.303300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.302800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.316100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>1.258400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.320200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.328300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>1.325600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>1.344400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.318500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>1.361000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.296600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>1.321200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>1.299800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>1.312000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>1.316800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.294800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>1.325400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>1.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>1.347300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>1.298500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.317100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>1.322600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>1.301300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>1.313300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>1.323400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.333000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>1.300900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>1.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>1.285000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>1.279500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.271600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>1.324600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>1.303700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>1.329600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>1.285600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.272000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>1.309800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>1.296400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>1.261100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>1.271300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.325400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>1.295000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>1.254500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>1.283700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>1.257700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.293400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>1.304500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>1.273200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>1.310200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>1.264700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.321800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>1.303700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>1.274200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>1.280400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>1.303000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.293400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1620</td>\n",
              "      <td>1.256600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1640</td>\n",
              "      <td>1.323200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1660</td>\n",
              "      <td>1.290900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1680</td>\n",
              "      <td>1.313400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.328300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1720</td>\n",
              "      <td>1.258000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1740</td>\n",
              "      <td>1.282300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1760</td>\n",
              "      <td>1.276200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1780</td>\n",
              "      <td>1.263400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.250100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1820</td>\n",
              "      <td>1.290800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1840</td>\n",
              "      <td>1.309600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1860</td>\n",
              "      <td>1.297200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1880</td>\n",
              "      <td>1.350700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.266600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1920</td>\n",
              "      <td>1.272900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1940</td>\n",
              "      <td>1.253700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1960</td>\n",
              "      <td>1.290800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1980</td>\n",
              "      <td>1.324800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning completado.\n",
            "Adaptador LoRA guardado en: /content/qwen2.5-3b-pls\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./qwen_lora_out\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=3e-4,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "\n",
        "    eval_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    run_name=\"qwen_lora_run\",\n",
        "\n",
        "    logging_nan_inf_filter=False,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=base_tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_lora,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Iniciando fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning completado.\")\n",
        "\n",
        "model_lora.save_pretrained(ADAPTER_DIR)\n",
        "print(\"Adaptador LoRA guardado en:\", ADAPTER_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTkm7eTji2B8",
        "outputId": "ac6e930f-912e-4ac3-c3f8-3ac35b3f8337"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando evaluador...\n",
            "Inicializando EvaluationMetrics...\n",
            "Dispositivo evaluador: cuda\n",
            "Checkpoint AlignScore local: /root/.cache/huggingface/hub/models--yzha--AlignScore/snapshots/8509e78d25bb914939fc585c626500c9b2944249/AlignScore-base.ckpt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v2.5.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/huggingface/hub/models--yzha--AlignScore/snapshots/8509e78d25bb914939fc585c626500c9b2944249/AlignScore-base.ckpt`\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['base_model.embeddings.position_ids']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EvaluationMetrics listo.\n"
          ]
        }
      ],
      "source": [
        "class EvaluationMetrics:\n",
        "    def __init__(self, device=None):\n",
        "        print(\"Inicializando EvaluationMetrics...\")\n",
        "\n",
        "        self.device = device or (\n",
        "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        )\n",
        "        print(\"Dispositivo evaluador:\", self.device)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # 1) CORRECTO: Descargar checkpoint desde HuggingFace\n",
        "        # ---------------------------------------------------------\n",
        "        from huggingface_hub import hf_hub_download\n",
        "\n",
        "        ckpt_path = hf_hub_download(\n",
        "            repo_id=\"yzha/AlignScore\",\n",
        "            filename=\"AlignScore-base.ckpt\",\n",
        "        )\n",
        "        print(\"Checkpoint AlignScore local:\", ckpt_path)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # 2) CORRECTO: Inicializar AlignScore desde SpeedOfMagic\n",
        "        # ---------------------------------------------------------\n",
        "        self.align_scorer = AlignScore(\n",
        "            model=\"roberta-base\",\n",
        "            batch_size=4,\n",
        "            device=str(self.device),\n",
        "            ckpt_path=ckpt_path,\n",
        "            evaluation_mode=\"nli_sp\",\n",
        "        )\n",
        "\n",
        "        print(\"EvaluationMetrics listo.\")\n",
        "\n",
        "    # ------------------------ RELEVANCIA ------------------------\n",
        "    def relevance(self, generated: str, reference: str):\n",
        "        precision, recall, f1 = bert_score(\n",
        "            [generated],\n",
        "            [reference],\n",
        "            lang=\"en\",\n",
        "            verbose=False,\n",
        "        )\n",
        "        return {\n",
        "            \"precision\": float(precision.item()),\n",
        "            \"recall\": float(recall.item()),\n",
        "            \"f1\": float(f1.item()),\n",
        "        }\n",
        "\n",
        "    # ------------------------ FACTUALIDAD ------------------------\n",
        "    def factuality(self, generated: str, source: str):\n",
        "        try:\n",
        "            if not generated or not isinstance(generated, str) or len(generated.strip()) < 10:\n",
        "                return {\"score\": 0.0}\n",
        "            if not source or not isinstance(source, str) or len(source.strip()) < 10:\n",
        "                return {\"score\": 0.0}\n",
        "            score = self.align_scorer.score(\n",
        "                contexts=[source],\n",
        "                claims=[generated],\n",
        "            )[0]\n",
        "            return {\"score\": float(score)}\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Error calculando factualidad: {e}\")\n",
        "            return {\"score\": 0.0}\n",
        "\n",
        "    # ------------------------ LEGIBILIDAD ------------------------\n",
        "    def readability(self, text: str):\n",
        "        try:\n",
        "            if not text or not isinstance(text, str):\n",
        "                return self._default_readability_metrics()\n",
        "\n",
        "            txt = text.strip()\n",
        "            if len(txt) < 10:\n",
        "                return self._default_readability_metrics()\n",
        "\n",
        "            fre  = float(textstat.flesch_reading_ease(txt))\n",
        "            fk   = float(textstat.flesch_kincaid_grade(txt))\n",
        "            cli  = float(textstat.coleman_liau_index(txt))\n",
        "            gfi  = float(textstat.gunning_fog(txt))\n",
        "            smog = float(textstat.smog_index(txt))\n",
        "            dale = float(textstat.dale_chall_readability_score(txt))\n",
        "\n",
        "            # Normalizar Flesch\n",
        "            fre = max(0.0, min(100.0, fre))\n",
        "\n",
        "            return {\n",
        "                \"flesch_reading_ease\": fre,\n",
        "                \"flesch_kincaid_grade_level\": fk,\n",
        "                \"coleman_liau_index\": cli,\n",
        "                \"gunning_fog_index\": gfi,\n",
        "                \"smog_index\": smog,\n",
        "                \"dale_chall_readability_score\": dale,\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Error calculando legibilidad: {e}\")\n",
        "            return self._default_readability_metrics()\n",
        "\n",
        "    def _default_readability_metrics(self):\n",
        "        return {\n",
        "            \"flesch_reading_ease\": 30.0,\n",
        "            \"flesch_kincaid_grade_level\": 12.0,\n",
        "            \"coleman_liau_index\": 12.0,\n",
        "            \"gunning_fog_index\": 12.0,\n",
        "            \"smog_index\": 12.0,\n",
        "            \"dale_chall_readability_score\": 9.0,\n",
        "        }\n",
        "\n",
        "    # ------------------------ M\u00c9TRICA FINAL ------------------------\n",
        "    def evaluate(self, generated: str, reference: str, source: str):\n",
        "        return {\n",
        "            \"relevance\":   self.relevance(generated, reference),\n",
        "            \"factuality\":  self.factuality(generated, source),\n",
        "            \"readability\": self.readability(generated),\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"Cargando evaluador...\")\n",
        "evaluator = EvaluationMetrics()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RnrwsWAvW6a"
      },
      "outputs": [],
      "source": [
        "def generate_summary(model, tokenizer, medical_text: str, max_new_tokens: int = 256) -> str:\n",
        "    prompt = (\n",
        "        INSTRUCTION\n",
        "        + \"### Medical text:\\n\"\n",
        "        + medical_text.strip()\n",
        "        + \"\\n\\n### Simplified summary:\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "        )\n",
        "\n",
        "    full_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    if \"### Simplified summary:\" in full_text:\n",
        "        summary = full_text.split(\"### Simplified summary:\")[-1].strip()\n",
        "    else:\n",
        "        summary = full_text.strip()\n",
        "    return summary\n",
        "\n",
        "\n",
        "def evaluate_model(model, tokenizer, pairs, sample_size: int = 30):\n",
        "    import random\n",
        "    subset = random.sample(pairs, min(sample_size, len(pairs)))\n",
        "\n",
        "    results = []\n",
        "    for p in subset:\n",
        "        gen = generate_summary(model, tokenizer, p.medical)\n",
        "        metrics = evaluator.evaluate(\n",
        "            generated=gen,\n",
        "            reference=p.plain,\n",
        "            source=p.medical,\n",
        "        )\n",
        "        results.append(metrics)\n",
        "    return results\n",
        "\n",
        "\n",
        "def summarize_metrics(results: List[Dict[str, Any]]) -> Dict[str, float]:\n",
        "    import numpy as np\n",
        "\n",
        "    if not results:\n",
        "        return {}\n",
        "\n",
        "    bert_f1 = [r[\"relevance\"][\"f1\"] for r in results]\n",
        "    factual = [r[\"factuality\"][\"score\"] for r in results]\n",
        "    flesch  = [r[\"readability\"][\"flesch_reading_ease\"] for r in results]\n",
        "\n",
        "    return {\n",
        "        \"bertscore_f1\": float(np.mean(bert_f1)),\n",
        "        \"factuality\": float(np.mean(factual)),\n",
        "        \"flesch_reading_ease\": float(np.mean(flesch)),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "baff22760ef440f7986a1011f48b9830",
            "531ea27c170347c0ad727ac5022bdb92",
            "2c755c38aef14843816e6eb4083cba3a",
            "114dadcdb4ad4e08963286f9fc14a0ed",
            "146f70f950bd46119f47a7269b83311d",
            "1afd3f824dd74dc68ebd0ef33c36d49f",
            "2a2c4b79577847749e3d0d2051523294",
            "203d0e9aa8754b3d8d2347842e9a9f50",
            "1712aa460b9c4a5aa15472922a1dbf6a",
            "d5975d2dba1d4fe8ad94f7b78c156b29",
            "ab6ce7bf1dde4b3bb9baebf637771319",
            "274d6f2db8e444e8b7cf1a25e3fc4f88",
            "4f35830c22f441e987afe9496eefea64",
            "98f529dd3f60461c86e8f3e88d0efc7a",
            "27f53c7c81d04b7aa3e2746bf64e81e3",
            "3c8972ecdc59417494afb4bdd2374d1e",
            "ab046ce69f6649e79dba57730792ad21",
            "265cb289798c4d74821a28f48a06b2bb",
            "7696e7f933db4a18b05a0a62eec5f4ab",
            "5e755f8b11024e1fbfa927defc25efa2",
            "65785f4ba2cf43e2a609605227871c29",
            "a3755d912aba46238a6c7b083fed20d8",
            "5b584572ac2b4c36b4a4f0f3dacdb91f",
            "89bf91b803c04e3390b3faa6cf74c9f1",
            "f610a5f0ea1843b08a1b44c9c2121681",
            "e52fdd8ccab54e229c7f9a9b221d28ee",
            "782f86b5e2054f679757ee7fe1869774",
            "4f4e86ed95a942d9b41403c04e9ab0e0",
            "b994941e981e4364be393a8ed44dc9d1",
            "a6599be7ed7747b2abca3fd9349e856a",
            "b79662e04d5147ecbc474ff073bef210",
            "7b45c44d04e742d8a94971c65b0f8bf5",
            "0db9272935094118b8007999ac03b36a",
            "84c067d426c247169ee8c1564291eb5a",
            "39fa467bf85c424aaa18f00c962dba2f",
            "99560eb4499e4255adcdbae38bc284a2",
            "6012b2d5e5214a0a9547e05a897d251a",
            "a4b2301ffa994ebabcec0d16c5cfff7b",
            "6a7c14b6078a4d3d8a8b8fef673be593",
            "ca1b48f378f94d7a8ce41ae7da305fbe",
            "9a2bdb9ee1ec41f783983c85f81833d8",
            "7ec5ff05a8e04cc3ac29d9033c211317",
            "5cfe27e2a52848acbcf1cee47a4e3bcb",
            "67638360d8cc4b9d8aa5691341ee6a6e",
            "d5979508fdf248999c86ea59cd0d7e8d",
            "e6766f3fdf28457bb4c3f74a0f0d027d",
            "8be373dd0afb4aa0aa1e758a335b795b",
            "2bd1f4c893cb4e4a90835caf25c817bd",
            "122c879ab39a404284fd3fa9fbd2b5fd",
            "3199fe1f77254d0a9109889cf8409ac8",
            "bcaccc0f419b425ea6f6e2ce796e0a93",
            "ac85031c6b794fa794359f6644a757bd",
            "39aae464856844de896681dbb7d067aa",
            "4f180409b1ab47d7a759d561ad149ccb",
            "2dfca49dc4ec41dcb98e723d65767f20",
            "71a8ad30f91e45cbb0971ae6a4bab3e1",
            "4fdb45052f3141979624de4da38659fe",
            "a4e50e0a81f143a0bccdc4fae0581710",
            "ae851b60f4d1409885af711ac42d8c4e",
            "cda7dcabd8c0441ca7c418e226adc3fe",
            "b8e94e710e9b4fa89c1abde2df431232",
            "f12d3a3a0bf34100ba885b3c30866d09",
            "b74575ea907f40e1bb58be9479e0ad3a",
            "ac8b5a0faabf4d178a719cf9a966b5c5",
            "616e52afc91f459aa7dcfeb3e248ba57",
            "c1d5adcbe58745bab08e1df6bb7b260d"
          ]
        },
        "id": "jUoVLeizvYzW",
        "outputId": "f2ed1c1b-52e3-4d13-c41f-05a55fe162b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EVALUANDO MODELO BASE\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "baff22760ef440f7986a1011f48b9830",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "274d6f2db8e444e8b7cf1a25e3fc4f88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5b584572ac2b4c36b4a4f0f3dacdb91f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84c067d426c247169ee8c1564291eb5a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5979508fdf248999c86ea59cd0d7e8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71a8ad30f91e45cbb0971ae6a4bab3e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.88it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.02it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.52it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.79it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.28it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.09it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.64it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.52it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.33it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.62it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.70it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.74it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.10it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.87it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base: {'bertscore_f1': 0.872568831841151, 'factuality': 0.6230997035900752, 'flesch_reading_ease': 37.96072779144881}\n",
            "================================================================================\n",
            "EVALUANDO MODELO FINE-TUNED (LoRA)\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.40it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.68it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.06it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.24it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.94it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.35it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.84it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.38it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.85it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.22it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.06it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.15it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.81it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.46it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.06it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.54it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.85it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.02it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.87it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.98it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.22it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned: {'bertscore_f1': 0.8633678078651428, 'factuality': 0.6695457835992177, 'flesch_reading_ease': 34.804142693873345}\n",
            "\n",
            "Tabla comparativa:\n",
            "                          Modelo Base  Fine-tuned (LoRA)\n",
            "BERTScore F1                 0.872569           0.863368\n",
            "AlignScore (Factualidad)     0.623100           0.669546\n",
            "Flesch Reading Ease         37.960728          34.804143\n",
            "\n",
            "Diferencias (Fine-tuned - Base):\n",
            "\u0394 BERTScore F1: -0.009201023976008194\n",
            "\u0394 Factuality  : 0.04644608000914252\n",
            "\u0394 Flesch      : -3.1565850975754657\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EVALUANDO MODELO BASE\")\n",
        "print(\"=\"*80)\n",
        "base_results = evaluate_model(base_model, base_tokenizer, eval_pairs, sample_size=min(30, len(eval_pairs)))\n",
        "BASE_MODEL_METRICS = summarize_metrics(base_results)\n",
        "print(\"Base:\", BASE_MODEL_METRICS)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EVALUANDO MODELO FINE-TUNED (LoRA)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# modelo fine-tuned es model_lora que ya est\u00e1 en memoria\n",
        "finetuned_model = model_lora\n",
        "finetuned_results = evaluate_model(finetuned_model, base_tokenizer, eval_pairs, sample_size=min(30, len(eval_pairs)))\n",
        "FINETUNED_MODEL_METRICS = summarize_metrics(finetuned_results)\n",
        "print(\"Fine-tuned:\", FINETUNED_MODEL_METRICS)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "metrics_table = {\n",
        "    \"BERTScore F1\": {\n",
        "        \"Modelo Base\": BASE_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        \"Fine-tuned (LoRA)\": FINETUNED_MODEL_METRICS[\"bertscore_f1\"],\n",
        "    },\n",
        "    \"AlignScore (Factualidad)\": {\n",
        "        \"Modelo Base\": BASE_MODEL_METRICS[\"factuality\"],\n",
        "        \"Fine-tuned (LoRA)\": FINETUNED_MODEL_METRICS[\"factuality\"],\n",
        "    },\n",
        "    \"Flesch Reading Ease\": {\n",
        "        \"Modelo Base\": BASE_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        \"Fine-tuned (LoRA)\": FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(metrics_table).T\n",
        "print(\"\\nTabla comparativa:\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\nDiferencias (Fine-tuned - Base):\")\n",
        "print(\"\u0394 BERTScore F1:\", FINETUNED_MODEL_METRICS[\"bertscore_f1\"] - BASE_MODEL_METRICS[\"bertscore_f1\"])\n",
        "print(\"\u0394 Factuality  :\", FINETUNED_MODEL_METRICS[\"factuality\"] - BASE_MODEL_METRICS[\"factuality\"])\n",
        "print(\"\u0394 Flesch      :\", FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"] - BASE_MODEL_METRICS[\"flesch_reading_ease\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQiDNJg3-N3m",
        "outputId": "ee60c918-7dd6-4496-e58f-6def3fa5cd7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stable-baselines3==2.3.0\n",
            "  Downloading stable_baselines3-2.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting sb3-contrib==2.3.0\n",
            "  Downloading sb3_contrib-2.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.3.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->stable-baselines3==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13->stable-baselines3==2.3.0) (3.0.3)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'stable-baselines3' candidate (version 2.3.0 at https://files.pythonhosted.org/packages/51/0b/6539076ed58343f1404dea0462167b079b5264508b8e5bbed01cea9f66b8/stable_baselines3-2.3.0-py3-none-any.whl (from https://pypi.org/simple/stable-baselines3/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Loading broken with PyTorch 1.13\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading stable_baselines3-2.3.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sb3_contrib-2.3.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium, stable-baselines3, sb3-contrib\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.2.2\n",
            "    Uninstalling gymnasium-1.2.2:\n",
            "      Successfully uninstalled gymnasium-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 sb3-contrib-2.3.0 stable-baselines3-2.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3==2.3.0 sb3-contrib==2.3.0 gymnasium==0.29.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgqUMMxc-SWH",
        "outputId": "6d8573ce-f54d-40d4-e607-b551585b49e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import stable_baselines3\n",
        "print(stable_baselines3.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVOsu8X_jOO1",
        "outputId": "10b82762-ad8e-4745-fe97-8542c179ae28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando TD3 para MODELO BASE...\n",
            "Using cuda device\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.28it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.10it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.27it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.67it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.662    |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 74       |\n",
            "|    total_timesteps | 4        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.58s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.64it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.02s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.664    |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 157      |\n",
            "|    total_timesteps | 8        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.71it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.31s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 240      |\n",
            "|    total_timesteps | 12       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.41s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.27s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 10.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.686    |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 323      |\n",
            "|    total_timesteps | 16       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.66s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.70it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.69s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.693    |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 404      |\n",
            "|    total_timesteps | 20       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.41s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.695    |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 486      |\n",
            "|    total_timesteps | 24       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.80it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.54it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.37it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.695    |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 568      |\n",
            "|    total_timesteps | 28       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.37it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.67s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.03s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.692    |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 652      |\n",
            "|    total_timesteps | 32       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.93it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.81it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.694    |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 732      |\n",
            "|    total_timesteps | 36       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.30it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.46s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.696    |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 814      |\n",
            "|    total_timesteps | 40       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.18it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.30s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.695    |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 896      |\n",
            "|    total_timesteps | 44       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.62it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.51s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.689    |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 981      |\n",
            "|    total_timesteps | 48       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.40s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.38it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.687    |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1062     |\n",
            "|    total_timesteps | 52       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.47it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.22it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.685    |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1143     |\n",
            "|    total_timesteps | 56       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.71it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.61s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.687    |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1225     |\n",
            "|    total_timesteps | 60       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.12it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.10it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1306     |\n",
            "|    total_timesteps | 64       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.75it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.47it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.19it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.686    |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1386     |\n",
            "|    total_timesteps | 68       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.01it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.69it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.87s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.689    |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1468     |\n",
            "|    total_timesteps | 72       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.59s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.61s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.46it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.69     |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1552     |\n",
            "|    total_timesteps | 76       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.77it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.55it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.88it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.688    |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1633     |\n",
            "|    total_timesteps | 80       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.27it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.68it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.65s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.686    |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1715     |\n",
            "|    total_timesteps | 84       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.56s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.95it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.08s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.686    |\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1799     |\n",
            "|    total_timesteps | 88       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.48s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.55it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.22it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.686    |\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1880     |\n",
            "|    total_timesteps | 92       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.28it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.18it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.75it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.683    |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1961     |\n",
            "|    total_timesteps | 96       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.11s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.02it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.41s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.683    |\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2044     |\n",
            "|    total_timesteps | 100      |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 19.47it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.69it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 22.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2124     |\n",
            "|    total_timesteps | 104      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.721   |\n",
            "|    critic_loss     | 0.0417   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.79it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 108      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2206     |\n",
            "|    total_timesteps | 108      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.823   |\n",
            "|    critic_loss     | 0.0462   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.56it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 10.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.85it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2286     |\n",
            "|    total_timesteps | 112      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.64    |\n",
            "|    critic_loss     | 0.0566   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.01it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.88it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.673    |\n",
            "| time/              |          |\n",
            "|    episodes        | 116      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2367     |\n",
            "|    total_timesteps | 116      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.709   |\n",
            "|    critic_loss     | 0.0246   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.87it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.70s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.32it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.668    |\n",
            "| time/              |          |\n",
            "|    episodes        | 120      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2449     |\n",
            "|    total_timesteps | 120      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.777   |\n",
            "|    critic_loss     | 0.0323   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 10.08it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.663    |\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2530     |\n",
            "|    total_timesteps | 124      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.641   |\n",
            "|    critic_loss     | 0.0199   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.18s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.09s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.34it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 23.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.662    |\n",
            "| time/              |          |\n",
            "|    episodes        | 128      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2612     |\n",
            "|    total_timesteps | 128      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.487   |\n",
            "|    critic_loss     | 0.0164   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.35it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.34it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.654    |\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2692     |\n",
            "|    total_timesteps | 132      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.55    |\n",
            "|    critic_loss     | 0.0138   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 31       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.68it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.55s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.653    |\n",
            "| time/              |          |\n",
            "|    episodes        | 136      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2774     |\n",
            "|    total_timesteps | 136      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.674   |\n",
            "|    critic_loss     | 0.00899  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 35       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 23.48it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.90it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.644    |\n",
            "| time/              |          |\n",
            "|    episodes        | 140      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2854     |\n",
            "|    total_timesteps | 140      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.642   |\n",
            "|    critic_loss     | 0.0137   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 39       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.05it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.08it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.56s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.638    |\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2936     |\n",
            "|    total_timesteps | 144      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.528   |\n",
            "|    critic_loss     | 0.00937  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 43       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.37it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 10.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.98it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.637    |\n",
            "| time/              |          |\n",
            "|    episodes        | 148      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3018     |\n",
            "|    total_timesteps | 148      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.529   |\n",
            "|    critic_loss     | 0.0119   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 47       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.22it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 23.30it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.635    |\n",
            "| time/              |          |\n",
            "|    episodes        | 152      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3099     |\n",
            "|    total_timesteps | 152      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.626   |\n",
            "|    critic_loss     | 0.0121   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 51       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.80it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.15s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.00it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.632    |\n",
            "| time/              |          |\n",
            "|    episodes        | 156      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3181     |\n",
            "|    total_timesteps | 156      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.596   |\n",
            "|    critic_loss     | 0.0107   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 55       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.06it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.57it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.51s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.63     |\n",
            "| time/              |          |\n",
            "|    episodes        | 160      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3264     |\n",
            "|    total_timesteps | 160      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.543   |\n",
            "|    critic_loss     | 0.00862  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 59       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.18s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.631    |\n",
            "| time/              |          |\n",
            "|    episodes        | 164      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3348     |\n",
            "|    total_timesteps | 164      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.615   |\n",
            "|    critic_loss     | 0.0126   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 63       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.45s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.40it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.81it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.626    |\n",
            "| time/              |          |\n",
            "|    episodes        | 168      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3429     |\n",
            "|    total_timesteps | 168      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.637   |\n",
            "|    critic_loss     | 0.0143   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 67       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.42it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.31s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.62     |\n",
            "| time/              |          |\n",
            "|    episodes        | 172      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3511     |\n",
            "|    total_timesteps | 172      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.57    |\n",
            "|    critic_loss     | 0.0081   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 71       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.36s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.71it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.616    |\n",
            "| time/              |          |\n",
            "|    episodes        | 176      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3593     |\n",
            "|    total_timesteps | 176      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.595   |\n",
            "|    critic_loss     | 0.012    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 75       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.64s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12.46it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.45s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.616    |\n",
            "| time/              |          |\n",
            "|    episodes        | 180      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3676     |\n",
            "|    total_timesteps | 180      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.551   |\n",
            "|    critic_loss     | 0.00767  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 79       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.65it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.95it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.616    |\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3757     |\n",
            "|    total_timesteps | 184      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.573   |\n",
            "|    critic_loss     | 0.0144   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 83       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 10.98it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.15s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.46s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 10.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.613    |\n",
            "| time/              |          |\n",
            "|    episodes        | 188      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3839     |\n",
            "|    total_timesteps | 188      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.634   |\n",
            "|    critic_loss     | 0.00891  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 87       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.12s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.73s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.39s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.612    |\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3924     |\n",
            "|    total_timesteps | 192      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.617   |\n",
            "|    critic_loss     | 0.0111   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 91       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.51it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.613    |\n",
            "| time/              |          |\n",
            "|    episodes        | 196      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4005     |\n",
            "|    total_timesteps | 196      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.611   |\n",
            "|    critic_loss     | 0.00391  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 95       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.51s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.63it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.609    |\n",
            "| time/              |          |\n",
            "|    episodes        | 200      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4087     |\n",
            "|    total_timesteps | 200      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.595   |\n",
            "|    critic_loss     | 0.0178   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 99       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenando TD3 para MODELO LoRA...\n",
            "Using cuda device\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.25it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.26it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.18it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 80       |\n",
            "|    total_timesteps | 4        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.30s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.22s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.698    |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 163      |\n",
            "|    total_timesteps | 8        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.45s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.47it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.707    |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 246      |\n",
            "|    total_timesteps | 12       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.97s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.32s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.54it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.695    |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 329      |\n",
            "|    total_timesteps | 16       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.35it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.74it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.49it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.701    |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 411      |\n",
            "|    total_timesteps | 20       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.49it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.24it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.691    |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 493      |\n",
            "|    total_timesteps | 24       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.33s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.38s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.694    |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 576      |\n",
            "|    total_timesteps | 28       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.99it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.41s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.693    |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 658      |\n",
            "|    total_timesteps | 32       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.17it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.56it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.693    |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 739      |\n",
            "|    total_timesteps | 36       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.61s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.05it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.76s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.689    |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 822      |\n",
            "|    total_timesteps | 40       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.32s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.92it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.692    |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 905      |\n",
            "|    total_timesteps | 44       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.50it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.77it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.63it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.691    |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 986      |\n",
            "|    total_timesteps | 48       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.64s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.08s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.695    |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1068     |\n",
            "|    total_timesteps | 52       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.14s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.48s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 19.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.694    |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1151     |\n",
            "|    total_timesteps | 56       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.19it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.685    |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1231     |\n",
            "|    total_timesteps | 60       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.31s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.76it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1312     |\n",
            "|    total_timesteps | 64       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.86it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.683    |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1393     |\n",
            "|    total_timesteps | 68       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.18it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.25s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.90it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.682    |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1474     |\n",
            "|    total_timesteps | 72       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.53it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.37it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.69it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1554     |\n",
            "|    total_timesteps | 76       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.32it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.34it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.37it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.54s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1636     |\n",
            "|    total_timesteps | 80       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.27it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.12s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1718     |\n",
            "|    total_timesteps | 84       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.64s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.71s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.683    |\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1803     |\n",
            "|    total_timesteps | 88       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.17s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.11s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.682    |\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1885     |\n",
            "|    total_timesteps | 92       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.62it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.47s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.87it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 22.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1966     |\n",
            "|    total_timesteps | 96       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.00it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.77it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2048     |\n",
            "|    total_timesteps | 100      |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.09it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.69s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.28s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.682    |\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2131     |\n",
            "|    total_timesteps | 104      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.786   |\n",
            "|    critic_loss     | 0.149    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.92it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.26s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 108      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2212     |\n",
            "|    total_timesteps | 108      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.735   |\n",
            "|    critic_loss     | 0.012    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.10it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 10.15it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.41s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2296     |\n",
            "|    total_timesteps | 112      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.586   |\n",
            "|    critic_loss     | 0.0679   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.35it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.50it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.45it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 116      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2377     |\n",
            "|    total_timesteps | 116      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.769   |\n",
            "|    critic_loss     | 0.0149   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.45it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 120      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2457     |\n",
            "|    total_timesteps | 120      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.903   |\n",
            "|    critic_loss     | 0.0432   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.40it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.86it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.45it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2539     |\n",
            "|    total_timesteps | 124      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.727   |\n",
            "|    critic_loss     | 0.0214   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.05s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.50it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.80it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 128      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2620     |\n",
            "|    total_timesteps | 128      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.596   |\n",
            "|    critic_loss     | 0.0233   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.66it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.66it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.18s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2702     |\n",
            "|    total_timesteps | 132      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.647   |\n",
            "|    critic_loss     | 0.00781  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 31       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.15it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.30it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.677    |\n",
            "| time/              |          |\n",
            "|    episodes        | 136      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2782     |\n",
            "|    total_timesteps | 136      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.743   |\n",
            "|    critic_loss     | 0.0257   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 35       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.52it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.35s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.85it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 140      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2862     |\n",
            "|    total_timesteps | 140      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.68    |\n",
            "|    critic_loss     | 0.00964  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 39       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.63it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.672    |\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2942     |\n",
            "|    total_timesteps | 144      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.649   |\n",
            "|    critic_loss     | 0.0157   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 43       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.42it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.32it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.12s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 148      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3025     |\n",
            "|    total_timesteps | 148      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.695   |\n",
            "|    critic_loss     | 0.0232   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 47       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.69it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.41s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.60s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.673    |\n",
            "| time/              |          |\n",
            "|    episodes        | 152      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3110     |\n",
            "|    total_timesteps | 152      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.728   |\n",
            "|    critic_loss     | 0.0161   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 51       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.15s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.71it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.674    |\n",
            "| time/              |          |\n",
            "|    episodes        | 156      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3193     |\n",
            "|    total_timesteps | 156      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.65    |\n",
            "|    critic_loss     | 0.00966  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 55       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.47s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 160      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3274     |\n",
            "|    total_timesteps | 160      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.669   |\n",
            "|    critic_loss     | 0.011    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 59       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.27s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.21it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.57it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 164      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3356     |\n",
            "|    total_timesteps | 164      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.699   |\n",
            "|    critic_loss     | 0.00785  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 63       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.21it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.36it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 168      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3437     |\n",
            "|    total_timesteps | 168      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.676   |\n",
            "|    critic_loss     | 0.0146   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 67       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.73s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.18it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.10s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.682    |\n",
            "| time/              |          |\n",
            "|    episodes        | 172      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3519     |\n",
            "|    total_timesteps | 172      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.661   |\n",
            "|    critic_loss     | 0.0127   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 71       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.54it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.57it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.682    |\n",
            "| time/              |          |\n",
            "|    episodes        | 176      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3600     |\n",
            "|    total_timesteps | 176      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.697   |\n",
            "|    critic_loss     | 0.00658  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 75       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.63s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.92it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.62it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 180      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3682     |\n",
            "|    total_timesteps | 180      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.693   |\n",
            "|    critic_loss     | 0.00435  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 79       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.02s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.16s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3765     |\n",
            "|    total_timesteps | 184      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.662   |\n",
            "|    critic_loss     | 0.00997  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 83       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.11s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.64s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 188      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3847     |\n",
            "|    total_timesteps | 188      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.694   |\n",
            "|    critic_loss     | 0.00559  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 87       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.70s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.37it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.92it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3930     |\n",
            "|    total_timesteps | 192      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.676   |\n",
            "|    critic_loss     | 0.00753  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 91       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.84it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.36it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.70s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.682    |\n",
            "| time/              |          |\n",
            "|    episodes        | 196      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4012     |\n",
            "|    total_timesteps | 196      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.662   |\n",
            "|    critic_loss     | 0.0176   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 95       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.85it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.81it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.25s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.39s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 200      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4094     |\n",
            "|    total_timesteps | 200      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.696   |\n",
            "|    critic_loss     | 0.00731  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 99       |\n",
            "---------------------------------\n",
            "Entrenamiento TD3 completado.\n",
            "\n",
            "Guardando agentes TD3...\n",
            "\u2713 Agente TD3 BASE guardado en: /content/td3_base_agent\n",
            "\u2713 Agente TD3 LORA guardado en: /content/td3_lora_agent\n",
            "\n",
            "Nota: Los agentes TD3 guardados pueden ser cargados en futuras ejecuciones para evitar reentrenar.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# SECCI\u00d3N 7: TD3 PARA AJUSTAR LA DECODIFICACI\u00d3N\n",
        "# ==============================================================\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "from stable_baselines3 import TD3\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7.1 Helper: construir prompt y generar resumen con temperatura/top_p\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def build_prompt(medical_text: str) -> str:\n",
        "    \"\"\"Plantilla de prompt (aj\u00fastala si en tu notebook usas otra).\"\"\"\n",
        "    return (\n",
        "        \"You are a specialist in healthcare communication. \"\n",
        "        \"Use the context to transform the following medical text into a clear, \"\n",
        "        \"concise, and easy-to-understand summary for a patient and their family. \"\n",
        "        \"Retain all relevant clinical data, but explain technical terms using simple \"\n",
        "        \"language and short sentences.\\n\\n\"\n",
        "        \"### Medical text:\\n\"\n",
        "        f\"{medical_text}\\n\\n\"\n",
        "        \"### Simplified summary:\\n\"\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with_params(model, tokenizer, medical_text: str,\n",
        "                         temperature: float = 0.7,\n",
        "                         top_p: float = 0.9,\n",
        "                         max_new_tokens: int = 256) -> str:\n",
        "    \"\"\"Genera un resumen usando el modelo con ciertos par\u00e1metros de decodificaci\u00f3n.\"\"\"\n",
        "    prompt = build_prompt(medical_text)\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=1024\n",
        "    ).to(model.device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=float(temperature),\n",
        "        top_p=float(top_p),\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    # Devolvemos solo lo que viene despu\u00e9s de \"### Simplified summary:\"\n",
        "    if \"### Simplified summary:\" in full_text:\n",
        "        return full_text.split(\"### Simplified summary:\")[-1].strip()\n",
        "    return full_text.strip()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7.2 Definici\u00f3n del entorno Gym para TD3\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "class SummarizationTD3Env(gym.Env):\n",
        "    metadata = {\"render.modes\": []}\n",
        "\n",
        "    def __init__(self, model, tokenizer, pairs, evaluator, max_new_tokens=256):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pairs = pairs\n",
        "        self.evaluator = evaluator\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "        # Acci\u00f3n continua: temperatura y top_p\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0.1, 0.1], dtype=np.float32),\n",
        "            high=np.array([1.0, 1.0], dtype=np.float32),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        # Observaci\u00f3n: [len(medical_characters_normalized)]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0.0], dtype=np.float32),\n",
        "            high=np.array([1.0], dtype=np.float32),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.current_index = 0\n",
        "\n",
        "    def _get_obs_for_index(self, idx: int):\n",
        "        med = self.pairs[idx].medical\n",
        "        n_chars = len(med)\n",
        "        # Normalizamos por 8000 chars para tener algo entre 0 y 1\n",
        "        return np.array([min(1.0, n_chars / 8000.0)], dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_index = np.random.randint(0, len(self.pairs))\n",
        "        obs = self._get_obs_for_index(self.current_index)\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # Clip de seguridad\n",
        "        temperature = float(np.clip(action[0], 0.1, 1.0))\n",
        "        top_p = float(np.clip(action[1], 0.1, 1.0))\n",
        "\n",
        "        pair = self.pairs[self.current_index]\n",
        "        medical = pair.medical\n",
        "        reference = pair.plain\n",
        "        source = pair.medical\n",
        "\n",
        "        # Generar resumen\n",
        "        generated = generate_with_params(\n",
        "            self.model,\n",
        "            self.tokenizer,\n",
        "            medical,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "        )\n",
        "\n",
        "        # M\u00e9tricas con manejo de errores\n",
        "        try:\n",
        "            metrics = self.evaluator.evaluate(\n",
        "                generated=generated,\n",
        "                reference=reference,\n",
        "                source=source,\n",
        "            )\n",
        "\n",
        "            bert_f1 = metrics[\"relevance\"][\"f1\"]\n",
        "            fact = metrics[\"factuality\"][\"score\"]\n",
        "            fre = metrics[\"readability\"][\"flesch_reading_ease\"]\n",
        "        except Exception as e:\n",
        "\n",
        "            print(f\"[WARN] Error en evaluaci\u00f3n TD3: {e}\")\n",
        "            bert_f1 = 0.0\n",
        "            fact = 0.0\n",
        "            fre = 0.0\n",
        "\n",
        "        # Normalizar Flesch (0-100) -> [0,1]\n",
        "        fre_norm = max(0.0, min(100.0, fre)) / 100.0\n",
        "\n",
        "        # -----------------------------\n",
        "        # REWARD (aj\u00fastalo a tu gusto)\n",
        "        # -----------------------------\n",
        "\n",
        "        reward = (\n",
        "            0.3 * fact +\n",
        "            0.3 * bert_f1 +\n",
        "            0.4 * fre_norm\n",
        "        )\n",
        "\n",
        "        obs = self._get_obs_for_index(self.current_index)\n",
        "        terminated = True\n",
        "        truncated = False\n",
        "        info = {\n",
        "            \"metrics\": metrics,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "        }\n",
        "\n",
        "        # Siguiente episodio usar\u00e1 otro \u00edndice\n",
        "        self.current_index = np.random.randint(0, len(self.pairs))\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7.3 Crear entornos para Base y LoRA\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "# Usamos eval_pairs como \"pool\" para RL\n",
        "rl_pairs = eval_pairs\n",
        "\n",
        "env_base = make_vec_env(\n",
        "    lambda: SummarizationTD3Env(base_model, base_tokenizer, rl_pairs, evaluator),\n",
        "    n_envs=1\n",
        ")\n",
        "\n",
        "env_lora = make_vec_env(\n",
        "    lambda: SummarizationTD3Env(model_lora, base_tokenizer, rl_pairs, evaluator),\n",
        "    n_envs=1\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7.4 Entrenar TD3 para Base y para LoRA\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "TD3_BASE_DIR = \"/content/td3_base_agent\"\n",
        "TD3_LORA_DIR = \"/content/td3_lora_agent\"\n",
        "td3_steps = 200\n",
        "\n",
        "if os.path.exists(TD3_BASE_DIR) and os.path.exists(os.path.join(TD3_BASE_DIR, \"policy.pkl\")):\n",
        "    print(\"Cargando agente TD3 BASE existente...\")\n",
        "    td3_base = TD3.load(TD3_BASE_DIR, env=env_base)\n",
        "    print(\"Agente TD3 BASE cargado exitosamente.\")\n",
        "else:\n",
        "    print(\"Entrenando TD3 para MODELO BASE...\")\n",
        "    td3_base = TD3(\n",
        "        \"MlpPolicy\",\n",
        "        env_base,\n",
        "        learning_rate=1e-3,\n",
        "        batch_size=32,\n",
        "        verbose=1,\n",
        "    )\n",
        "    td3_base.learn(total_timesteps=td3_steps)\n",
        "\n",
        "if os.path.exists(TD3_LORA_DIR) and os.path.exists(os.path.join(TD3_LORA_DIR, \"policy.pkl\")):\n",
        "    print(\"\\nCargando agente TD3 LORA existente...\")\n",
        "    td3_lora = TD3.load(TD3_LORA_DIR, env=env_lora)\n",
        "    print(\"Agente TD3 LORA cargado exitosamente.\")\n",
        "else:\n",
        "    print(\"\\nEntrenando TD3 para MODELO LoRA...\")\n",
        "    td3_lora = TD3(\n",
        "        \"MlpPolicy\",\n",
        "        env_lora,\n",
        "        learning_rate=1e-3,\n",
        "        batch_size=32,\n",
        "        verbose=1,\n",
        "    )\n",
        "    td3_lora.learn(total_timesteps=td3_steps)\n",
        "\n",
        "print(\"Entrenamiento TD3 completado.\")\n",
        "\n",
        "# Guardar los agentes TD3\n",
        "os.makedirs(TD3_BASE_DIR, exist_ok=True)\n",
        "os.makedirs(TD3_LORA_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\nGuardando agentes TD3...\")\n",
        "td3_base.save(TD3_BASE_DIR)\n",
        "td3_lora.save(TD3_LORA_DIR)\n",
        "print(f\"\u2713 Agente TD3 BASE guardado en: {TD3_BASE_DIR}\")\n",
        "print(f\"\u2713 Agente TD3 LORA guardado en: {TD3_LORA_DIR}\")\n",
        "print(\"\\nNota: Los agentes TD3 guardados pueden ser cargados en futuras ejecuciones para evitar reentrenar.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUFZTcVFIHfi",
        "outputId": "6a67d9c9-f49f-4f8c-8b74-03bc6dab1475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2713 M\u00e9todo step parcheado en entornos existentes\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# Parchear el m\u00e9todo step de los entornos existentes\n",
        "# ==============================================================\n",
        "\n",
        "\n",
        "import types\n",
        "\n",
        "def step_patched(self, action):\n",
        "    action = np.asarray(action, dtype=np.float32)\n",
        "    if action.ndim == 0:  # Escalar\n",
        "        temperature = float(np.clip(action, 0.1, 1.0))\n",
        "        top_p = float(np.clip(action, 0.1, 1.0))\n",
        "    elif action.ndim == 1:\n",
        "        if len(action) >= 2:\n",
        "            temperature = float(np.clip(action[0], 0.1, 1.0))\n",
        "            top_p = float(np.clip(action[1], 0.1, 1.0))\n",
        "        elif len(action) == 1:\n",
        "            temperature = float(np.clip(action[0], 0.1, 1.0))\n",
        "            top_p = float(np.clip(action[0], 0.1, 1.0))\n",
        "        else:\n",
        "            temperature = 0.7\n",
        "            top_p = 0.9\n",
        "    else:\n",
        "        action_flat = action.flatten()\n",
        "        if len(action_flat) >= 2:\n",
        "            temperature = float(np.clip(action_flat[0], 0.1, 1.0))\n",
        "            top_p = float(np.clip(action_flat[1], 0.1, 1.0))\n",
        "        else:\n",
        "            temperature = float(np.clip(action_flat[0], 0.1, 1.0))\n",
        "            top_p = float(np.clip(action_flat[0], 0.1, 1.0))\n",
        "\n",
        "    pair = self.pairs[self.current_index]\n",
        "    medical = pair.medical\n",
        "    reference = pair.plain\n",
        "    source = pair.medical\n",
        "\n",
        "    # Generar resumen\n",
        "    generated = generate_with_params(\n",
        "        self.model,\n",
        "        self.tokenizer,\n",
        "        medical,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_new_tokens=self.max_new_tokens,\n",
        "    )\n",
        "\n",
        "    # M\u00e9tricas con manejo de errores\n",
        "    try:\n",
        "        metrics = self.evaluator.evaluate(\n",
        "            generated=generated,\n",
        "            reference=reference,\n",
        "            source=source,\n",
        "        )\n",
        "\n",
        "        bert_f1 = metrics[\"relevance\"][\"f1\"]\n",
        "        fact = metrics[\"factuality\"][\"score\"]\n",
        "        fre = metrics[\"readability\"][\"flesch_reading_ease\"]\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Error en evaluaci\u00f3n TD3: {e}\")\n",
        "        bert_f1 = 0.0\n",
        "        fact = 0.0\n",
        "        fre = 0.0\n",
        "        metrics = {\n",
        "            \"relevance\": {\"f1\": 0.0},\n",
        "            \"factuality\": {\"score\": 0.0},\n",
        "            \"readability\": {\"flesch_reading_ease\": 0.0}\n",
        "        }\n",
        "\n",
        "    # Normalizar Flesch (0-100) -> [0,1]\n",
        "    fre_norm = max(0.0, min(100.0, fre)) / 100.0\n",
        "\n",
        "    # REWARD\n",
        "    reward = (\n",
        "        0.3 * fact +\n",
        "        0.3 * bert_f1 +\n",
        "        0.4 * fre_norm\n",
        "    )\n",
        "\n",
        "    obs = self._get_obs_for_index(self.current_index)\n",
        "    terminated = True\n",
        "    truncated = False\n",
        "    info = {\n",
        "        \"metrics\": metrics,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "    }\n",
        "    self.current_index = np.random.randint(0, len(self.pairs))\n",
        "\n",
        "    return obs, reward, terminated, truncated, info\n",
        "# Aplicar el parche a los entornos existentes\n",
        "env_base.envs[0].env.step = types.MethodType(step_patched, env_base.envs[0].env)\n",
        "env_lora.envs[0].env.step = types.MethodType(step_patched, env_lora.envs[0].env)\n",
        "\n",
        "print(\"\u2713 M\u00e9todo step parcheado en entornos existentes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhyszfquESsO",
        "outputId": "270ec414-b53d-466f-b77e-59f1bbe8c00d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.15s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.09s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.72it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.88it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.42it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.52it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.78it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.30it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.35it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.28it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.40it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.66it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.06it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.49it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.32s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 13.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.77it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.37it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.63it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.72it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.32s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.06it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.66s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.63s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.19it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.60it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.53it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.98it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.06it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.86s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.58s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.42s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.08s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12.34it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.72it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.50it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.70it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.06it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.42it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.48it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.34s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.90it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.20s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.69it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.49it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.35it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.55it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.00it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.57it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.20s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.73s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.37s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.02s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.02s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.01s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.28s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.89s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.19s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.68s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.08s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.76it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.95it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.10s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.87s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.28s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.11s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.51it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.99it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.05s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.84it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.05it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.19it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.42s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.90it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.48it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.88it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.38s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.46it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.05it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.51it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.56it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.02it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.53s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.79s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.30s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.18s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.52it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.02s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.35it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.06s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.09s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.34s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.95it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.08it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.34it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.99it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.34it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.43s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Par\u00e1metros TD3 - BASE: temperature=1.000, top_p=1.000\n",
            "Par\u00e1metros TD3 - LORA: temperature=0.100, top_p=1.000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 7.5 Obtener par\u00e1metros de decodificaci\u00f3n \"\u00f3ptimos\" de TD3\n",
        "# ==============================================================\n",
        "\n",
        "def get_mean_action(agent, env, n_samples=64):\n",
        "    obs = env.reset()\n",
        "    actions = []\n",
        "    for _ in range(n_samples):\n",
        "        action, _ = agent.predict(obs, deterministic=True)\n",
        "        # Convertir acci\u00f3n a array numpy y asegurar que tenga 2 elementos\n",
        "        action = np.asarray(action, dtype=np.float32)\n",
        "        if action.ndim == 0:  # Escalar\n",
        "            action = np.array([action, action], dtype=np.float32)\n",
        "        elif action.ndim == 1:\n",
        "            if len(action) == 1:\n",
        "                action = np.array([action[0], action[0]], dtype=np.float32)\n",
        "            elif len(action) > 2:\n",
        "                action = action[:2]\n",
        "        else:\n",
        "            # Multidimensional, aplanar y tomar primeros 2\n",
        "            action_flat = action.flatten()\n",
        "            if len(action_flat) >= 2:\n",
        "                action = action_flat[:2]\n",
        "            else:\n",
        "                action = np.array([action_flat[0], action_flat[0]], dtype=np.float32)\n",
        "\n",
        "        actions.append(action)\n",
        "\n",
        "        step_result = env.step(action)\n",
        "        obs = step_result[0]\n",
        "\n",
        "    actions = np.array(actions)\n",
        "    mean_action = actions.mean(axis=0)\n",
        "    temperature = float(np.clip(mean_action[0], 0.1, 1.0))\n",
        "    top_p = float(np.clip(mean_action[1], 0.1, 1.0))\n",
        "    return temperature, top_p\n",
        "\n",
        "temp_base, top_p_base = get_mean_action(td3_base, env_base)\n",
        "temp_lora, top_p_lora = get_mean_action(td3_lora, env_lora)\n",
        "\n",
        "print(f\"Par\u00e1metros TD3 - BASE: temperature={temp_base:.3f}, top_p={top_p_base:.3f}\")\n",
        "print(f\"Par\u00e1metros TD3 - LORA: temperature={temp_lora:.3f}, top_p={top_p_lora:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b8304f5df51b4f858918d7ab7e0a9190",
            "3228987561054dcdaae65a76d881cd92",
            "4451572a01764f7f800a1ae06c92acd1",
            "a09a54ee29974e1cb32fef5850096d48",
            "21e28b9d7bd947aaaad492158f0f63ec",
            "39a454e77efc47eb90617be4a9c64f20",
            "b5f3b21fee394fa291101d4fbfd828f5",
            "2c4ca07d38044478b374de1083234272",
            "20eb3fb649bd4544a1845dc9c553aaf3",
            "3882f9ba4c2e460db85188dcd401a61d",
            "843ddd739d1e47428d4de7608a755db5",
            "a5326ca86445415c9aa42500fbc66b2b",
            "9714cbe7c9f4434bbe2d8a5de29a8270",
            "cb044c30fae24d9d8e1ca776c04a2fb1",
            "abb7622bd2a049af8f16d08d518ac5c7",
            "700dcbc9309a49788603d48cbee40d96",
            "72b0f85a0c654ca995723beab6781459",
            "83608f47000d4c0f9312e529d5dc2004",
            "6ccb61f71c4c47748553ade717548693",
            "42e196fdb41e4c16827e5e3ca13f5ef7",
            "db1725544bd74b14a81984cff02649b0",
            "40bd09d0c8964d65ba584522b288558e"
          ]
        },
        "id": "j1E157qY96Ek",
        "outputId": "8d40f1dc-762f-40d2-e6cb-a1671155cb38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluando BASE + TD3 ===\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8304f5df51b4f858918d7ab7e0a9190",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating with TD3 params:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.49it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.36it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.66s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.10it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.78it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.53it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.35it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.72it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.23s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.99it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.80it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.05it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.30it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.42it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.45it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.19it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.54it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluando LORA + TD3 ===\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5326ca86445415c9aa42500fbc66b2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating with TD3 params:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.63it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.72s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.59it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.33it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.66it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.22it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.32it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.21s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.55it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.78it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.68it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.25it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.56it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.50it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.51it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.24s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BASE + TD3: {'bertscore_f1': 0.8428637425104777, 'factuality': 0.4228625352183978, 'flesch_reading_ease': 33.40290794075375}\n",
            "LORA + TD3: {'bertscore_f1': 0.8605288883050283, 'factuality': 0.6916326433420181, 'flesch_reading_ease': 31.04161867018751}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 7.6 Evaluar modelos con TD3 vs sin TD3\n",
        "# ==============================================================\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def evaluate_model_with_params(model, tokenizer, pairs, sample_size=30,\n",
        "                               temperature=0.7, top_p=0.9, max_new_tokens=256):\n",
        "    sample = pairs[:sample_size]\n",
        "    results = []\n",
        "    for pair in tqdm(sample, desc=\"Evaluating with TD3 params\"):\n",
        "        medical = pair.medical\n",
        "        reference = pair.plain\n",
        "        source = pair.medical\n",
        "\n",
        "        generated = generate_with_params(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            medical,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "\n",
        "        metrics = evaluator.evaluate(generated, reference, source)\n",
        "        results.append(metrics)\n",
        "\n",
        "    # Resumimos igualmente que en summarize_metrics\n",
        "    bert_f1 = np.mean([m[\"relevance\"][\"f1\"] for m in results])\n",
        "    fact = np.mean([m[\"factuality\"][\"score\"] for m in results])\n",
        "    fre = np.mean([m[\"readability\"][\"flesch_reading_ease\"] for m in results])\n",
        "\n",
        "    return {\n",
        "        \"bertscore_f1\": float(bert_f1),\n",
        "        \"factuality\": float(fact),\n",
        "        \"flesch_reading_ease\": float(fre),\n",
        "    }\n",
        "\n",
        "# Usamos el mismo eval_pairs y sample_size que antes\n",
        "sample_size = min(30, len(eval_pairs))\n",
        "\n",
        "print(\"\\n=== Evaluando BASE + TD3 ===\")\n",
        "BASE_TD3_METRICS = evaluate_model_with_params(\n",
        "    base_model,\n",
        "    base_tokenizer,\n",
        "    eval_pairs,\n",
        "    sample_size=sample_size,\n",
        "    temperature=temp_base,\n",
        "    top_p=top_p_base,\n",
        ")\n",
        "\n",
        "print(\"\\n=== Evaluando LORA + TD3 ===\")\n",
        "LORA_TD3_METRICS = evaluate_model_with_params(\n",
        "    model_lora,\n",
        "    base_tokenizer,\n",
        "    eval_pairs,\n",
        "    sample_size=sample_size,\n",
        "    temperature=temp_lora,\n",
        "    top_p=top_p_lora,\n",
        ")\n",
        "\n",
        "print(\"BASE + TD3:\", BASE_TD3_METRICS)\n",
        "print(\"LORA + TD3:\", LORA_TD3_METRICS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5XuIgYoYkSn",
        "outputId": "c5a8e522-908f-422a-bb4d-0bd42e0ae38f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== TABLA COMPARATIVA FINAL ===\n",
            "                               Base  Base + TD3       LoRA  LoRA + TD3\n",
            "BERTScore F1               0.872569    0.842864   0.863368    0.860529\n",
            "AlignScore (Factualidad)   0.623100    0.422863   0.669546    0.691633\n",
            "Flesch Reading Ease       37.960728   33.402908  34.804143   31.041619\n",
            "\n",
            "Diferencias (modelo - Base):\n",
            "\n",
            "BERTScore F1:\n",
            "  Base + TD3 - Base: -0.0297\n",
            "  LoRA - Base: -0.0092\n",
            "  LoRA + TD3 - Base: -0.0120\n",
            "\n",
            "AlignScore (Factualidad):\n",
            "  Base + TD3 - Base: -0.2002\n",
            "  LoRA - Base: +0.0464\n",
            "  LoRA + TD3 - Base: +0.0685\n",
            "\n",
            "Flesch Reading Ease:\n",
            "  Base + TD3 - Base: -4.5578\n",
            "  LoRA - Base: -3.1566\n",
            "  LoRA + TD3 - Base: -6.9191\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 7.7 Tabla comparativa final: Base vs LoRA vs TD3\n",
        "# ==============================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "metrics_table = {\n",
        "    \"BERTScore F1\": {\n",
        "        \"Base\": BASE_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        \"Base + TD3\": BASE_TD3_METRICS[\"bertscore_f1\"],\n",
        "        \"LoRA\": FINETUNED_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        \"LoRA + TD3\": LORA_TD3_METRICS[\"bertscore_f1\"],\n",
        "    },\n",
        "    \"AlignScore (Factualidad)\": {\n",
        "        \"Base\": BASE_MODEL_METRICS[\"factuality\"],\n",
        "        \"Base + TD3\": BASE_TD3_METRICS[\"factuality\"],\n",
        "        \"LoRA\": FINETUNED_MODEL_METRICS[\"factuality\"],\n",
        "        \"LoRA + TD3\": LORA_TD3_METRICS[\"factuality\"],\n",
        "    },\n",
        "    \"Flesch Reading Ease\": {\n",
        "        \"Base\": BASE_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        \"Base + TD3\": BASE_TD3_METRICS[\"flesch_reading_ease\"],\n",
        "        \"LoRA\": FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        \"LoRA + TD3\": LORA_TD3_METRICS[\"flesch_reading_ease\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(metrics_table).T\n",
        "print(\"\\n=== TABLA COMPARATIVA FINAL ===\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\nDiferencias (modelo - Base):\")\n",
        "for name, row in df.iterrows():\n",
        "    print(f\"\\n{name}:\")\n",
        "    for col in df.columns:\n",
        "        if col == \"Base\":\n",
        "            continue\n",
        "        delta = row[col] - row[\"Base\"]\n",
        "        print(f\"  {col} - Base: {delta:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a8138fa88a52430f985dd807fb8330d2",
            "6c36e80995c04ff6ac98fb14857b5857",
            "3a30bbe47210429a8370e9542bd6b6c3",
            "e2987148008543efb25cad8f6960888e",
            "e81bd513664046288516bee8a068fde3",
            "a259e5ab3230453eaab7c80be4d96072",
            "67179f65aaa04aad94465457dd026067",
            "a29818ddc06e4a7d8266c9695f7dbfef",
            "d3a17440c5374ab0b269ebaa3a0eecd4",
            "f06e134c6f1d47209817c57388fceb1d",
            "cb64e112941f451cb83b4427e0a64a98",
            "82be4deff6ea4edab5634d08f5cbfc31",
            "8397fcffb64e4f859f20a4760a368398",
            "a9d2a8ccf6f4434fa3fadee9cf601422",
            "8cbe475c9fe843fd8440174b3b45846b",
            "13860a1c9ad74ba6bb51f6b843a64d47",
            "a0701bba4615476c95298f91efe1f669",
            "88f37318b3034d9e939133a78b607d52",
            "3947f22bc9a74487ae8877476564953e",
            "c116cc22d13c4184ad9f90a73d0bb1cf",
            "b97539fb6cf146c98d8a9e365f34fbce",
            "97b9083079bb4348a64dfe67b1cc1b9a",
            "0dcd9606aa2d4fb7b1b512f810e04a9b",
            "dd497308e1ca433a928dec5f8aa9b61b",
            "4891a0ed4bf1484692a783e2174206c6",
            "b8cb9936ddc449babd3de8bfab209d0d",
            "8610ba8de29f4a92a4dafd54576fc439",
            "feab56b2538846ff940a2a5942d06949",
            "ab52e714e59c49ca8ed4a94923d27f84",
            "06d7b75ac17a4e55a3fc02f36a80fc92",
            "4d671cc44275493390e83a71abac0212",
            "bc9462c809d94e1489cb9c6220aebf86",
            "3d7353fbcb7242f99bc19eaf84aefcbe"
          ]
        },
        "id": "4xEybZvRlejQ",
        "outputId": "19c0c3dc-0fbe-47d9-fd5a-3c04fc16a583"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Anthropic configurado correctamente (key: sk-ant-api...kxQAA)\n",
            "================================================================================\n",
            "EVALUANDO APIs COMERCIALES\n",
            "================================================================================\n",
            "Tama\u00f1o de muestra: 30\n",
            "\n",
            "Evaluando Anthropic Claude...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a8138fa88a52430f985dd807fb8330d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Anthropic Claude:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.22it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.46it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.90it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.48it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.09it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.60it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.74it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.22it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 19.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.94it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.24it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.93it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.38it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.35it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.87it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.27it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.57it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.24it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.76it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.47it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anthropic Claude: {'bertscore_f1': 0.8310331463813782, 'factuality': 0.4041078880429268, 'flesch_reading_ease': 27.835454138059724, 'n_samples': 30, 'errors': 0}\n",
            "\n",
            "Evaluando Google Gemini...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82be4deff6ea4edab5634d08f5cbfc31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating Google Gemini:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            "Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\n",
            " 26 errores durante la evaluaci\u00f3n de Google Gemini\n",
            "Google Gemini: {'bertscore_f1': 0.8324978351593018, 'factuality': 0.49457748979330063, 'flesch_reading_ease': 40.757987128529614, 'n_samples': 4, 'errors': 26}\n",
            "\n",
            "Evaluando OpenAI GPT-4o...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0dcd9606aa2d4fb7b1b512f810e04a9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating OpenAI GPT-4o:   0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.92it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.25it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.32it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.60it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.46it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.72it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.19it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.68it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.57it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.59it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.88it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.95it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.09it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.95it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.99it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.77it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.24it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.33it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.24it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.15it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.09it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.75it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Evaluating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI GPT-4o: {'bertscore_f1': 0.8525808036327363, 'factuality': 0.6002819925546646, 'flesch_reading_ease': 49.569877976408506, 'n_samples': 30, 'errors': 0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# SECCI\u00d3N 8: EVALUACI\u00d3N DE APIs COMERCIALES\n",
        "# ==============================================================\n",
        "\n",
        "# Instalar librer\u00edas necesarias para APIs comerciales\n",
        "!pip install openai anthropic google-generativeai -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ==============================================================\n",
        "# CONFIGURAR API KEYS\n",
        "# ==============================================================\n",
        "# Buscar API keys primero en secretos de Colab, luego en variables de entorno\n",
        "API_KEY_ANTHROPIC = None\n",
        "API_KEY_GEMINI = None\n",
        "API_KEY_OPENAI = None\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    try:\n",
        "        API_KEY_ANTHROPIC = userdata.get(\"API_KEY_ANTHROPIC\")\n",
        "    except (KeyError, Exception):\n",
        "        pass\n",
        "    try:\n",
        "        API_KEY_GEMINI = userdata.get(\"API_KEY_GEMINI\")\n",
        "    except (KeyError, Exception):\n",
        "        pass\n",
        "    try:\n",
        "        API_KEY_OPENAI = userdata.get(\"API_KEY_OPENAI\")\n",
        "    except (KeyError, Exception):\n",
        "        pass\n",
        "except ImportError:\n",
        "    pass\n",
        "\n",
        "if not API_KEY_ANTHROPIC:\n",
        "    API_KEY_ANTHROPIC = os.getenv(\"API_KEY_ANTHROPIC\")\n",
        "if not API_KEY_GEMINI:\n",
        "    API_KEY_GEMINI = os.getenv(\"API_KEY_GEMINI\")\n",
        "if not API_KEY_OPENAI:\n",
        "    API_KEY_OPENAI = os.getenv(\"API_KEY_OPENAI\")\n",
        "\n",
        "# Verificar que las API keys est\u00e9n configuradas\n",
        "if not API_KEY_ANTHROPIC:\n",
        "    print(\" ADVERTENCIA: API_KEY_ANTHROPIC no est\u00e1 configurada\")\n",
        "if not API_KEY_GEMINI:\n",
        "    print(\" ADVERTENCIA: API_KEY_GEMINI no est\u00e1 configurada\")\n",
        "if not API_KEY_OPENAI:\n",
        "    print(\" ADVERTENCIA: API_KEY_OPENAI no est\u00e1 configurada\")\n",
        "\n",
        "# Importar clientes de APIs\n",
        "try:\n",
        "    import anthropic\n",
        "    # Validar y limpiar API key antes de crear el cliente\n",
        "    if API_KEY_ANTHROPIC:\n",
        "        API_KEY_ANTHROPIC = API_KEY_ANTHROPIC.strip()\n",
        "        if API_KEY_ANTHROPIC.startswith('sk-ant-'):\n",
        "            try:\n",
        "                client_anthropic = anthropic.Anthropic(api_key=API_KEY_ANTHROPIC)\n",
        "                print(f\" Anthropic configurado correctamente (key: {API_KEY_ANTHROPIC[:10]}...{API_KEY_ANTHROPIC[-5:]})\")\n",
        "            except Exception as e:\n",
        "                client_anthropic = None\n",
        "                print(f\" Error al inicializar Anthropic: {e}\")\n",
        "        else:\n",
        "            client_anthropic = None\n",
        "            print(f\" ADVERTENCIA: API_KEY_ANTHROPIC no tiene el formato correcto (debe empezar con 'sk-ant-'). Valor recibido: {API_KEY_ANTHROPIC[:20]}...\")\n",
        "    else:\n",
        "        client_anthropic = None\n",
        "except ImportError:\n",
        "    client_anthropic = None\n",
        "    print(\" No se pudo importar anthropic\")\n",
        "\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    if API_KEY_GEMINI:\n",
        "        genai.configure(api_key=API_KEY_GEMINI)\n",
        "        model_gemini = genai.GenerativeModel('gemini-2.5-flash')\n",
        "    else:\n",
        "        model_gemini = None\n",
        "except ImportError:\n",
        "    model_gemini = None\n",
        "    print(\" No se pudo importar google.generativeai\")\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    client_openai = OpenAI(api_key=API_KEY_OPENAI) if API_KEY_OPENAI else None\n",
        "except ImportError:\n",
        "    client_openai = None\n",
        "    print(\" No se pudo importar openai\")\n",
        "\n",
        "# Funci\u00f3n para construir el prompt (mismo que se usa localmente)\n",
        "def build_api_prompt(medical_text: str) -> str:\n",
        "    \"\"\"Construye el prompt para las APIs comerciales (mismo formato que localmente).\"\"\"\n",
        "    return (\n",
        "        \"You are a specialist in healthcare communication. \"\n",
        "        \"Use the context to transform the following medical text into a clear, \"\n",
        "        \"concise, and easy-to-understand summary for a patient and their family. \"\n",
        "        \"Retain all relevant clinical data, but explain technical terms using simple \"\n",
        "        \"language and short sentences.\\n\\n\"\n",
        "        \"### Medical text:\\n\"\n",
        "        f\"{medical_text}\\n\\n\"\n",
        "        \"### Simplified summary:\\n\"\n",
        "    )\n",
        "\n",
        "# Funciones para generar res\u00famenes con cada API\n",
        "def generate_anthropic(medical_text: str, max_retries: int = 3) -> Optional[str]:\n",
        "    \"\"\"Genera resumen usando Anthropic Claude.\"\"\"\n",
        "    if not client_anthropic:\n",
        "        return None\n",
        "\n",
        "    prompt = build_api_prompt(medical_text)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            message = client_anthropic.messages.create(\n",
        "                model=\"claude-sonnet-4-20250514\",\n",
        "                max_tokens=1024,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            response = message.content[0].text\n",
        "            # Extraer solo el resumen si hay marcadores\n",
        "            if \"### Simplified summary:\" in response:\n",
        "                return response.split(\"### Simplified summary:\")[-1].strip()\n",
        "            return response.strip()\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "                continue\n",
        "            print(f\"Error en Anthropic: {e}\")\n",
        "            return None\n",
        "\n",
        "def generate_gemini(medical_text: str, max_retries: int = 3) -> Optional[str]:\n",
        "    \"\"\"Genera resumen usando Google Gemini.\"\"\"\n",
        "    if not model_gemini:\n",
        "        return None\n",
        "\n",
        "    prompt = build_api_prompt(medical_text)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model_gemini.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    max_output_tokens=2048,\n",
        "                    temperature=0.7,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            # Verificar finish_reason antes de acceder a response.text\n",
        "            if response.candidates and len(response.candidates) > 0:\n",
        "                candidate = response.candidates[0]\n",
        "                finish_reason = candidate.finish_reason\n",
        "\n",
        "                # finish_reason 2 = MAX_TOKENS, 3 = SAFETY, 4 = RECITATION\n",
        "                if finish_reason in [2, 3, 4]:\n",
        "                    if finish_reason == 2:\n",
        "                        print(f\"Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2)\")\n",
        "                    elif finish_reason == 3:\n",
        "                        print(f\"Warning: Gemini bloque\u00f3 contenido por seguridad (finish_reason=3)\")\n",
        "                    elif finish_reason == 4:\n",
        "                        print(f\"Warning: Gemini bloque\u00f3 contenido por recitaci\u00f3n (finish_reason=4)\")\n",
        "                    if candidate.content and candidate.content.parts:\n",
        "                        text = candidate.content.parts[0].text\n",
        "                        if text and len(text.strip()) > 10:\n",
        "                            if \"### Simplified summary:\" in text:\n",
        "                                return text.split(\"### Simplified summary:\")[-1].strip()\n",
        "                            return text.strip()\n",
        "                    return None\n",
        "\n",
        "                # finish_reason 1 = STOP\n",
        "                if finish_reason == 1 and candidate.content and candidate.content.parts:\n",
        "                    text = candidate.content.parts[0].text\n",
        "                    if text:\n",
        "                        if \"### Simplified summary:\" in text:\n",
        "                            return text.split(\"### Simplified summary:\")[-1].strip()\n",
        "                        return text.strip()\n",
        "            try:\n",
        "                text = response.text\n",
        "                if text:\n",
        "                    if \"### Simplified summary:\" in text:\n",
        "                        return text.split(\"### Simplified summary:\")[-1].strip()\n",
        "                    return text.strip()\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "                continue\n",
        "            print(f\"Error en Gemini: {e}\")\n",
        "            return None\n",
        "\n",
        "def generate_openai(medical_text: str, max_retries: int = 3) -> Optional[str]:\n",
        "    \"\"\"Genera resumen usando OpenAI GPT.\"\"\"\n",
        "    if not client_openai:\n",
        "        return None\n",
        "\n",
        "    prompt = build_api_prompt(medical_text)\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client_openai.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a specialist in healthcare communication.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=1024,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "            text = response.choices[0].message.content\n",
        "\n",
        "            if \"### Simplified summary:\" in text:\n",
        "                return text.split(\"### Simplified summary:\")[-1].strip()\n",
        "            return text.strip()\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "                continue\n",
        "            print(f\"Error en OpenAI: {e}\")\n",
        "            return None\n",
        "\n",
        "# Funci\u00f3n para evaluar una API\n",
        "def evaluate_api(generate_fn, api_name: str, pairs, sample_size: int = 30):\n",
        "    \"\"\"Eval\u00faa una API comercial con las mismas m\u00e9tricas.\"\"\"\n",
        "    sample = pairs[:sample_size]\n",
        "    results = []\n",
        "    errors = 0\n",
        "\n",
        "    print(f\"\\nEvaluando {api_name}...\")\n",
        "    for i, pair in enumerate(tqdm(sample, desc=f\"Evaluating {api_name}\")):\n",
        "        medical = pair.medical\n",
        "        reference = pair.plain\n",
        "        source = pair.medical\n",
        "\n",
        "        generated = generate_fn(medical)\n",
        "\n",
        "        if generated is None or len(generated.strip()) < 10:\n",
        "            errors += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            metrics = evaluator.evaluate(generated, reference, source)\n",
        "            results.append(metrics)\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluando muestra {i}: {e}\")\n",
        "            errors += 1\n",
        "            continue\n",
        "\n",
        "    if not results:\n",
        "        print(f\" No se pudieron evaluar muestras para {api_name}\")\n",
        "        return None\n",
        "\n",
        "    if errors > 0:\n",
        "        print(f\" {errors} errores durante la evaluaci\u00f3n de {api_name}\")\n",
        "\n",
        "    # Resumir m\u00e9tricas\n",
        "    bert_f1 = np.mean([m[\"relevance\"][\"f1\"] for m in results])\n",
        "    fact = np.mean([m[\"factuality\"][\"score\"] for m in results])\n",
        "    fre = np.mean([m[\"readability\"][\"flesch_reading_ease\"] for m in results])\n",
        "\n",
        "    return {\n",
        "        \"bertscore_f1\": float(bert_f1),\n",
        "        \"factuality\": float(fact),\n",
        "        \"flesch_reading_ease\": float(fre),\n",
        "        \"n_samples\": len(results),\n",
        "        \"errors\": errors,\n",
        "    }\n",
        "\n",
        "# Evaluar todas las APIs comerciales\n",
        "sample_size = min(30, len(eval_pairs))\n",
        "print(\"=\"*80)\n",
        "print(\"EVALUANDO APIs COMERCIALES\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Tama\u00f1o de muestra: {sample_size}\")\n",
        "\n",
        "ANTHROPIC_METRICS = None\n",
        "GEMINI_METRICS = None\n",
        "OPENAI_METRICS = None\n",
        "\n",
        "if client_anthropic:\n",
        "    ANTHROPIC_METRICS = evaluate_api(generate_anthropic, \"Anthropic Claude\", eval_pairs, sample_size)\n",
        "    if ANTHROPIC_METRICS:\n",
        "        print(f\"Anthropic Claude: {ANTHROPIC_METRICS}\")\n",
        "else:\n",
        "    print(\" Anthropic no disponible (API key no configurada)\")\n",
        "\n",
        "if model_gemini:\n",
        "    GEMINI_METRICS = evaluate_api(generate_gemini, \"Google Gemini\", eval_pairs, sample_size)\n",
        "    if GEMINI_METRICS:\n",
        "        print(f\"Google Gemini: {GEMINI_METRICS}\")\n",
        "else:\n",
        "    print(\" Gemini no disponible (API key no configurada)\")\n",
        "\n",
        "if client_openai:\n",
        "    OPENAI_METRICS = evaluate_api(generate_openai, \"OpenAI GPT-4o\", eval_pairs, sample_size)\n",
        "    if OPENAI_METRICS:\n",
        "        print(f\"OpenAI GPT-4o: {OPENAI_METRICS}\")\n",
        "else:\n",
        "    print(\" OpenAI no disponible (API key no configurada)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkZfms6xy0as",
        "outputId": "f8366459-473a-4c7b-8bf0-2d5d7a0bbcb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TABLA COMPARATIVA FINAL: MODELOS LOCALES vs APIs COMERCIALES\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "                               Base  Base + TD3       LoRA  LoRA + TD3  Anthropic Claude  Google Gemini  OpenAI GPT-4o\n",
            "BERTScore F1               0.872569    0.842864   0.863368    0.860529          0.831033       0.832498       0.852581\n",
            "AlignScore (Factualidad)   0.623100    0.422863   0.669546    0.691633          0.404108       0.494577       0.600282\n",
            "Flesch Reading Ease       37.960728   33.402908  34.804143   31.041619         27.835454      40.757987      49.569878\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "DIFERENCIAS RESPECTO AL MODELO BASE\n",
            "================================================================================\n",
            "\n",
            "BERTScore F1:\n",
            "  Base + TD3               : 0.8429 (\u0394 -0.0297)\n",
            "  LoRA                     : 0.8634 (\u0394 -0.0092)\n",
            "  LoRA + TD3               : 0.8605 (\u0394 -0.0120)\n",
            "  Anthropic Claude         : 0.8310 (\u0394 -0.0415)\n",
            "  Google Gemini            : 0.8325 (\u0394 -0.0401)\n",
            "  OpenAI GPT-4o            : 0.8526 (\u0394 -0.0200)\n",
            "\n",
            "AlignScore (Factualidad):\n",
            "  Base + TD3               : 0.4229 (\u0394 -0.2002)\n",
            "  LoRA                     : 0.6695 (\u0394 +0.0464)\n",
            "  LoRA + TD3               : 0.6916 (\u0394 +0.0685)\n",
            "  Anthropic Claude         : 0.4041 (\u0394 -0.2190)\n",
            "  Google Gemini            : 0.4946 (\u0394 -0.1285)\n",
            "  OpenAI GPT-4o            : 0.6003 (\u0394 -0.0228)\n",
            "\n",
            "Flesch Reading Ease:\n",
            "  Base + TD3               : 33.4029 (\u0394 -4.5578)\n",
            "  LoRA                     : 34.8041 (\u0394 -3.1566)\n",
            "  LoRA + TD3               : 31.0416 (\u0394 -6.9191)\n",
            "  Anthropic Claude         : 27.8355 (\u0394 -10.1253)\n",
            "  Google Gemini            : 40.7580 (\u0394 +2.7973)\n",
            "  OpenAI GPT-4o            : 49.5699 (\u0394 +11.6092)\n",
            "\n",
            "================================================================================\n",
            "MEJORES MODELOS POR M\u00c9TRICA\n",
            "================================================================================\n",
            "BERTScore F1                  : Base                      (0.8726)\n",
            "AlignScore (Factualidad)      : LoRA + TD3                (0.6916)\n",
            "Flesch Reading Ease           : OpenAI GPT-4o             (49.5699)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 8.1 Tabla comparativa final: Modelos locales vs APIs comerciales\n",
        "# ==============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TABLA COMPARATIVA FINAL: MODELOS LOCALES vs APIs COMERCIALES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Construir tabla de m\u00e9tricas\n",
        "metrics_table = {\n",
        "    \"BERTScore F1\": {},\n",
        "    \"AlignScore (Factualidad)\": {},\n",
        "    \"Flesch Reading Ease\": {},\n",
        "}\n",
        "\n",
        "# Agregar modelos locales\n",
        "metrics_table[\"BERTScore F1\"][\"Base\"] = BASE_MODEL_METRICS[\"bertscore_f1\"]\n",
        "metrics_table[\"BERTScore F1\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"bertscore_f1\"]\n",
        "metrics_table[\"BERTScore F1\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"bertscore_f1\"]\n",
        "metrics_table[\"BERTScore F1\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"bertscore_f1\"]\n",
        "\n",
        "metrics_table[\"AlignScore (Factualidad)\"][\"Base\"] = BASE_MODEL_METRICS[\"factuality\"]\n",
        "metrics_table[\"AlignScore (Factualidad)\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"factuality\"]\n",
        "metrics_table[\"AlignScore (Factualidad)\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"factuality\"]\n",
        "metrics_table[\"AlignScore (Factualidad)\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"factuality\"]\n",
        "\n",
        "metrics_table[\"Flesch Reading Ease\"][\"Base\"] = BASE_MODEL_METRICS[\"flesch_reading_ease\"]\n",
        "metrics_table[\"Flesch Reading Ease\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "metrics_table[\"Flesch Reading Ease\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"]\n",
        "metrics_table[\"Flesch Reading Ease\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "# Agregar APIs comerciales si est\u00e1n disponibles\n",
        "if ANTHROPIC_METRICS:\n",
        "    metrics_table[\"BERTScore F1\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if GEMINI_METRICS:\n",
        "    metrics_table[\"BERTScore F1\"][\"Google Gemini\"] = GEMINI_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"Google Gemini\"] = GEMINI_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"Google Gemini\"] = GEMINI_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if OPENAI_METRICS:\n",
        "    metrics_table[\"BERTScore F1\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "# Crear DataFrame y mostrar\n",
        "df_final = pd.DataFrame(metrics_table).T\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(df_final.to_string())\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Mostrar diferencias respecto al modelo base\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DIFERENCIAS RESPECTO AL MODELO BASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for name, row in df_final.iterrows():\n",
        "    print(f\"\\n{name}:\")\n",
        "    base_value = row.get(\"Base\", None)\n",
        "    if base_value is None:\n",
        "        continue\n",
        "    for col in df_final.columns:\n",
        "        if col == \"Base\":\n",
        "            continue\n",
        "        if col in row and pd.notna(row[col]):\n",
        "            delta = row[col] - base_value\n",
        "            print(f\"  {col:25s}: {row[col]:.4f} (\u0394 {delta:+.4f})\")\n",
        "\n",
        "# Resumen de mejores modelos\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MEJORES MODELOS POR M\u00c9TRICA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for metric in df_final.index:\n",
        "    row = df_final.loc[metric]\n",
        "    # Excluir NaN y encontrar el m\u00e1ximo\n",
        "    valid_values = row.dropna()\n",
        "    if len(valid_values) > 0:\n",
        "        best_model = valid_values.idxmax()\n",
        "        best_value = valid_values.max()\n",
        "        print(f\"{metric:30s}: {best_model:25s} ({best_value:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nrhh1aoCy3fp",
        "outputId": "4a4c90df-921a-4500-9fd6-973916b23b28"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1800x600 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1600x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "GR\u00c1FICAS GENERADAS\n",
            "================================================================================\n",
            "\u2713 Gr\u00e1fica 1: Comparaci\u00f3n por m\u00e9trica (3 subplots)\n",
            "\u2713 Gr\u00e1fica 2: Comparaci\u00f3n combinada (todas las m\u00e9tricas)\n",
            "\n",
            "Nota: Flesch Reading Ease est\u00e1 normalizado (\u00f710) en la gr\u00e1fica combinada para mejor visualizaci\u00f3n.\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 8.2 Gr\u00e1ficas comparativas: Modelos locales vs APIs comerciales\n",
        "# ==============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ==============================================================\n",
        "# SELECCIONAR MODELO LOCAL A COMPARAR\n",
        "# ==============================================================\n",
        "\n",
        "MODELO_LOCAL_SELECCIONADO = \"LoRA\"  # Se escoge el el mejor modelo con el mejor balance\n",
        "\n",
        "# Preparar datos para las gr\u00e1ficas\n",
        "metrics_data = {\n",
        "    \"BERTScore F1\": {\n",
        "        # \"Base\": BASE_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        # \"Base + TD3\": BASE_TD3_METRICS[\"bertscore_f1\"],\n",
        "        # \"LoRA\": FINETUNED_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        # \"LoRA + TD3\": LORA_TD3_METRICS[\"bertscore_f1\"],\n",
        "    },\n",
        "    \"AlignScore (Factualidad)\": {\n",
        "        # \"Base\": BASE_MODEL_METRICS[\"factuality\"],\n",
        "        # \"Base + TD3\": BASE_TD3_METRICS[\"factuality\"],\n",
        "        # \"LoRA\": FINETUNED_MODEL_METRICS[\"factuality\"],\n",
        "        # \"LoRA + TD3\": LORA_TD3_METRICS[\"factuality\"],\n",
        "    },\n",
        "    \"Flesch Reading Ease\": {\n",
        "        # \"Base\": BASE_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        # \"Base + TD3\": BASE_TD3_METRICS[\"flesch_reading_ease\"],\n",
        "        # \"LoRA\": FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        # \"LoRA + TD3\": LORA_TD3_METRICS[\"flesch_reading_ease\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Agregar el modelo local seleccionado\n",
        "if MODELO_LOCAL_SELECCIONADO == \"Base\":\n",
        "    metrics_data[\"BERTScore F1\"][\"Base\"] = BASE_MODEL_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"Base\"] = BASE_MODEL_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"Base\"] = BASE_MODEL_METRICS[\"flesch_reading_ease\"]\n",
        "elif MODELO_LOCAL_SELECCIONADO == \"Base + TD3\":\n",
        "    metrics_data[\"BERTScore F1\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "elif MODELO_LOCAL_SELECCIONADO == \"LoRA\":\n",
        "    metrics_data[\"BERTScore F1\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"]\n",
        "elif MODELO_LOCAL_SELECCIONADO == \"LoRA + TD3\":\n",
        "    metrics_data[\"BERTScore F1\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "# Agregar APIs comerciales si est\u00e1n disponibles\n",
        "if ANTHROPIC_METRICS:\n",
        "    metrics_data[\"BERTScore F1\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if GEMINI_METRICS:\n",
        "    metrics_data[\"BERTScore F1\"][\"Google Gemini\"] = GEMINI_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"Google Gemini\"] = GEMINI_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"Google Gemini\"] = GEMINI_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if OPENAI_METRICS:\n",
        "    metrics_data[\"BERTScore F1\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "# Definir colores para cada modelo\n",
        "model_colors = {\n",
        "    \"Base\": \"#8B7355\",  # Beige/Cream\n",
        "    \"Base + TD3\": \"#4A90E2\",  # Blue\n",
        "    \"LoRA\": \"#D3D3D3\",  # Light Gray\n",
        "    \"LoRA + TD3\": \"#FF6B35\",  # Red/Orange gradient\n",
        "    \"Anthropic Claude\": \"#2E7D32\",  # Green\n",
        "    \"Google Gemini\": \"#4285F4\",  # Google Blue\n",
        "    \"OpenAI GPT-4o\": \"#10A37F\",  # OpenAI Green\n",
        "}\n",
        "\n",
        "# Orden de modelos para la gr\u00e1fica (solo el modelo local seleccionado + APIs comerciales)\n",
        "model_order = [MODELO_LOCAL_SELECCIONADO]\n",
        "if ANTHROPIC_METRICS:\n",
        "    model_order.append(\"Anthropic Claude\")\n",
        "if GEMINI_METRICS:\n",
        "    model_order.append(\"Google Gemini\")\n",
        "if OPENAI_METRICS:\n",
        "    model_order.append(\"OpenAI GPT-4o\")\n",
        "\n",
        "# Crear figura con subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "fig.suptitle(f\"Comparaci\u00f3n de M\u00e9tricas: {MODELO_LOCAL_SELECCIONADO} vs APIs Comerciales\",\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "# Colores para las barras\n",
        "colors = [model_colors.get(model, \"#808080\") for model in model_order]\n",
        "\n",
        "# Graficar cada m\u00e9trica\n",
        "for idx, (metric_name, metric_values) in enumerate(metrics_data.items()):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Obtener valores en el orden correcto\n",
        "    values = [metric_values.get(model, np.nan) for model in model_order]\n",
        "\n",
        "    # Crear barras\n",
        "    bars = ax.bar(range(len(model_order)), values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "\n",
        "    # Agregar valores en las barras\n",
        "    for i, (bar, val) in enumerate(zip(bars, values)):\n",
        "        if not np.isnan(val):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{val:.2f}',\n",
        "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "    # Configurar ejes\n",
        "    ax.set_xticks(range(len(model_order)))\n",
        "    ax.set_xticklabels(model_order, rotation=45, ha='right', fontsize=10)\n",
        "    ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(metric_name, fontsize=12, fontweight='bold', pad=10)\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax.set_ylim(0, max([v for v in values if not np.isnan(v)]) * 1.15 if any(not np.isnan(v) for v in values) else 100)\n",
        "\n",
        "    # Agregar l\u00ednea de referencia en el valor del modelo Base\n",
        "    if \"Base\" in metric_values and not np.isnan(metric_values[\"Base\"]):\n",
        "        ax.axhline(y=metric_values[\"Base\"], color='gray', linestyle=':', linewidth=1.5, alpha=0.7, label='Base')\n",
        "        if idx == 0:  # Solo mostrar leyenda en el primer subplot\n",
        "            ax.legend(loc='upper right', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gr\u00e1fica combinada (todas las m\u00e9tricas en una sola)\n",
        "fig2, ax2 = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "x = np.arange(len(model_order))\n",
        "width = 0.25  # Ancho de las barras\n",
        "\n",
        "# Posiciones de las barras para cada m\u00e9trica\n",
        "x1 = x - width\n",
        "x2 = x\n",
        "x3 = x + width\n",
        "\n",
        "# Valores para cada m\u00e9trica\n",
        "y1 = [metrics_data[\"BERTScore F1\"].get(model, np.nan) for model in model_order]\n",
        "y2 = [metrics_data[\"AlignScore (Factualidad)\"].get(model, np.nan) for model in model_order]\n",
        "y3 = [metrics_data[\"Flesch Reading Ease\"].get(model, np.nan) for model in model_order]\n",
        "\n",
        "# Normalizar Flesch Reading Ease para mejor visualizaci\u00f3n (dividir por 10)\n",
        "y3_norm = [v / 10 if not np.isnan(v) else np.nan for v in y3]\n",
        "\n",
        "# Crear barras\n",
        "bars1 = ax2.bar(x1, y1, width, label='BERTScore F1', color='#FF6B35', alpha=0.8, edgecolor='black', linewidth=1)\n",
        "bars2 = ax2.bar(x2, y2, width, label='AlignScore (Factualidad)', color='#4A90E2', alpha=0.8, edgecolor='black', linewidth=1)\n",
        "bars3 = ax2.bar(x3, y3_norm, width, label='Flesch Reading Ease (\u00f710)', color='#2E7D32', alpha=0.8, edgecolor='black', linewidth=1)\n",
        "\n",
        "# Agregar valores en las barras\n",
        "for bars, values, norm_factor in [(bars1, y1, 1), (bars2, y2, 1), (bars3, y3, 10)]:\n",
        "    for bar, val in zip(bars, values):\n",
        "        if not np.isnan(val):\n",
        "            height = bar.get_height()\n",
        "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{val:.2f}',\n",
        "                    ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "# Configurar ejes\n",
        "ax2.set_xlabel('Modelos', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "ax2.set_title(f'Comparaci\u00f3n Completa de M\u00e9tricas: {MODELO_LOCAL_SELECCIONADO} vs APIs Comerciales',\n",
        "              fontsize=14, fontweight='bold', pad=15)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(model_order, rotation=45, ha='right', fontsize=10)\n",
        "ax2.legend(loc='upper left', fontsize=10)\n",
        "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GR\u00c1FICAS GENERADAS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\u2713 Gr\u00e1fica 1: Comparaci\u00f3n por m\u00e9trica (3 subplots)\")\n",
        "print(\"\u2713 Gr\u00e1fica 2: Comparaci\u00f3n combinada (todas las m\u00e9tricas)\")\n",
        "print(\"\\nNota: Flesch Reading Ease est\u00e1 normalizado (\u00f710) en la gr\u00e1fica combinada para mejor visualizaci\u00f3n.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NoyD3Odeni3r",
        "outputId": "1101bc07-144f-4d5b-acb1-04a1009b1df2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DESCARGAR MODELOS DESDE COLAB\n",
            "================================================================================\n",
            " Modelo LoRA encontrado en: /content/qwen2.5-3b-pls\n",
            " Agente TD3 Base encontrado en: /content/td3_base_agent\n",
            " Agente TD3 LoRA encontrado en: /content/td3_lora_agent\n",
            "\n",
            " Se encontraron 3 modelo(s) para descargar.\n",
            "\n",
            "================================================================================\n",
            "OPCI\u00d3N 1: DESCARGAR COMO ARCHIVO ZIP\n",
            "================================================================================\n",
            "Agregando LoRA (Finetuning)...\n",
            "Agregando TD3 Base...\n",
            "Agregando TD3 LoRA...\n",
            "\n",
            "\u2713 ZIP creado: /content/modelos_qwen2.5_3b.zip\n",
            "  Tama\u00f1o: 11.05 MB\n",
            "\n",
            "\u2b07 Descargando ZIP...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_25b52f15-9328-4254-a8af-750b8e908f17\", \"modelos_qwen2.5_3b.zip\", 11587108)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u2713 Descarga iniciada. El archivo se descargar\u00e1 autom\u00e1ticamente.\n",
            "\n",
            "================================================================================\n",
            "OPCI\u00d3N 2: SUBIR A GOOGLE DRIVE (Opcional)\n",
            "================================================================================\n",
            "Si el archivo ZIP es muy grande, puedes subirlo a Google Drive.\n",
            "Descomenta las siguientes l\u00edneas para usar esta opci\u00f3n:\n",
            "\n",
            "# Montar Google Drive\n",
            "# drive.mount('/content/drive')\n",
            "#\n",
            "# Copiar ZIP a Drive\n",
            "# drive_dest = '/content/drive/MyDrive/modelos_qwen2.5_3b.zip'\n",
            "# shutil.copy(ZIP_OUTPUT, drive_dest)\n",
            "# print(f'\u2713 Archivo copiado a: {drive_dest}')\n",
            "\n",
            "================================================================================\n",
            "OPCI\u00d3N 3: DESCARGAR MODELOS INDIVIDUALES (Solo para archivos peque\u00f1os)\n",
            "================================================================================\n",
            "Nota: Esta opci\u00f3n puede fallar si los archivos son muy grandes.\n",
            "Recomendamos usar la OPCI\u00d3N 1 (ZIP) en su lugar.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "RESUMEN\n",
            "================================================================================\n",
            "\u2713 ZIP creado con 3 modelo(s)\n",
            "\u2713 Ubicaci\u00f3n: /content/modelos_qwen2.5_3b.zip\n",
            "\u2713 Tama\u00f1o: 11.05 MB\n",
            "\n",
            " Para usar los modelos en otro lugar:\n",
            "   1. Descarga el archivo ZIP\n",
            "   2. Extrae el contenido\n",
            "   3. Carga los modelos usando:\n",
            "      - LoRA: model_lora = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
            "      - TD3: td3_agent = TD3.load(TD3_DIR, env=env)\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 9. DESCARGAR MODELOS DESDE COLAB\n",
        "# ==============================================================\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files, drive\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DESCARGAR MODELOS DESDE COLAB\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Definir rutas (deben coincidir con las usadas en el notebook)\n",
        "ADAPTER_DIR = \"/content/qwen2.5-3b-pls\"\n",
        "TD3_BASE_DIR = \"/content/td3_base_agent\"\n",
        "TD3_LORA_DIR = \"/content/td3_lora_agent\"\n",
        "\n",
        "# Verificar qu\u00e9 modelos existen\n",
        "modelos_disponibles = []\n",
        "\n",
        "if os.path.exists(ADAPTER_DIR):\n",
        "    modelos_disponibles.append((\"LoRA (Finetuning)\", ADAPTER_DIR))\n",
        "    print(f\" Modelo LoRA encontrado en: {ADAPTER_DIR}\")\n",
        "else:\n",
        "    print(f\" Modelo LoRA NO encontrado en: {ADAPTER_DIR}\")\n",
        "\n",
        "if os.path.exists(TD3_BASE_DIR):\n",
        "    modelos_disponibles.append((\"TD3 Base\", TD3_BASE_DIR))\n",
        "    print(f\" Agente TD3 Base encontrado en: {TD3_BASE_DIR}\")\n",
        "else:\n",
        "    print(f\" Agente TD3 Base NO encontrado en: {TD3_BASE_DIR}\")\n",
        "\n",
        "if os.path.exists(TD3_LORA_DIR):\n",
        "    modelos_disponibles.append((\"TD3 LoRA\", TD3_LORA_DIR))\n",
        "    print(f\" Agente TD3 LoRA encontrado en: {TD3_LORA_DIR}\")\n",
        "else:\n",
        "    print(f\" Agente TD3 LoRA NO encontrado en: {TD3_LORA_DIR}\")\n",
        "\n",
        "if not modelos_disponibles:\n",
        "    print(\"\\n No se encontraron modelos para descargar.\")\n",
        "    print(\"Aseg\u00farate de haber ejecutado las celdas de entrenamiento primero.\")\n",
        "else:\n",
        "    print(f\"\\n Se encontraron {len(modelos_disponibles)} modelo(s) para descargar.\")\n",
        "\n",
        "    # ==============================================================\n",
        "    # OPCI\u00d3N 1: Descargar como ZIP\n",
        "    # ==============================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPCI\u00d3N 1: DESCARGAR COMO ARCHIVO ZIP\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    ZIP_OUTPUT = \"/content/modelos_qwen2.5_3b.zip\"\n",
        "\n",
        "    # Crear ZIP con todos los modelos\n",
        "    with zipfile.ZipFile(ZIP_OUTPUT, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for nombre, ruta in modelos_disponibles:\n",
        "            if os.path.exists(ruta):\n",
        "                print(f\"Agregando {nombre}...\")\n",
        "                # Agregar todo el directorio al ZIP\n",
        "                for root, dirs, filenames in os.walk(ruta):\n",
        "                    for filename in filenames:\n",
        "                        file_path = os.path.join(root, filename)\n",
        "                        # Mantener estructura de directorios en el ZIP\n",
        "                        arcname = os.path.relpath(file_path, os.path.dirname(ruta))\n",
        "                        arcname = os.path.join(os.path.basename(ruta), arcname)\n",
        "                        zipf.write(file_path, arcname)\n",
        "\n",
        "    # Obtener tama\u00f1o del archivo\n",
        "    zip_size_mb = os.path.getsize(ZIP_OUTPUT) / (1024 * 1024)\n",
        "    print(f\"\\n\u2713 ZIP creado: {ZIP_OUTPUT}\")\n",
        "    print(f\"  Tama\u00f1o: {zip_size_mb:.2f} MB\")\n",
        "\n",
        "    # Descargar el ZIP\n",
        "    print(\"\\n\u2b07 Descargando ZIP...\")\n",
        "    files.download(ZIP_OUTPUT)\n",
        "    print(\"\u2713 Descarga iniciada. El archivo se descargar\u00e1 autom\u00e1ticamente.\")\n",
        "\n",
        "    # ==============================================================\n",
        "    # OPCI\u00d3N 2: Subir a Google Drive (para archivos muy grandes)\n",
        "    # ==============================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPCI\u00d3N 2: SUBIR A GOOGLE DRIVE (Opcional)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Si el archivo ZIP es muy grande, puedes subirlo a Google Drive.\")\n",
        "    print(\"Descomenta las siguientes l\u00edneas para usar esta opci\u00f3n:\\n\")\n",
        "\n",
        "    print(\"# Montar Google Drive\")\n",
        "    print(\"# drive.mount('/content/drive')\")\n",
        "    print(\"#\")\n",
        "    print(\"# Copiar ZIP a Drive\")\n",
        "    print(\"# drive_dest = '/content/drive/MyDrive/modelos_qwen2.5_3b.zip'\")\n",
        "    print(\"# shutil.copy(ZIP_OUTPUT, drive_dest)\")\n",
        "    print(\"# print(f'\u2713 Archivo copiado a: {drive_dest}')\")\n",
        "\n",
        "    # ==============================================================\n",
        "    # OPCI\u00d3N 3: Descargar modelos individuales\n",
        "    # ==============================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPCI\u00d3N 3: DESCARGAR MODELOS INDIVIDUALES (Solo para archivos peque\u00f1os)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Nota: Esta opci\u00f3n puede fallar si los archivos son muy grandes.\")\n",
        "    print(\"Recomendamos usar la OPCI\u00d3N 1 (ZIP) en su lugar.\\n\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESUMEN\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\u2713 ZIP creado con {len(modelos_disponibles)} modelo(s)\")\n",
        "    print(f\"\u2713 Ubicaci\u00f3n: {ZIP_OUTPUT}\")\n",
        "    print(f\"\u2713 Tama\u00f1o: {zip_size_mb:.2f} MB\")\n",
        "    print(\"\\n Para usar los modelos en otro lugar:\")\n",
        "    print(\"   1. Descarga el archivo ZIP\")\n",
        "    print(\"   2. Extrae el contenido\")\n",
        "    print(\"   3. Carga los modelos usando:\")\n",
        "    print(\"      - LoRA: model_lora = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\")\n",
        "    print(\"      - TD3: td3_agent = TD3.load(TD3_DIR, env=env)\")\n",
        "    print(\"\\n\" + \"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "input_path = \"notebook.ipynb\"\n",
        "output_path = \"notebook_clean.ipynb\"\n",
        "\n",
        "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# Remove widget metadata safely\n",
        "if \"widgets\" in nb.get(\"metadata\", {}):\n",
        "    del nb[\"metadata\"][\"widgets\"]\n",
        "\n",
        "# Also fix cells that contain widget metadata\n",
        "for cell in nb.get(\"cells\", []):\n",
        "    if \"metadata\" in cell and \"widgets\" in cell[\"metadata\"]:\n",
        "        del cell[\"metadata\"][\"widgets\"]\n",
        "\n",
        "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"Notebook cleaned \u2192\", output_path)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}