{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GsP7Fazij3fl",
        "outputId": "3e8c993e-f0aa-4960-96b0-5d2854c87a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.43.3\n",
            "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.11.1\n",
            "  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting accelerate==0.30.1\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (2.32.4)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (0.7.0)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.43.3)\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.43.3) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft==0.11.1) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.3) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.3) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.3) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft==0.11.1) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.43.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.43.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.43.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.43.3) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft==0.11.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft==0.11.1) (3.0.3)\n",
            "Downloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m188.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m348.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m155.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, accelerate, peft\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.11.0\n",
            "    Uninstalling accelerate-1.11.0:\n",
            "      Successfully uninstalled accelerate-1.11.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.18.0\n",
            "    Uninstalling peft-0.18.0:\n",
            "      Successfully uninstalled peft-0.18.0\n",
            "Successfully installed accelerate-0.30.1 peft-0.11.1 tokenizers-0.19.1 transformers-4.43.3\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting textstat\n",
            "  Downloading textstat-0.7.11-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.9.0+cu126)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.43.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from textstat) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from textstat) (75.2.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.7.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.19.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->textstat) (1.5.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.3)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m254.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textstat-0.7.11-py3-none-any.whl (176 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m346.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m194.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat, bert-score\n",
            "Successfully installed bert-score-0.3.13 pyphen-0.17.2 textstat-0.7.11\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Collecting alignscore-SpeedOfMagic\n",
            "  Downloading alignscore_speedofmagic-0.1.4-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (4.0.0)\n",
            "Collecting jsonlines<3,>=2.0.0 (from alignscore-SpeedOfMagic)\n",
            "  Downloading jsonlines-2.0.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting numpy<2,>=1.23.1 (from alignscore-SpeedOfMagic)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m231.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf<=3.20.3 (from alignscore-SpeedOfMagic)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Collecting pytorch-lightning>=1.7.7 (from alignscore-SpeedOfMagic)\n",
            "  Downloading pytorch_lightning-2.5.6-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (1.6.1)\n",
            "Requirement already satisfied: scipy<2,>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (1.16.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.12.0 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (2.19.0)\n",
            "Requirement already satisfied: torch>=1.12.1 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm<5,>=4.64.0 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (4.67.1)\n",
            "Requirement already satisfied: transformers<5,>=4.20.1 in /usr/local/lib/python3.12/dist-packages (from alignscore-SpeedOfMagic) (4.43.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.3.2->alignscore-SpeedOfMagic) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Collecting torchmetrics>0.7.0 (from pytorch-lightning>=1.7.7->alignscore-SpeedOfMagic)\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=1.7.7->alignscore-SpeedOfMagic)\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2,>=1.1.2->alignscore-SpeedOfMagic) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (3.10)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard<3,>=2.12.0->alignscore-SpeedOfMagic) (3.1.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.1->alignscore-SpeedOfMagic) (3.5.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.20.1->alignscore-SpeedOfMagic) (0.7.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.12/dist-packages (from transformers<5,>=4.20.1->alignscore-SpeedOfMagic) (0.19.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (3.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (1.2.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12.1->alignscore-SpeedOfMagic) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.3.2->alignscore-SpeedOfMagic) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.3.2->alignscore-SpeedOfMagic) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.3.2->alignscore-SpeedOfMagic) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.3.2->alignscore-SpeedOfMagic) (1.22.0)\n",
            "Downloading alignscore_speedofmagic-0.1.4-py3-none-any.whl (18 kB)\n",
            "Downloading jsonlines-2.0.0-py3-none-any.whl (6.3 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m196.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m358.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.6-py3-none-any.whl (831 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m831.6/831.6 kB\u001b[0m \u001b[31m402.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m405.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, numpy, lightning-utilities, jsonlines, torchmetrics, pytorch-lightning, alignscore-SpeedOfMagic\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alignscore-SpeedOfMagic-0.1.4 jsonlines-2.0.0 lightning-utilities-0.15.2 numpy-1.26.4 protobuf-3.20.3 pytorch-lightning-2.5.6 torchmetrics-1.8.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "04dfb8b8133745cba252632f99acc223",
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m123.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m\u2714 Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m\u26a0 Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3096275508.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python -m spacy download en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"punkt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"punkt_tab\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;31m# Import top-level functionality into top-level namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorators\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemoize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatstruct\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/collocations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# these two unused imports are referenced in collocations.doctest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m from nltk.metrics import (\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mContingencyMeasures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/metrics/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magreement\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotationTask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from nltk.metrics.association import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mBigramAssocMeasures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mContingencyMeasures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/metrics/association.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfisher_exact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    624\u001b[0m from ._warnings_errors import (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    625\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 626\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_stats_py\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_variation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvariation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/stats/_stats_py.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistance_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'scipy.{name}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_importlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      9\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_todata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                        matrix, validateaxis, getdtype, is_pydata_spmatrix)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m from scipy._lib._array_api import (Array, array_namespace, is_lazy_array,\n\u001b[0m\u001b[1;32m     15\u001b[0m                                    \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_result_device\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                    xp_size, xp_result_type)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mis_lazy_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403  # pyright: ignore[reportWildcardImportFromLibrary]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y transformers tokenizers -q\n",
        "\n",
        "!pip install --no-cache-dir \"transformers==4.43.3\" \"peft==0.11.1\" \"accelerate==0.30.1\"\n",
        "!pip install --no-cache-dir datasets bert-score textstat\n",
        "!pip install --no-cache-dir sentencepiece\n",
        "!pip install --no-cache-dir alignscore-SpeedOfMagic spacy nltk\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7HXjS1tvzUH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "os.environ[\"BITSANDBYTES_DISABLE\"] = \"1\"\n",
        "\n",
        "# Desactivar wandb completamente\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ewZfFeBunZj",
        "outputId": "3f53aa8e-3e51-4856-b036-86f363033c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datos extra\u00eddos en: /content/training_data\n",
            "content\n",
            "  - training_data.zip\n",
            "  .config\n",
            "    - .last_opt_in_prompt.yaml\n",
            "    - .last_survey_prompt.yaml\n",
            "    - .last_update_check.json\n",
            "    - hidden_gcloud_config_universe_descriptor_data_cache_configs.db\n",
            "    - config_sentinel\n",
            "    - active_config\n",
            "    - gce\n",
            "    - default_configs.db\n",
            "    configurations\n",
            "      - config_default\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "from bert_score import score as bert_score\n",
        "import textstat\n",
        "from alignscore import AlignScore\n",
        "\n",
        "\n",
        "ZIP_PATH = \"/content/training_data.zip\"\n",
        "ROOT_DATA_DIR = \"/content/training_data\"\n",
        "TRAINING_DATA_PATH = \"/content/training_data/training_data\"\n",
        "\n",
        "BASE_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "ADAPTER_DIR = \"/content/deepseek-r1-distill-qwen-1.5b-pls\"\n",
        "\n",
        "os.makedirs(ROOT_DATA_DIR, exist_ok=True)\n",
        "\n",
        "\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "        z.extractall(ROOT_DATA_DIR)\n",
        "    print(\"Datos extra\u00eddos en:\", ROOT_DATA_DIR)\n",
        "else:\n",
        "    print(\"No encontr\u00e9 training_data.zip\")\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(\"/content\"):\n",
        "    level = root.replace(\"/content\", \"\").count(os.sep)\n",
        "    indent = \" \" * (2 * level)\n",
        "    print(f\"{indent}{os.path.basename(root)}\")\n",
        "    for f in files:\n",
        "        print(f\"{indent}  - {f}\")\n",
        "    if level >= 2:\n",
        "        break\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUgtpu6UkRzT",
        "outputId": "8a3567b1-6834-4bd1-b2a5-f0b3d52c3fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== DEBUG RUTA DE DATOS ===\n",
            "training_data_path: /content/training_data/training_data\n",
            "Contenido de la carpeta: ['no_pls_clean.parquet', 'main_train.parquet', 'augmented_test.parquet', 'augmented_train.parquet', 'main_test.parquet', 'pls_clean.parquet']\n",
            "\n",
            "Leyendo archivos y agrupando por doc_id normalizado...\n",
            " - [OK] main_train.parquet: 7408 filas\n",
            " - [OK] main_test.parquet: 1851 filas\n",
            " - [OK] augmented_train.parquet: 16469 filas\n",
            " - [OK] augmented_test.parquet: 4120 filas\n",
            " - [OK] no_pls_clean.parquet: 8401 filas\n",
            " - [OK] pls_clean.parquet: 6778 filas\n",
            "\n",
            "Total de doc_id (normalizados) con al menos 1 medical y 1 plain: 5688\n",
            "Total de pares construidos: 5688\n",
            "Split:\n",
            " - Train: 3981\n",
            " - Val:   853\n",
            " - Test:  854\n",
            "\n",
            "Total de pares preparados para RL / LoRA: 5688\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class PromptPair:\n",
        "    medical: str\n",
        "    plain: str\n",
        "    context: str | None = None\n",
        "    pair_id: str | None = None\n",
        "    flesch_score: float | None = None\n",
        "\n",
        "\n",
        "def normalize_doc_id(doc_id: str) -> str:\n",
        "    s = str(doc_id)\n",
        "    if \"_\" in s:\n",
        "        s = s.split(\"_\")[00]\n",
        "    s = s.replace(\"-abstract\", \"\").replace(\"-pls\", \"\")\n",
        "    return s\n",
        "\n",
        "\n",
        "def load_cochrane_pairs(training_data_path: str | Path):\n",
        "    training_data_path = Path(training_data_path)\n",
        "\n",
        "    print(\"\\n=== DEBUG RUTA DE DATOS ===\")\n",
        "    print(\"training_data_path:\", training_data_path)\n",
        "    if training_data_path.exists():\n",
        "        print(\"Contenido de la carpeta:\", os.listdir(training_data_path))\n",
        "    else:\n",
        "        print(\"Ruta no existe\")\n",
        "\n",
        "    parquet_files = [\n",
        "        \"main_train.parquet\",\n",
        "        \"main_test.parquet\",\n",
        "        \"augmented_train.parquet\",\n",
        "        \"augmented_test.parquet\",\n",
        "        \"no_pls_clean.parquet\",\n",
        "        \"pls_clean.parquet\",\n",
        "    ]\n",
        "\n",
        "    all_rows = []\n",
        "\n",
        "    print(\"\\nLeyendo archivos y agrupando por doc_id normalizado...\")\n",
        "    for fname in parquet_files:\n",
        "        fp = training_data_path / fname\n",
        "        if not fp.exists():\n",
        "            print(f\" - [OMITIDO] {fname} (no existe en {training_data_path})\")\n",
        "            continue\n",
        "\n",
        "        df = pd.read_parquet(fp)\n",
        "        print(f\" - [OK] {fname}: {df.shape[0]} filas\")\n",
        "        for col in [\"doc_id\", \"text\", \"label\"]:\n",
        "            if col not in df.columns:\n",
        "                raise ValueError(f\"{fname} no tiene columna {col}\")\n",
        "        df = df[[\"doc_id\", \"text\", \"label\"]].copy()\n",
        "        df[\"doc_id_norm\"] = df[\"doc_id\"].apply(normalize_doc_id)\n",
        "        all_rows.append(df)\n",
        "\n",
        "    if not all_rows:\n",
        "        raise RuntimeError(\"No se encontraron datos parquet v\u00e1lidos en la ruta.\")\n",
        "\n",
        "    full_df = pd.concat(all_rows, ignore_index=True)\n",
        "\n",
        "    grouped = full_df.groupby(\"doc_id_norm\")\n",
        "\n",
        "    pairs: list[PromptPair] = []\n",
        "    for doc_id_norm, g in grouped:\n",
        "        med_candidates = g[g[\"label\"] == \"no_pls\"]\n",
        "        plain_candidates = g[g[\"label\"] == \"pls\"]\n",
        "        if med_candidates.empty or plain_candidates.empty:\n",
        "            continue\n",
        "\n",
        "        med_row = med_candidates.loc[med_candidates[\"text\"].str.len().idxmax()]\n",
        "        plain_row = plain_candidates.loc[plain_candidates[\"text\"].str.len().idxmax()]\n",
        "\n",
        "        pairs.append(\n",
        "            PromptPair(\n",
        "                medical=str(med_row[\"text\"]),\n",
        "                plain=str(plain_row[\"text\"]),\n",
        "                context=None,\n",
        "                pair_id=doc_id_norm,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    print(f\"\\nTotal de doc_id (normalizados) con al menos 1 medical y 1 plain: {len(pairs)}\")\n",
        "\n",
        "    import random\n",
        "    random.seed(42)\n",
        "    random.shuffle(pairs)\n",
        "\n",
        "    n_total = len(pairs)\n",
        "    n_train = int(n_total * 0.70)\n",
        "    n_val   = int(n_total * 0.15)\n",
        "    n_test  = n_total - n_train - n_val\n",
        "\n",
        "    train_pairs = pairs[:n_train]\n",
        "    val_pairs   = pairs[n_train:n_train + n_val]\n",
        "    eval_pairs  = pairs[n_train + n_val:]\n",
        "\n",
        "    print(f\"Total de pares construidos: {n_total}\")\n",
        "    print(\"Split:\")\n",
        "    print(f\" - Train: {len(train_pairs)}\")\n",
        "    print(f\" - Val:   {len(val_pairs)}\")\n",
        "    print(f\" - Test:  {len(eval_pairs)}\")\n",
        "\n",
        "    return train_pairs, val_pairs, eval_pairs\n",
        "\n",
        "\n",
        "train_pairs, val_pairs, eval_pairs = load_cochrane_pairs(TRAINING_DATA_PATH)\n",
        "all_pairs = train_pairs + val_pairs + eval_pairs\n",
        "print(f\"\\nTotal de pares preparados para RL / LoRA: {len(all_pairs)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_EWSXNYvB_o"
      },
      "outputs": [],
      "source": [
        "INSTRUCTION = (\n",
        "    \"You are a specialist in healthcare communication. \"\n",
        "    \"Use the context to transform the following medical text into a clear, concise, \"\n",
        "    \"and easy-to-understand summary for a patient and their family. \"\n",
        "    \"Retain all relevant clinical data, but explain technical terms using simple language and short sentences.\\n\\n\"\n",
        ")\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PLSDataset(Dataset):\n",
        "    def __init__(self, pairs, tokenizer, max_length=2048):\n",
        "        \"\"\"\n",
        "        pairs: lista de PromptPair (medical, plain, ...)\n",
        "        tokenizer: tokenizer del modelo (DeepSeek-R1-Distill-Qwen-1.5B)\n",
        "        max_length: longitud m\u00e1xima del input\n",
        "        \"\"\"\n",
        "        self.pairs = pairs\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        pair = self.pairs[idx]\n",
        "\n",
        "        # Construir el prompt completo\n",
        "        prompt_input = (\n",
        "            \"You are a specialist in healthcare communication. \"\n",
        "            \"Use the context to transform the following medical text into a clear, concise, \"\n",
        "            \"and easy-to-understand summary for a patient and their family. \"\n",
        "            \"Retain all relevant clinical data, but explain technical terms using simple \"\n",
        "            \"language and short sentences.\\n\\n\"\n",
        "            \"### Medical text:\\n\"\n",
        "            f\"{pair.medical}\\n\\n\"\n",
        "            \"### Simplified summary:\\n\"\n",
        "        )\n",
        "        \n",
        "        # Tokenizar el prompt de input (sin el resumen)\n",
        "        enc_input = self.tokenizer(\n",
        "            prompt_input,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # Tokenizar el resumen objetivo (sin special tokens al inicio, pero con EOS al final)\n",
        "        enc_target = self.tokenizer(\n",
        "            pair.plain,\n",
        "            add_special_tokens=False,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        \n",
        "        # Obtener los token IDs\n",
        "        input_ids_input = enc_input[\"input_ids\"][0]\n",
        "        input_ids_target = enc_target[\"input_ids\"][0]\n",
        "        \n",
        "        # Concatenar input + target\n",
        "        full_input_ids = torch.cat([input_ids_input, input_ids_target])\n",
        "        \n",
        "        # Truncar si es necesario\n",
        "        if len(full_input_ids) > self.max_length:\n",
        "            # Priorizar mantener el input completo, truncar el target si es necesario\n",
        "            input_len = len(input_ids_input)\n",
        "            if input_len < self.max_length:\n",
        "                # Hay espacio para parte del target\n",
        "                target_len = self.max_length - input_len\n",
        "                full_input_ids = torch.cat([\n",
        "                    input_ids_input,\n",
        "                    input_ids_target[:target_len]\n",
        "                ])\n",
        "            else:\n",
        "                # El input es muy largo, truncarlo\n",
        "                full_input_ids = input_ids_input[:self.max_length]\n",
        "        \n",
        "        # Padding\n",
        "        padding_length = self.max_length - len(full_input_ids)\n",
        "        pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.eos_token_id\n",
        "        if padding_length > 0:\n",
        "            padding = torch.full((padding_length,), pad_token_id, dtype=full_input_ids.dtype)\n",
        "            full_input_ids = torch.cat([full_input_ids, padding])\n",
        "        \n",
        "        # Attention mask\n",
        "        attention_mask = (full_input_ids != pad_token_id).long()\n",
        "        \n",
        "        # Labels: -100 para input (instrucci\u00f3n + texto m\u00e9dico), tokens reales solo para el resumen\n",
        "        labels = full_input_ids.clone()\n",
        "        labels.fill_(-100)  # Marcar todo como ignorado inicialmente\n",
        "        \n",
        "        # Solo las etiquetas del resumen deben tener los tokens reales\n",
        "        input_len = len(input_ids_input)\n",
        "        actual_target_len = min(len(input_ids_target), len(full_input_ids) - input_len)\n",
        "        if actual_target_len > 0:\n",
        "            labels[input_len:input_len + actual_target_len] = input_ids_target[:actual_target_len]\n",
        "        \n",
        "        # Marcar padding como -100\n",
        "        labels[attention_mask == 0] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": full_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632,
          "referenced_widgets": [
            "cca13968ed5440c4b6c389f996bf6623",
            "12bec9cb29514440ad9ed0c5f038f743",
            "0c11ae11ae86492ea0407bfc3f0d42ac",
            "7ff00061b0434d66a5fe44ba3a1c4010",
            "934fe31f8bdf4f5d82117e2ba05b6a52",
            "7de08f18b9984a67acd169cee57f444b",
            "c0e39fb0a51642fb8f1b7467b337db50",
            "4569fc69256d44c6a886a595d5ef579c",
            "f37f741ed59f403d8a4259db47191b70",
            "59e5dc72261a49c088c889abeaaee0c5",
            "dfb52a964c6f4c6f86dc79698ac977dd",
            "a23d9823332d488fa8ff8e34e5a68df8",
            "a421d674a4d04a6898b454b58b48693c",
            "13df9eb9230f4dedb8318710f95e30cf",
            "d4db73dd07954dac83ca57e0acc622eb",
            "92fca0d887d04c1ebe856ba693a0df03",
            "94551221f5784f0c9efdc2e202c0629e",
            "9bcb41f22a204b2ea7376ee87154f7b8",
            "8fbbc911bd8942699a30b4f3884debe4",
            "124fae30a5ef4777a8ae3ed0509d344a",
            "a8b5a42793e34a09ae9e4cb4e5d33c36",
            "cff83534ef854d2e9d2b4e3f657befc4",
            "cb405198b1ae4fc9b6bd4dd22f3274e5",
            "82509a1d1ee04c979aa57a590576cfbd",
            "ab56d4bac2bb40da97cae8f283028eb5",
            "cc3cabb8f4d9441c9c06192f3e05f477",
            "1c70066f786a4435a70fe6c8feef2824",
            "26e1884c083b40a6a3654d85f396f3a6",
            "d6c2c43c4c7c41f2b2d04f37924f3137",
            "72795e88c7cf4ffcac73adf7d2afe145",
            "6eaeb006ae5c41f7a9b50024a78d387d",
            "c9d49b921fa94250b5c076c2e22b6d09",
            "0adec556923c45989b60ca6a1150cd5d",
            "9a0ecd9c252f4fecbd021d699553f60a",
            "cc53adc342b049c5abd78bd3e7c8d2b3",
            "df735949f5204a57b7982fdb6f9ca4ef",
            "4a1fcce8ccca420f9c2c2110d29a5de5",
            "97fe821f32d04682bbf934f761d63a5b",
            "ae337d7a65bc4d6c9d6e5eb0b38f5a58",
            "6d982451540745f2968cec32a928074f",
            "df25b2f4fc5f42a7a7f0086717a1891b",
            "aafe49599dd74aea89a5f6c4aaab2532",
            "be7b49412a9c4612827b05e2e77d578e",
            "b42304e75d9a4ca5a17d2294913e287d",
            "d47dda4c9ae94cc1bfdee3fad25bea1d",
            "3d912cad658b416c97775c63306a1620",
            "2f90db5c6ae4461fb32591fe0149b27c",
            "d25604cd01884d09819ddf7921929bca",
            "2ad1bebba418408e85d93958e420820e",
            "94c261e5814e4e3e9c7f2e61bd1b6a73",
            "fa4d144799334a1c93fd6eb7982a9a9a",
            "5346328abc0d45dfb8a89fe292873e91",
            "d8172041acbf430ca2c8c8179a85509e",
            "00b473e971e54b319ecb6dff14ddd144",
            "d1b02fb001ff4763afcd781c13597f2a",
            "e4df87b6a4c04b90b24bdf3f99b2565a",
            "52857c5cad0b47e78f6bceebdefac62b",
            "387b0b07142146419504e913d4136c9c",
            "2ee0bb2544dd49be884643b69a2e817a",
            "27cc8c289b1649d7b2a18be8feee64dd",
            "95761360f38048a8b4bfd59626e0c043",
            "169b0e2c68a84cc1a226cebfff05262a",
            "4bbb4e4fdca94de1a4bff597cd113604",
            "e925294c3d1645aa968ac3e4f8696010",
            "607c250a18734187bd333682064292d7",
            "3e069be2892c451bac8a7e99d09f9b7d",
            "6fbdb95c0f9f442798d419dd2c55badc",
            "2688352da5b041428f500f90073d045e",
            "dd606dece49c4800a47a7f5aed51c850",
            "6c625bb3948b4cffb5575d409089a1c7",
            "c3b08c1606a24d9f901e39f6f45d6199",
            "d1fb03f3f10d4f89a2ba95940794cecc",
            "6b4a6b6373b548dba7fb736601192eb4",
            "f59f0866f9814f4385e1d4439b15f57c",
            "2e25524f0b1b4205a2c2bbb71b305e0d",
            "5a01a340992b45e396b73e44afb421e7",
            "57abfe2d064d4f66bd3ce82d5927b6db",
            "992a1cb78ab44817b0920466f582ce8e",
            "38ac8bfcdacd45f7b2959a7615f187ff",
            "e165ee7f9be347b1891a27ef79deb433",
            "f3fd21ffaef94927aa26a2268b15d452",
            "0292dea0db4c4640bbe79cd48bac688c",
            "23a9bb1f940a41e3825845fb477520a2",
            "cab9406cd4314badac5376ae458dee57",
            "0126865a4b88475d92aff40a6d814ebd",
            "cf51339ee7424d378fba858f826a0d95",
            "aa2cd7a1e3bc4b809d5ac1ea68f59cee",
            "4013333692844c409818cf25046bccf0",
            "1831fcf360f2483cb55270f1d5acaf4b",
            "e41fbe952b1e4173b38d6a729a928850",
            "a9ef6a83d12544cc9858dbe501079ac6",
            "3b0d4413626d4e92ab7e41e84a7f7c33",
            "6b112f8a41f24f659476c8aaf297e7be",
            "2a6d43d2d2c541f1b03b698cc4646e68",
            "e7269907c2514d4fbd72d91a97a7c2a5",
            "74bac56ce07a440f8b16c1a0f379d9cd",
            "69f5a0fc0507441fab8842b85df2f6ba",
            "93e1a3abc31545518d6d8e71a92a9ba9",
            "8b3fd1ae16c049a59b12737f6da126ef",
            "8cfe57292550482498eda2b8e9aeb10e",
            "83954c8121ca4fa6b5f9723345e8a96d",
            "272c2f30c1314b06ab148132bb5b7d71",
            "bb09de42a95444ccbb9fb214723665c2",
            "7b05f3156e614b92b483f7d23cbea56e",
            "924f67f362cd457a82bda1a967b5d520",
            "0ef63265cb5c46f1896cf30d0cec72ca",
            "06a6e7fd34bf400ea981bd03784d7e66",
            "afaa1bf0d6744bc3adcac79030ef04da",
            "141304e7b6484ac2937fd1d2fb4d1a53",
            "8e2ff6713a6b41aabd14750ed1848ca0",
            "7a75eaa64c4b44759bae200cc119444d",
            "e3a25a3a835341dbbef94d072404329e",
            "92fbda7570064497ba0a49ee89fca2d5",
            "d7b630b326cf46d7bce3f4d3c86c3c02",
            "2473d166629f48c2b9d83c60261f5b7a",
            "21ee2722f61a49399d656d08d4aacc4e",
            "75a9af3d1cef402280c7a0f36ad7b69a",
            "4528217d6a8840528d725b844b572453",
            "62252e3727ac4e7f8744a1c74f4f0c9c",
            "8cfbf24b615747699614fded0792c325",
            "0a55a5a0b95e44869e07a2359f9ed0c9"
          ]
        },
        "id": "E2BKD1BQvGbZ",
        "outputId": "a2dd994a-cde4-40fd-ac78-ea887a0e2024"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CARGANDO MODELO BASE deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cca13968ed5440c4b6c389f996bf6623",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a23d9823332d488fa8ff8e34e5a68df8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cb405198b1ae4fc9b6bd4dd22f3274e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a0ecd9c252f4fecbd021d699553f60a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d47dda4c9ae94cc1bfdee3fad25bea1d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4df87b6a4c04b90b24bdf3f99b2565a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fbdb95c0f9f442798d419dd2c55badc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "992a1cb78ab44817b0920466f582ce8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1831fcf360f2483cb55270f1d5acaf4b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cfe57292550482498eda2b8e9aeb10e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a75eaa64c4b44759bae200cc119444d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparando modelo para LoRA...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'setup_lora_config' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3942777167.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preparando modelo para LoRA...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mlora_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_lora_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mmodel_lora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlora_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mmodel_lora\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_trainable_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'setup_lora_config' is not defined"
          ]
        }
      ],
      "source": [
        "def setup_quantization_config():\n",
        "    return BitsAndBytesConfig(\n",
        "        load_in_4bit=False,\n",
        "        load_in_8bit=False,\n",
        "    )\n",
        "\n",
        "def setup_lora_config():\n",
        "    return LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"CARGANDO MODELO BASE {BASE_MODEL_ID}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "base_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "if base_tokenizer.pad_token is None:\n",
        "    base_tokenizer.pad_token = base_tokenizer.eos_token\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=None,\n",
        ")\n",
        "\n",
        "print(\"Preparando modelo para LoRA...\")\n",
        "lora_config = setup_lora_config()\n",
        "model_lora = get_peft_model(base_model, lora_config)\n",
        "model_lora.print_trainable_parameters()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOZ_5quLkT38",
        "outputId": "40279336-1096-4f1b-fe84-8d750355ea36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tama\u00f1o train_dataset: 3981\n",
            "Tama\u00f1o val_dataset: 853\n",
            "input_ids torch.Size([2048]) torch.int64\n",
            "attention_mask torch.Size([2048]) torch.int64\n",
            "labels torch.Size([2048]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "train_dataset = PLSDataset(train_pairs, base_tokenizer, max_length=2048)\n",
        "val_dataset   = PLSDataset(val_pairs,   base_tokenizer, max_length=2048)\n",
        "\n",
        "print(\"Tama\u00f1o train_dataset:\", len(train_dataset))\n",
        "print(\"Tama\u00f1o val_dataset:\", len(val_dataset))\n",
        "\n",
        "# Verificamos un ejemplo\n",
        "sample = train_dataset[0]\n",
        "for k, v in sample.items():\n",
        "    print(k, v.shape, v.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XyNJmsUwvQvY",
        "outputId": "c6e851ec-9f90-4b35-a095-05d1a51b5e6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando fine-tuning...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='995' max='995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [995/995 18:28, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.507000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.355700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.379500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>1.377600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.351300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>1.344300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>1.363000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.303900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>1.315500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.332600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>1.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>1.356100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>1.303100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>1.377500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.324100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>1.325300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>1.354200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>1.338600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>1.345500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.343000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>1.335000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>1.335500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>1.295500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>1.316700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.304700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>1.304600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>1.318300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>1.260200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>1.322700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.331200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>1.327800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>1.346200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>1.320300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>1.363500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.299900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>1.324600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>1.303400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>1.315300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>1.320900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.299700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>1.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>1.327300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>1.352000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>1.303500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.322100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>1.327200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>1.306900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>1.319200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>1.331100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuning completado.\n",
            "Adaptador LoRA guardado en: /content/deepseek-r1-distill-qwen-1.5b-pls\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./deepseek_lora_out\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=3e-4,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=20,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    fp16=False,\n",
        "    bf16=True,  # en A100/T4/L4 est\u00e1 bien, si ves NaN pon bf16=False\n",
        "\n",
        "    eval_strategy=\"no\",\n",
        "    report_to=\"none\",\n",
        "    run_name=\"deepseek_lora_run\",\n",
        "\n",
        "    logging_nan_inf_filter=False,  # <- si hay NaN ver\u00e1s el error real\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=base_tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_lora,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Iniciando fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning completado.\")\n",
        "\n",
        "model_lora.save_pretrained(ADAPTER_DIR)\n",
        "print(\"Adaptador LoRA guardado en:\", ADAPTER_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wzh93QS0ATX",
        "outputId": "83e54493-9741-40fb-becb-13de02f53b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando evaluador...\n",
            "Inicializando EvaluationMetrics...\n",
            "Dispositivo evaluador: cuda\n",
            "Checkpoint AlignScore local: /root/.cache/huggingface/hub/models--yzha--AlignScore/snapshots/8509e78d25bb914939fc585c626500c9b2944249/AlignScore-base.ckpt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.7.7 to v2.5.6. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/huggingface/hub/models--yzha--AlignScore/snapshots/8509e78d25bb914939fc585c626500c9b2944249/AlignScore-base.ckpt`\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.12/dist-packages/pytorch_lightning/core/saving.py:195: Found keys that are not in the model state dict but in the checkpoint: ['base_model.embeddings.position_ids']\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EvaluationMetrics listo.\n"
          ]
        }
      ],
      "source": [
        "class EvaluationMetrics:\n",
        "    def __init__(self, device=None):\n",
        "        print(\"Inicializando EvaluationMetrics...\")\n",
        "\n",
        "        self.device = device or (\n",
        "            torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        )\n",
        "        print(\"Dispositivo evaluador:\", self.device)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # 1) CORRECTO: Descargar checkpoint desde HuggingFace\n",
        "        # ---------------------------------------------------------\n",
        "        from huggingface_hub import hf_hub_download\n",
        "\n",
        "        ckpt_path = hf_hub_download(\n",
        "            repo_id=\"yzha/AlignScore\",\n",
        "            filename=\"AlignScore-base.ckpt\",\n",
        "        )\n",
        "        print(\"Checkpoint AlignScore local:\", ckpt_path)\n",
        "\n",
        "        # ---------------------------------------------------------\n",
        "        # 2) CORRECTO: Inicializar AlignScore desde SpeedOfMagic\n",
        "        # ---------------------------------------------------------\n",
        "        self.align_scorer = AlignScore(\n",
        "            model=\"roberta-base\",\n",
        "            batch_size=4,\n",
        "            device=str(self.device),\n",
        "            ckpt_path=ckpt_path,\n",
        "            evaluation_mode=\"nli_sp\",\n",
        "        )\n",
        "\n",
        "        print(\"EvaluationMetrics listo.\")\n",
        "\n",
        "    # ------------------------ RELEVANCIA ------------------------\n",
        "    def relevance(self, generated: str, reference: str):\n",
        "        precision, recall, f1 = bert_score(\n",
        "            [generated],\n",
        "            [reference],\n",
        "            lang=\"en\",\n",
        "            verbose=False,\n",
        "        )\n",
        "        return {\n",
        "            \"precision\": float(precision.item()),\n",
        "            \"recall\": float(recall.item()),\n",
        "            \"f1\": float(f1.item()),\n",
        "        }\n",
        "\n",
        "    # ------------------------ FACTUALIDAD ------------------------\n",
        "    def factuality(self, generated: str, source: str):\n",
        "        try:\n",
        "            if not generated or not isinstance(generated, str) or len(generated.strip()) < 10:\n",
        "                return {\"score\": 0.0}\n",
        "            if not source or not isinstance(source, str) or len(source.strip()) < 10:\n",
        "                return {\"score\": 0.0}\n",
        "            score = self.align_scorer.score(\n",
        "                contexts=[source],\n",
        "                claims=[generated],\n",
        "            )[0]\n",
        "            return {\"score\": float(score)}\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Error calculando factualidad: {e}\")\n",
        "            return {\"score\": 0.0}\n",
        "\n",
        "    # ------------------------ LEGIBILIDAD ------------------------\n",
        "    def readability(self, text: str):\n",
        "        try:\n",
        "            if not text or not isinstance(text, str):\n",
        "                return self._default_readability_metrics()\n",
        "\n",
        "            txt = text.strip()\n",
        "            if len(txt) < 10:\n",
        "                return self._default_readability_metrics()\n",
        "\n",
        "            fre  = float(textstat.flesch_reading_ease(txt))\n",
        "            fk   = float(textstat.flesch_kincaid_grade(txt))\n",
        "            cli  = float(textstat.coleman_liau_index(txt))\n",
        "            gfi  = float(textstat.gunning_fog(txt))\n",
        "            smog = float(textstat.smog_index(txt))\n",
        "            dale = float(textstat.dale_chall_readability_score(txt))\n",
        "\n",
        "            # Normalizar Flesch\n",
        "            fre = max(0.0, min(100.0, fre))\n",
        "\n",
        "            return {\n",
        "                \"flesch_reading_ease\": fre,\n",
        "                \"flesch_kincaid_grade_level\": fk,\n",
        "                \"coleman_liau_index\": cli,\n",
        "                \"gunning_fog_index\": gfi,\n",
        "                \"smog_index\": smog,\n",
        "                \"dale_chall_readability_score\": dale,\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] Error calculando legibilidad: {e}\")\n",
        "            return self._default_readability_metrics()\n",
        "\n",
        "    def _default_readability_metrics(self):\n",
        "        return {\n",
        "            \"flesch_reading_ease\": 30.0,\n",
        "            \"flesch_kincaid_grade_level\": 12.0,\n",
        "            \"coleman_liau_index\": 12.0,\n",
        "            \"gunning_fog_index\": 12.0,\n",
        "            \"smog_index\": 12.0,\n",
        "            \"dale_chall_readability_score\": 9.0,\n",
        "        }\n",
        "\n",
        "    # ------------------------ M\u00c9TRICA FINAL ------------------------\n",
        "    def evaluate(self, generated: str, reference: str, source: str):\n",
        "        return {\n",
        "            \"relevance\":   self.relevance(generated, reference),\n",
        "            \"factuality\":  self.factuality(generated, source),\n",
        "            \"readability\": self.readability(generated),\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"Cargando evaluador...\")\n",
        "evaluator = EvaluationMetrics()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RnrwsWAvW6a"
      },
      "outputs": [],
      "source": [
        "def generate_summary(model, tokenizer, medical_text: str, max_new_tokens: int = 256) -> str:\n",
        "    prompt = (\n",
        "        INSTRUCTION\n",
        "        + \"### Medical text:\\n\"\n",
        "        + medical_text.strip()\n",
        "        + \"\\n\\n### Simplified summary:\\n\"\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=2048,\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "        )\n",
        "\n",
        "    full_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    if \"### Simplified summary:\" in full_text:\n",
        "        summary = full_text.split(\"### Simplified summary:\")[-1].strip()\n",
        "    else:\n",
        "        summary = full_text.strip()\n",
        "    return summary\n",
        "\n",
        "\n",
        "def evaluate_model(model, tokenizer, pairs, sample_size: int = 30):\n",
        "    import random\n",
        "    subset = random.sample(pairs, min(sample_size, len(pairs)))\n",
        "\n",
        "    results = []\n",
        "    for p in subset:\n",
        "        gen = generate_summary(model, tokenizer, p.medical)\n",
        "        metrics = evaluator.evaluate(\n",
        "            generated=gen,\n",
        "            reference=p.plain,\n",
        "            source=p.medical,\n",
        "        )\n",
        "        results.append(metrics)\n",
        "    return results\n",
        "\n",
        "\n",
        "def summarize_metrics(results: List[Dict[str, Any]]) -> Dict[str, float]:\n",
        "    import numpy as np\n",
        "\n",
        "    if not results:\n",
        "        return {}\n",
        "\n",
        "    bert_f1 = [r[\"relevance\"][\"f1\"] for r in results]\n",
        "    factual = [r[\"factuality\"][\"score\"] for r in results]\n",
        "    flesch  = [r[\"readability\"][\"flesch_reading_ease\"] for r in results]\n",
        "\n",
        "    return {\n",
        "        \"bertscore_f1\": float(np.mean(bert_f1)),\n",
        "        \"factuality\": float(np.mean(factual)),\n",
        "        \"flesch_reading_ease\": float(np.mean(flesch)),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "95089bad9bff4438911ac4aa55179d51",
            "daea2cf314804168a3256a0a846851f3",
            "4b37fa55bbf44f219b602a8ddbe57340",
            "873dab442a704671a8dd8ffad74a9a1f",
            "5e287c5db3ce4e548d5c9b9a21529577",
            "4e01eefb73644829b65dc67fd94e2e02",
            "b98d256db4764eb59249cc8514f30eda",
            "27b7dcc633f6418e96178cdc5d8fb4b8",
            "98c6c63db43b4b5eb5fe01886c70cc31",
            "e2ca426bba3d43318ed6e6ed6a18c2d8",
            "0fe0015ac85047cdb7024a475e6c2330",
            "3adb6c74101d4067ac1dfc0a72937c13",
            "f24febd181d1483cba9412af189b9a46",
            "045aefe76f85488fa64bf85dc0c230a6",
            "8c5e974686c0491892e83fb8e001e2bb",
            "f7b754dffc9d4b2d8b8b90e895fd9932",
            "e38a7d074b3e4bc79646815e70ba4f40",
            "cbceaaa122a14a36a874bfcaaab02519",
            "985de45e288948c68ee88e7d02004d58",
            "87a47d6bc4974430ad135fa092891a32",
            "e4a04353da8b413a8cb044d46415f8ce",
            "d09508e58cb64a77bee462933442ba3c",
            "7981b48a9032479485188d529b795318",
            "8c9c29197ab84d9792f48da328217dfc",
            "123aee7e7e35460c93a9e9482afddeb1",
            "caed2586829e4531a0fee1c2d5bac147",
            "b92f526cc7ce48279680eeafa6a4e9ed",
            "a26ff460e058499ca759b4ae8f609a81",
            "3c79c3aa7df64826ac4bf830dc170c4b",
            "55ed0116e8884bd9a8f87cb11e49ec9a",
            "e49a824aa72443febec3189a8cbe2a7a",
            "6173b3bf68fe41039290f44c5ea36162",
            "29002dacc69b4b7ead03cb42ee38b33d",
            "28cf909869f940a3aef205218df0851f",
            "8a1fe732a44c4f9d970d1da07d9251f8",
            "bc2744787c0841c9b8cd21a7e382c791",
            "c41486cc69bb41919924671b52f0e064",
            "f19dc913f5c34c4b8a0791151d7eb568",
            "da25c04d057e4d4ba21adc84a6d149db",
            "59ccac6496a24de5b149a95da98eacb2",
            "371fa846d62748a6977029bd3524f593",
            "756b7874a19e469486cbbcdc15adc010",
            "0d153e9ce3a34a40aee2bf96a5f14d1d",
            "ec5db9e307a847ecb170ba15a8fcc58b",
            "931dbe5119384080bc1f286e200d158a",
            "231b2ae1208740fc8f5d3d3fed76fc7d",
            "7476b66c908f4c2d8cdc46818c125e4b",
            "cce0312848fb4db6a913bf9442c615e8",
            "6b04b7bee6d3471690efb3f6e9560790",
            "d2042d4e6c6440cdaff19b12f4de3699",
            "963a1377442e4cc39341583f38251eba",
            "cbbb3fbad7b44a3ca8f3e20a8b796329",
            "4cecabf98b034b2f8fb20b8a5cef47cd",
            "a902437575eb47d2b0df7ded494aa324",
            "dd880d00e01a4d0499e49c025ce381d8",
            "57b316ac759642499ce74dd6c3faa589",
            "60e0f9351678484abcf9774113640cc1",
            "920eaa0c0bee46b0918fc2368cbf04a7",
            "86f3bb97186b444bb8be2cb52bc63be8",
            "b550922cc6f547fcbec75af6cfb70d73",
            "659071bc5c1c4fc19af5ddfe49b9d0ca",
            "e7f397bef16044049b8182d811a64baf",
            "7ef39ac7a1bf4cf6aa50bce82eb8f0fd",
            "55d0dd99ed634bfea256577d2f30d173",
            "10cf55950f08456db4de763e8b87193d",
            "a9eda02558ad4c19874374b8c69e4400"
          ]
        },
        "id": "jUoVLeizvYzW",
        "outputId": "b49320d6-6b11-45d3-e39d-01ba07f3b0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EVALUANDO MODELO BASE\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95089bad9bff4438911ac4aa55179d51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3adb6c74101d4067ac1dfc0a72937c13",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7981b48a9032479485188d529b795318",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28cf909869f940a3aef205218df0851f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "931dbe5119384080bc1f286e200d158a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57b316ac759642499ce74dd6c3faa589",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.64it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.45it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.98it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.72it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.79it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.84it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.80it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.25it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.72it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.05it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.99it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.34it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.74it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.77it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base: {'bertscore_f1': 0.8654821276664734, 'factuality': 0.6583624462286631, 'flesch_reading_ease': 32.77305093932154}\n",
            "================================================================================\n",
            "EVALUANDO MODELO FINE-TUNED (LoRA)\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.73it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.02it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.93it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.85it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.25it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.74it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.00it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.69it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.99it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.60it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.65it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.85it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.28it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.05it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.18it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.07it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fine-tuned: {'bertscore_f1': 0.8636986096700032, 'factuality': 0.6487543299794197, 'flesch_reading_ease': 34.11356237484331}\n",
            "\n",
            "Tabla comparativa:\n",
            "                          Modelo Base  Fine-tuned (LoRA)\n",
            "BERTScore F1                 0.865482           0.863699\n",
            "AlignScore (Factualidad)     0.658362           0.648754\n",
            "Flesch Reading Ease         32.773051          34.113562\n",
            "\n",
            "Diferencias (Fine-tuned - Base):\n",
            "\u0394 BERTScore F1: -0.0017835179964701409\n",
            "\u0394 Factuality  : -0.009608116249243404\n",
            "\u0394 Flesch      : 1.340511435521769\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EVALUANDO MODELO BASE\")\n",
        "print(\"=\"*80)\n",
        "base_results = evaluate_model(base_model, base_tokenizer, eval_pairs, sample_size=min(30, len(eval_pairs)))\n",
        "BASE_MODEL_METRICS = summarize_metrics(base_results)\n",
        "print(\"Base:\", BASE_MODEL_METRICS)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EVALUANDO MODELO FINE-TUNED (LoRA)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# modelo fine-tuned es model_lora que ya est\u00e1 en memoria\n",
        "finetuned_model = model_lora\n",
        "finetuned_results = evaluate_model(finetuned_model, base_tokenizer, eval_pairs, sample_size=min(30, len(eval_pairs)))\n",
        "FINETUNED_MODEL_METRICS = summarize_metrics(finetuned_results)\n",
        "print(\"Fine-tuned:\", FINETUNED_MODEL_METRICS)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "metrics_table = {\n",
        "    \"BERTScore F1\": {\n",
        "        \"Modelo Base\": BASE_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        \"Fine-tuned (LoRA)\": FINETUNED_MODEL_METRICS[\"bertscore_f1\"],\n",
        "    },\n",
        "    \"AlignScore (Factualidad)\": {\n",
        "        \"Modelo Base\": BASE_MODEL_METRICS[\"factuality\"],\n",
        "        \"Fine-tuned (LoRA)\": FINETUNED_MODEL_METRICS[\"factuality\"],\n",
        "    },\n",
        "    \"Flesch Reading Ease\": {\n",
        "        \"Modelo Base\": BASE_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        \"Fine-tuned (LoRA)\": FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(metrics_table).T\n",
        "print(\"\\nTabla comparativa:\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\nDiferencias (Fine-tuned - Base):\")\n",
        "print(\"\u0394 BERTScore F1:\", FINETUNED_MODEL_METRICS[\"bertscore_f1\"] - BASE_MODEL_METRICS[\"bertscore_f1\"])\n",
        "print(\"\u0394 Factuality  :\", FINETUNED_MODEL_METRICS[\"factuality\"] - BASE_MODEL_METRICS[\"factuality\"])\n",
        "print(\"\u0394 Flesch      :\", FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"] - BASE_MODEL_METRICS[\"flesch_reading_ease\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SQiDNJg3-N3m",
        "outputId": "4015ccc8-0bf9-4e69-bdb6-ce1843f98008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting stable-baselines3==2.3.0\n",
            "  Downloading stable_baselines3-2.3.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting sb3-contrib==2.3.0\n",
            "  Downloading sb3_contrib-2.3.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting gymnasium==0.29.1\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3==2.3.0) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium==0.29.1) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13->stable-baselines3==2.3.0) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3==2.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3==2.3.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3==2.3.0) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13->stable-baselines3==2.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13->stable-baselines3==2.3.0) (3.0.3)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'stable-baselines3' candidate (version 2.3.0 at https://files.pythonhosted.org/packages/51/0b/6539076ed58343f1404dea0462167b079b5264508b8e5bbed01cea9f66b8/stable_baselines3-2.3.0-py3-none-any.whl (from https://pypi.org/simple/stable-baselines3/) (requires-python:>=3.8))\n",
            "Reason for being yanked: Loading broken with PyTorch 1.13\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading stable_baselines3-2.3.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m182.1/182.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sb3_contrib-2.3.0-py3-none-any.whl (80 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium, stable-baselines3, sb3-contrib\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.2.2\n",
            "    Uninstalling gymnasium-1.2.2:\n",
            "      Successfully uninstalled gymnasium-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gymnasium-0.29.1 sb3-contrib-2.3.0 stable-baselines3-2.3.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "4c470e7a9e294a67b378ef5111274d47",
              "pip_warning": {
                "packages": [
                  "gymnasium"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install stable-baselines3==2.3.0 sb3-contrib==2.3.0 gymnasium==0.29.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgqUMMxc-SWH",
        "outputId": "79cd81d3-fdb7-4d86-d01b-c65f7d37d464"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import stable_baselines3\n",
        "print(stable_baselines3.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvlmwfO09xSy",
        "outputId": "763a2a6e-97de-4e00-98f6-f7c09827655f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entrenando TD3 para MODELO BASE...\n",
            "Using cuda device\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.07it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.23s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 85       |\n",
            "|    total_timesteps | 4        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.34s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.22it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.665    |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 169      |\n",
            "|    total_timesteps | 8        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.61s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.49s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.674    |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 12       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.54it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.70it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.62it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.661    |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 337      |\n",
            "|    total_timesteps | 16       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.55s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.29it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.666    |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 422      |\n",
            "|    total_timesteps | 20       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.80it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.99it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.667    |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 505      |\n",
            "|    total_timesteps | 24       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.24it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.48it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.67     |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 589      |\n",
            "|    total_timesteps | 28       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.50it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.12it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 672      |\n",
            "|    total_timesteps | 32       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.46it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.53s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.71it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.682    |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 756      |\n",
            "|    total_timesteps | 36       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.48it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.41s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.31s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.685    |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 841      |\n",
            "|    total_timesteps | 40       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.81it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.88it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.21it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.682    |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 924      |\n",
            "|    total_timesteps | 44       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.07s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.56it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.683    |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1010     |\n",
            "|    total_timesteps | 48       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.33s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.76it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.94it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1095     |\n",
            "|    total_timesteps | 52       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.53s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.98it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1181     |\n",
            "|    total_timesteps | 56       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.09it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.59it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.02it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.12s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1264     |\n",
            "|    total_timesteps | 60       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.31s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.98it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1349     |\n",
            "|    total_timesteps | 64       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.69it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.78it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.18s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1433     |\n",
            "|    total_timesteps | 68       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.18it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.72it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.07it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1516     |\n",
            "|    total_timesteps | 72       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.99it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.677    |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1599     |\n",
            "|    total_timesteps | 76       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.38it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.87it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1681     |\n",
            "|    total_timesteps | 80       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.40s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.24s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.66s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.00s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1769     |\n",
            "|    total_timesteps | 84       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.28it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.677    |\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1852     |\n",
            "|    total_timesteps | 88       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.18s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 10.69it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.32s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1937     |\n",
            "|    total_timesteps | 92       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.82it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2021     |\n",
            "|    total_timesteps | 96       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.16s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.24it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2106     |\n",
            "|    total_timesteps | 100      |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.49s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.79it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12.65it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.677    |\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2191     |\n",
            "|    total_timesteps | 104      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.698   |\n",
            "|    critic_loss     | 0.0435   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.66it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.43s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 108      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2277     |\n",
            "|    total_timesteps | 108      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.813   |\n",
            "|    critic_loss     | 0.0691   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.63s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.677    |\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2364     |\n",
            "|    total_timesteps | 112      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.508   |\n",
            "|    critic_loss     | 0.0522   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.69it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.39s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.75s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 116      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2450     |\n",
            "|    total_timesteps | 116      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.614   |\n",
            "|    critic_loss     | 0.039    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.42it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 120      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2535     |\n",
            "|    total_timesteps | 120      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.834   |\n",
            "|    critic_loss     | 0.0259   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2619     |\n",
            "|    total_timesteps | 124      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.798   |\n",
            "|    critic_loss     | 0.0218   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.13s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.12it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 128      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2705     |\n",
            "|    total_timesteps | 128      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.607   |\n",
            "|    critic_loss     | 0.0127   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.05it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.94it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.34s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2787     |\n",
            "|    total_timesteps | 132      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.612   |\n",
            "|    critic_loss     | 0.024    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 31       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.58s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.08it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 136      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2873     |\n",
            "|    total_timesteps | 136      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.768   |\n",
            "|    critic_loss     | 0.0107   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 35       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.59it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.49s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.17it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 140      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2958     |\n",
            "|    total_timesteps | 140      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.762   |\n",
            "|    critic_loss     | 0.0101   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 39       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.42it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.52s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3042     |\n",
            "|    total_timesteps | 144      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.621   |\n",
            "|    critic_loss     | 0.0109   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 43       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.12it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.06it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 148      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3127     |\n",
            "|    total_timesteps | 148      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.617   |\n",
            "|    critic_loss     | 0.0132   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 47       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.77it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.09s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.77it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 152      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3212     |\n",
            "|    total_timesteps | 152      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.728   |\n",
            "|    critic_loss     | 0.00615  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 51       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.62it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.73it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 156      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3296     |\n",
            "|    total_timesteps | 156      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.716   |\n",
            "|    critic_loss     | 0.0132   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 55       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.35s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.21it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 160      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3381     |\n",
            "|    total_timesteps | 160      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.635   |\n",
            "|    critic_loss     | 0.00706  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 59       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.01it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.60it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.79it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 164      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3467     |\n",
            "|    total_timesteps | 164      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.674   |\n",
            "|    critic_loss     | 0.00924  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 63       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.98it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.06s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.677    |\n",
            "| time/              |          |\n",
            "|    episodes        | 168      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3554     |\n",
            "|    total_timesteps | 168      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.71    |\n",
            "|    critic_loss     | 0.0103   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 67       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.25s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.39it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.53s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 172      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3640     |\n",
            "|    total_timesteps | 172      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.679   |\n",
            "|    critic_loss     | 0.0112   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 71       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.47it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 176      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3724     |\n",
            "|    total_timesteps | 176      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.672   |\n",
            "|    critic_loss     | 0.0115   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 75       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 31.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.86it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.19s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 180      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3808     |\n",
            "|    total_timesteps | 180      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.709   |\n",
            "|    critic_loss     | 0.00452  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 79       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.63it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.21it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.70it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3893     |\n",
            "|    total_timesteps | 184      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.697   |\n",
            "|    critic_loss     | 0.00666  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 83       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.12s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.47s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.18s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.677    |\n",
            "| time/              |          |\n",
            "|    episodes        | 188      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3982     |\n",
            "|    total_timesteps | 188      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.644   |\n",
            "|    critic_loss     | 0.0112   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 87       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.23it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.40s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.60it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4066     |\n",
            "|    total_timesteps | 192      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.705   |\n",
            "|    critic_loss     | 0.0111   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 91       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.26it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.92it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.98it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 196      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4150     |\n",
            "|    total_timesteps | 196      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.652   |\n",
            "|    critic_loss     | 0.0106   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 95       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.36it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.53s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 200      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4235     |\n",
            "|    total_timesteps | 200      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.7     |\n",
            "|    critic_loss     | 0.00755  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 99       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Entrenando TD3 para MODELO LoRA...\n",
            "Using cuda device\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.68it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.62it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.37s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.665    |\n",
            "| time/              |          |\n",
            "|    episodes        | 4        |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 85       |\n",
            "|    total_timesteps | 4        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.23s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.44it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.53it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.654    |\n",
            "| time/              |          |\n",
            "|    episodes        | 8        |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 169      |\n",
            "|    total_timesteps | 8        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.34s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.10it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.673    |\n",
            "| time/              |          |\n",
            "|    episodes        | 12       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 254      |\n",
            "|    total_timesteps | 12       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.18it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.91it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.42s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 16       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 339      |\n",
            "|    total_timesteps | 16       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.17it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.29s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.674    |\n",
            "| time/              |          |\n",
            "|    episodes        | 20       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 425      |\n",
            "|    total_timesteps | 20       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.61s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 23.14it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 24       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 510      |\n",
            "|    total_timesteps | 24       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.88it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.32it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.673    |\n",
            "| time/              |          |\n",
            "|    episodes        | 28       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 594      |\n",
            "|    total_timesteps | 28       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.56it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.45s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.677    |\n",
            "| time/              |          |\n",
            "|    episodes        | 32       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 679      |\n",
            "|    total_timesteps | 32       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.91it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.49it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.666    |\n",
            "| time/              |          |\n",
            "|    episodes        | 36       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 763      |\n",
            "|    total_timesteps | 36       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.72it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.45it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.54it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.661    |\n",
            "| time/              |          |\n",
            "|    episodes        | 40       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 846      |\n",
            "|    total_timesteps | 40       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.28it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.36it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.29s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.664    |\n",
            "| time/              |          |\n",
            "|    episodes        | 44       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 931      |\n",
            "|    total_timesteps | 44       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.32it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.30it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.668    |\n",
            "| time/              |          |\n",
            "|    episodes        | 48       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1016     |\n",
            "|    total_timesteps | 48       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.50it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.71it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.47it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.665    |\n",
            "| time/              |          |\n",
            "|    episodes        | 52       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1099     |\n",
            "|    total_timesteps | 52       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.80it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.664    |\n",
            "| time/              |          |\n",
            "|    episodes        | 56       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1184     |\n",
            "|    total_timesteps | 56       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.45s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.45it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.665    |\n",
            "| time/              |          |\n",
            "|    episodes        | 60       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1270     |\n",
            "|    total_timesteps | 60       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.63it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.12it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.67     |\n",
            "| time/              |          |\n",
            "|    episodes        | 64       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1357     |\n",
            "|    total_timesteps | 64       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.03s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.50s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.50s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 68       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1445     |\n",
            "|    total_timesteps | 68       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.42it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.68it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 72       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1529     |\n",
            "|    total_timesteps | 72       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.84it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.51it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.42it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 76       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1612     |\n",
            "|    total_timesteps | 76       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.01it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.46it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 80       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1696     |\n",
            "|    total_timesteps | 80       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.09it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.43it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 84       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1779     |\n",
            "|    total_timesteps | 84       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 22.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.675    |\n",
            "| time/              |          |\n",
            "|    episodes        | 88       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1864     |\n",
            "|    total_timesteps | 88       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.16it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.53it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.10it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 92       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 1947     |\n",
            "|    total_timesteps | 92       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.46s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.45it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.10s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 96       |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2034     |\n",
            "|    total_timesteps | 96       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.05it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.57it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.676    |\n",
            "| time/              |          |\n",
            "|    episodes        | 100      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2118     |\n",
            "|    total_timesteps | 100      |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.76it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 104      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2202     |\n",
            "|    total_timesteps | 104      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.627   |\n",
            "|    critic_loss     | 0.0274   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 3        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.41s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.27s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.75it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 108      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2289     |\n",
            "|    total_timesteps | 108      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.853   |\n",
            "|    critic_loss     | 0.107    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 7        |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.33s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.07it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.67it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 112      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2374     |\n",
            "|    total_timesteps | 112      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.541   |\n",
            "|    critic_loss     | 0.0574   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 11       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.25it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.06s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  9.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.678    |\n",
            "| time/              |          |\n",
            "|    episodes        | 116      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2459     |\n",
            "|    total_timesteps | 116      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.629   |\n",
            "|    critic_loss     | 0.0499   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 15       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.28it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.57it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.97it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 120      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2542     |\n",
            "|    total_timesteps | 120      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.886   |\n",
            "|    critic_loss     | 0.0188   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 19       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.63s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.32it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.74s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.677    |\n",
            "| time/              |          |\n",
            "|    episodes        | 124      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2628     |\n",
            "|    total_timesteps | 124      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.952   |\n",
            "|    critic_loss     | 0.0352   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 23       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.89it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.10it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.20it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.679    |\n",
            "| time/              |          |\n",
            "|    episodes        | 128      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2712     |\n",
            "|    total_timesteps | 128      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.706   |\n",
            "|    critic_loss     | 0.016    |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 27       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.53it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.10it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 23.36it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.68     |\n",
            "| time/              |          |\n",
            "|    episodes        | 132      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2796     |\n",
            "|    total_timesteps | 132      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.57    |\n",
            "|    critic_loss     | 0.0313   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 31       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.39s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.83it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.56it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 136      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2881     |\n",
            "|    total_timesteps | 136      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.712   |\n",
            "|    critic_loss     | 0.0177   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 35       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.95it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.41it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.28it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.687    |\n",
            "| time/              |          |\n",
            "|    episodes        | 140      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 2966     |\n",
            "|    total_timesteps | 140      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.802   |\n",
            "|    critic_loss     | 0.0152   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 39       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.75it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.74it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.01it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.36s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.687    |\n",
            "| time/              |          |\n",
            "|    episodes        | 144      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3051     |\n",
            "|    total_timesteps | 144      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.698   |\n",
            "|    critic_loss     | 0.0185   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 43       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.12it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.04it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.90it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.685    |\n",
            "| time/              |          |\n",
            "|    episodes        | 148      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3135     |\n",
            "|    total_timesteps | 148      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.641   |\n",
            "|    critic_loss     | 0.0116   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 47       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.29it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.686    |\n",
            "| time/              |          |\n",
            "|    episodes        | 152      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3221     |\n",
            "|    total_timesteps | 152      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.69    |\n",
            "|    critic_loss     | 0.0062   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 51       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.62s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.13it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.62it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.688    |\n",
            "| time/              |          |\n",
            "|    episodes        | 156      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3303     |\n",
            "|    total_timesteps | 156      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.714   |\n",
            "|    critic_loss     | 0.00825  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 55       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.01it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.26it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.69s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.688    |\n",
            "| time/              |          |\n",
            "|    episodes        | 160      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3389     |\n",
            "|    total_timesteps | 160      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.686   |\n",
            "|    critic_loss     | 0.00783  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 59       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.62s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.47it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.60it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 164      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3473     |\n",
            "|    total_timesteps | 164      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.687   |\n",
            "|    critic_loss     | 0.0136   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 63       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.86it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.55it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.681    |\n",
            "| time/              |          |\n",
            "|    episodes        | 168      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3559     |\n",
            "|    total_timesteps | 168      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.668   |\n",
            "|    critic_loss     | 0.0102   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 67       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.48s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.42it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.61s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.682    |\n",
            "| time/              |          |\n",
            "|    episodes        | 172      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3645     |\n",
            "|    total_timesteps | 172      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.662   |\n",
            "|    critic_loss     | 0.0087   |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 71       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.00s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.64it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.73it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 176      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3729     |\n",
            "|    total_timesteps | 176      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.689   |\n",
            "|    critic_loss     | 0.00561  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 75       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.85it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 11.33it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.52it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 180      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3813     |\n",
            "|    total_timesteps | 180      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.683   |\n",
            "|    critic_loss     | 0.00476  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 79       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.62it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  7.11it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.56it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  5.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.685    |\n",
            "| time/              |          |\n",
            "|    episodes        | 184      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3896     |\n",
            "|    total_timesteps | 184      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.695   |\n",
            "|    critic_loss     | 0.00941  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 83       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.57s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.40it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.03it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  1.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.686    |\n",
            "| time/              |          |\n",
            "|    episodes        | 188      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 3981     |\n",
            "|    total_timesteps | 188      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.672   |\n",
            "|    critic_loss     | 0.00729  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 87       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.31it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.32it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.61it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.687    |\n",
            "| time/              |          |\n",
            "|    episodes        | 192      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4065     |\n",
            "|    total_timesteps | 192      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.7     |\n",
            "|    critic_loss     | 0.00703  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 91       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.75it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  8.58it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.12it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  6.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.686    |\n",
            "| time/              |          |\n",
            "|    episodes        | 196      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4150     |\n",
            "|    total_timesteps | 196      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.695   |\n",
            "|    critic_loss     | 0.00981  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 95       |\n",
            "---------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  2.96it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 18.57it/s]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.55s/it]\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Evaluating: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  3.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1        |\n",
            "|    ep_rew_mean     | 0.684    |\n",
            "| time/              |          |\n",
            "|    episodes        | 200      |\n",
            "|    fps             | 0        |\n",
            "|    time_elapsed    | 4235     |\n",
            "|    total_timesteps | 200      |\n",
            "| train/             |          |\n",
            "|    actor_loss      | -0.67    |\n",
            "|    critic_loss     | 0.00826  |\n",
            "|    learning_rate   | 0.001    |\n",
            "|    n_updates       | 99       |\n",
            "---------------------------------\n",
            "Entrenamiento TD3 completado.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# SECCI\u00d3N 7: TD3 PARA AJUSTAR LA DECODIFICACI\u00d3N\n",
        "# ==============================================================\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "from stable_baselines3 import TD3\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7.1 Helper: construir prompt y generar resumen con temperatura/top_p\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "def build_prompt(medical_text: str) -> str:\n",
        "    \"\"\"Plantilla de prompt (aj\u00fastala si en tu notebook usas otra).\"\"\"\n",
        "    return (\n",
        "        \"You are a specialist in healthcare communication. \"\n",
        "        \"Use the context to transform the following medical text into a clear, \"\n",
        "        \"concise, and easy-to-understand summary for a patient and their family. \"\n",
        "        \"Retain all relevant clinical data, but explain technical terms using simple \"\n",
        "        \"language and short sentences.\\n\\n\"\n",
        "        \"### Medical text:\\n\"\n",
        "        f\"{medical_text}\\n\\n\"\n",
        "        \"### Simplified summary:\\n\"\n",
        "    )\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with_params(model, tokenizer, medical_text: str,\n",
        "                         temperature: float = 0.7,\n",
        "                         top_p: float = 0.9,\n",
        "                         max_new_tokens: int = 256) -> str:\n",
        "    \"\"\"Genera un resumen usando el modelo con ciertos par\u00e1metros de decodificaci\u00f3n.\"\"\"\n",
        "    prompt = build_prompt(medical_text)\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=1024\n",
        "    ).to(model.device)\n",
        "\n",
        "    output_ids = model.generate(\n",
        "        **inputs,\n",
        "        do_sample=True,\n",
        "        temperature=float(temperature),\n",
        "        top_p=float(top_p),\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    full_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    # Devolvemos solo lo que viene despu\u00e9s de \"### Simplified summary:\"\n",
        "    if \"### Simplified summary:\" in full_text:\n",
        "        return full_text.split(\"### Simplified summary:\")[-1].strip()\n",
        "    return full_text.strip()\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7.2 Definici\u00f3n del entorno Gym para TD3\n",
        "#     - Acci\u00f3n: [temperature, top_p] en [0.1, 1.0]\n",
        "#     - Observaci\u00f3n: vector simple con longitud del texto\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "class SummarizationTD3Env(gym.Env):\n",
        "    metadata = {\"render.modes\": []}\n",
        "\n",
        "    def __init__(self, model, tokenizer, pairs, evaluator, max_new_tokens=256):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pairs = pairs\n",
        "        self.evaluator = evaluator\n",
        "        self.max_new_tokens = max_new_tokens\n",
        "\n",
        "        # Acci\u00f3n continua: temperatura y top_p\n",
        "        self.action_space = spaces.Box(\n",
        "            low=np.array([0.1, 0.1], dtype=np.float32),\n",
        "            high=np.array([1.0, 1.0], dtype=np.float32),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        # Observaci\u00f3n: [len(medical_characters_normalized)]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=np.array([0.0], dtype=np.float32),\n",
        "            high=np.array([1.0], dtype=np.float32),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.current_index = 0\n",
        "\n",
        "    def _get_obs_for_index(self, idx: int):\n",
        "        med = self.pairs[idx].medical\n",
        "        n_chars = len(med)\n",
        "        # Normalizamos por 8000 chars para tener algo entre 0 y 1\n",
        "        return np.array([min(1.0, n_chars / 8000.0)], dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_index = np.random.randint(0, len(self.pairs))\n",
        "        obs = self._get_obs_for_index(self.current_index)\n",
        "        return obs, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # Convertir acci\u00f3n a array numpy si es necesario\n",
        "        action = np.asarray(action, dtype=np.float32)\n",
        "        \n",
        "        # Manejar diferentes formatos de acci\u00f3n\n",
        "        if action.ndim == 0:  # Escalar\n",
        "            temperature = float(np.clip(action, 0.1, 1.0))\n",
        "            top_p = float(np.clip(action, 0.1, 1.0))\n",
        "        elif action.ndim == 1:\n",
        "            if len(action) >= 2:\n",
        "                temperature = float(np.clip(action[0], 0.1, 1.0))\n",
        "                top_p = float(np.clip(action[1], 0.1, 1.0))\n",
        "            elif len(action) == 1:\n",
        "                temperature = float(np.clip(action[0], 0.1, 1.0))\n",
        "                top_p = float(np.clip(action[0], 0.1, 1.0))\n",
        "            else:\n",
        "                temperature = 0.7\n",
        "                top_p = 0.9\n",
        "        else:\n",
        "            action_flat = action.flatten()\n",
        "            if len(action_flat) >= 2:\n",
        "                temperature = float(np.clip(action_flat[0], 0.1, 1.0))\n",
        "                top_p = float(np.clip(action_flat[1], 0.1, 1.0))\n",
        "            else:\n",
        "                temperature = float(np.clip(action_flat[0], 0.1, 1.0))\n",
        "                top_p = float(np.clip(action_flat[0], 0.1, 1.0))\n",
        "\n",
        "        pair = self.pairs[self.current_index]\n",
        "        medical = pair.medical\n",
        "        reference = pair.plain\n",
        "        source = pair.medical  # para factualidad usamos el texto t\u00e9cnico\n",
        "\n",
        "        # Generar resumen\n",
        "        generated = generate_with_params(\n",
        "            self.model,\n",
        "            self.tokenizer,\n",
        "            medical,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            max_new_tokens=self.max_new_tokens,\n",
        "        )\n",
        "\n",
        "        # M\u00e9tricas con manejo de errores\n",
        "        try:\n",
        "            metrics = self.evaluator.evaluate(\n",
        "                generated=generated,\n",
        "                reference=reference,\n",
        "                source=source,\n",
        "            )\n",
        "\n",
        "            bert_f1 = metrics[\"relevance\"][\"f1\"]\n",
        "            fact = metrics[\"factuality\"][\"score\"]\n",
        "            fre = metrics[\"readability\"][\"flesch_reading_ease\"]\n",
        "        except Exception as e:\n",
        "            # Si hay error en la evaluaci\u00f3n, usar valores por defecto (recompensa baja)\n",
        "            print(f\"[WARN] Error en evaluaci\u00f3n TD3: {e}\")\n",
        "            bert_f1 = 0.0\n",
        "            fact = 0.0\n",
        "            fre = 0.0\n",
        "\n",
        "        # Normalizar Flesch (0-100) -> [0,1]\n",
        "        fre_norm = max(0.0, min(100.0, fre)) / 100.0\n",
        "\n",
        "        # -----------------------------\n",
        "        # REWARD (aj\u00fastalo a tu gusto)\n",
        "        # -----------------------------\n",
        "        # Damos m\u00e1s peso a factualidad, luego relevancia y legibilidad\n",
        "        reward = (\n",
        "            0.4 * fact +\n",
        "            0.4 * bert_f1 +\n",
        "            0.2 * fre_norm\n",
        "        )\n",
        "\n",
        "        obs = self._get_obs_for_index(self.current_index)\n",
        "        terminated = True   # Un paso = un episodio (bandit contextual)\n",
        "        truncated = False\n",
        "        info = {\n",
        "            \"metrics\": metrics,\n",
        "            \"temperature\": temperature,\n",
        "            \"top_p\": top_p,\n",
        "        }\n",
        "\n",
        "        # Siguiente episodio usar\u00e1 otro \u00edndice\n",
        "        self.current_index = np.random.randint(0, len(self.pairs))\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7.3 Crear entornos para Base y LoRA\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "# Usamos eval_pairs como \"pool\" para RL (podr\u00edas usar train_pairs tambi\u00e9n)\n",
        "rl_pairs = eval_pairs\n",
        "\n",
        "env_base = make_vec_env(\n",
        "    lambda: SummarizationTD3Env(base_model, base_tokenizer, rl_pairs, evaluator),\n",
        "    n_envs=1\n",
        ")\n",
        "\n",
        "env_lora = make_vec_env(\n",
        "    lambda: SummarizationTD3Env(model_lora, base_tokenizer, rl_pairs, evaluator),\n",
        "    n_envs=1\n",
        ")\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 7.4 Entrenar TD3 para Base y para LoRA\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "TD3_BASE_DIR = \"/content/td3_base_agent\"\n",
        "TD3_LORA_DIR = \"/content/td3_lora_agent\"\n",
        "td3_steps = 200  # puedes subir a 1000+ si tienes tiempo\n",
        "\n",
        "# Intentar cargar agentes TD3 existentes, si no existen, entrenarlos\n",
        "if os.path.exists(TD3_BASE_DIR) and os.path.exists(os.path.join(TD3_BASE_DIR, \"policy.pkl\")):\n",
        "    print(\"Cargando agente TD3 BASE existente...\")\n",
        "    td3_base = TD3.load(TD3_BASE_DIR, env=env_base)\n",
        "    print(\"Agente TD3 BASE cargado exitosamente.\")\n",
        "else:\n",
        "    print(\"Entrenando TD3 para MODELO BASE...\")\n",
        "    td3_base = TD3(\n",
        "        \"MlpPolicy\",\n",
        "        env_base,\n",
        "        learning_rate=1e-3,\n",
        "        batch_size=32,\n",
        "        verbose=1,\n",
        "    )\n",
        "    td3_base.learn(total_timesteps=td3_steps)\n",
        "\n",
        "if os.path.exists(TD3_LORA_DIR) and os.path.exists(os.path.join(TD3_LORA_DIR, \"policy.pkl\")):\n",
        "    print(\"\\nCargando agente TD3 LORA existente...\")\n",
        "    td3_lora = TD3.load(TD3_LORA_DIR, env=env_lora)\n",
        "    print(\"Agente TD3 LORA cargado exitosamente.\")\n",
        "else:\n",
        "    print(\"\\nEntrenando TD3 para MODELO LoRA...\")\n",
        "    td3_lora = TD3(\n",
        "        \"MlpPolicy\",\n",
        "        env_lora,\n",
        "        learning_rate=1e-3,\n",
        "        batch_size=32,\n",
        "        verbose=1,\n",
        "    )\n",
        "    td3_lora.learn(total_timesteps=td3_steps)\n",
        "\n",
        "print(\"Entrenamiento TD3 completado.\")\n",
        "\n",
        "# Guardar los agentes TD3 (si fueron entrenados o cargados)\n",
        "os.makedirs(TD3_BASE_DIR, exist_ok=True)\n",
        "os.makedirs(TD3_LORA_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\nGuardando agentes TD3...\")\n",
        "td3_base.save(TD3_BASE_DIR)\n",
        "td3_lora.save(TD3_LORA_DIR)\n",
        "print(f\"\u2713 Agente TD3 BASE guardado en: {TD3_BASE_DIR}\")\n",
        "print(f\"\u2713 Agente TD3 LORA guardado en: {TD3_LORA_DIR}\")\n",
        "print(\"\\nNota: Los agentes TD3 guardados pueden ser cargados en futuras ejecuciones para evitar reentrenar.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Parchear el m\u00e9todo step de los entornos existentes\n",
        "# ==============================================================\n",
        "# Esto es necesario porque los entornos ya fueron creados con la versi\u00f3n antigua\n",
        "# de la clase. Parcheamos el m\u00e9todo step sin tener que recrear los entornos.\n",
        "\n",
        "import types\n",
        "\n",
        "def step_patched(self, action):\n",
        "    \"\"\"Versi\u00f3n corregida del m\u00e9todo step con manejo robusto de acciones.\"\"\"\n",
        "    # Convertir acci\u00f3n a array numpy si es necesario\n",
        "    action = np.asarray(action, dtype=np.float32)\n",
        "    \n",
        "    # Manejar diferentes formatos de acci\u00f3n\n",
        "    if action.ndim == 0:  # Escalar\n",
        "        temperature = float(np.clip(action, 0.1, 1.0))\n",
        "        top_p = float(np.clip(action, 0.1, 1.0))\n",
        "    elif action.ndim == 1:\n",
        "        if len(action) >= 2:\n",
        "            temperature = float(np.clip(action[0], 0.1, 1.0))\n",
        "            top_p = float(np.clip(action[1], 0.1, 1.0))\n",
        "        elif len(action) == 1:\n",
        "            temperature = float(np.clip(action[0], 0.1, 1.0))\n",
        "            top_p = float(np.clip(action[0], 0.1, 1.0))\n",
        "        else:\n",
        "            temperature = 0.7\n",
        "            top_p = 0.9\n",
        "    else:\n",
        "        action_flat = action.flatten()\n",
        "        if len(action_flat) >= 2:\n",
        "            temperature = float(np.clip(action_flat[0], 0.1, 1.0))\n",
        "            top_p = float(np.clip(action_flat[1], 0.1, 1.0))\n",
        "        else:\n",
        "            temperature = float(np.clip(action_flat[0], 0.1, 1.0))\n",
        "            top_p = float(np.clip(action_flat[0], 0.1, 1.0))\n",
        "\n",
        "    pair = self.pairs[self.current_index]\n",
        "    medical = pair.medical\n",
        "    reference = pair.plain\n",
        "    source = pair.medical\n",
        "\n",
        "    # Generar resumen\n",
        "    generated = generate_with_params(\n",
        "        self.model,\n",
        "        self.tokenizer,\n",
        "        medical,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        max_new_tokens=self.max_new_tokens,\n",
        "    )\n",
        "\n",
        "    # M\u00e9tricas con manejo de errores\n",
        "    try:\n",
        "        metrics = self.evaluator.evaluate(\n",
        "            generated=generated,\n",
        "            reference=reference,\n",
        "            source=source,\n",
        "        )\n",
        "\n",
        "        bert_f1 = metrics[\"relevance\"][\"f1\"]\n",
        "        fact = metrics[\"factuality\"][\"score\"]\n",
        "        fre = metrics[\"readability\"][\"flesch_reading_ease\"]\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] Error en evaluaci\u00f3n TD3: {e}\")\n",
        "        bert_f1 = 0.0\n",
        "        fact = 0.0\n",
        "        fre = 0.0\n",
        "        # Crear m\u00e9tricas por defecto para evitar error en info\n",
        "        metrics = {\n",
        "            \"relevance\": {\"f1\": 0.0},\n",
        "            \"factuality\": {\"score\": 0.0},\n",
        "            \"readability\": {\"flesch_reading_ease\": 0.0}\n",
        "        }\n",
        "\n",
        "    # Normalizar Flesch (0-100) -> [0,1]\n",
        "    fre_norm = max(0.0, min(100.0, fre)) / 100.0\n",
        "\n",
        "    # REWARD\n",
        "    reward = (\n",
        "        0.4 * fact +\n",
        "        0.4 * bert_f1 +\n",
        "        0.2 * fre_norm\n",
        "    )\n",
        "\n",
        "    obs = self._get_obs_for_index(self.current_index)\n",
        "    terminated = True\n",
        "    truncated = False\n",
        "    info = {\n",
        "        \"metrics\": metrics,\n",
        "        \"temperature\": temperature,\n",
        "        \"top_p\": top_p,\n",
        "    }\n",
        "\n",
        "    # Siguiente episodio usar\u00e1 otro \u00edndice\n",
        "    self.current_index = np.random.randint(0, len(self.pairs))\n",
        "\n",
        "    return obs, reward, terminated, truncated, info\n",
        "\n",
        "# Aplicar el parche a los entornos existentes\n",
        "env_base.envs[0].env.step = types.MethodType(step_patched, env_base.envs[0].env)\n",
        "env_lora.envs[0].env.step = types.MethodType(step_patched, env_lora.envs[0].env)\n",
        "\n",
        "print(\"\u2713 M\u00e9todo step parcheado en entornos existentes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Gc0mqe8c93Ud",
        "outputId": "a2c20e07-88aa-48d6-c8e0-08b7ba961248"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1709285401.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtemp_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mean_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd3_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_base\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mtemp_lora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_p_lora\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_mean_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtd3_lora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_lora\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1709285401.py\u001b[0m in \u001b[0;36mget_mean_action\u001b[0;34m(agent, env, n_samples)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_mean_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"\"\"Promedia acciones del agente sobre varios estados aleatorios.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
          ]
        }
      ],
      "source": [
        "# ==============================================================\n",
        "# 7.5 Obtener par\u00e1metros de decodificaci\u00f3n \"\u00f3ptimos\" de TD3\n",
        "# ==============================================================\n",
        "\n",
        "def get_mean_action(agent, env, n_samples=64):\n",
        "    \"\"\"Promedia acciones del agente sobre varios estados aleatorios.\"\"\"\n",
        "    # En entornos vectorizados de stable-baselines3, reset() devuelve solo la observaci\u00f3n\n",
        "    obs = env.reset()\n",
        "    \n",
        "    actions = []\n",
        "    for _ in range(n_samples):\n",
        "        action, _ = agent.predict(obs, deterministic=True)\n",
        "        # Convertir acci\u00f3n a array numpy y asegurar que tenga 2 elementos\n",
        "        action = np.asarray(action, dtype=np.float32)\n",
        "        if action.ndim == 0:  # Escalar\n",
        "            action = np.array([action, action], dtype=np.float32)\n",
        "        elif action.ndim == 1:\n",
        "            if len(action) == 1:\n",
        "                action = np.array([action[0], action[0]], dtype=np.float32)\n",
        "            elif len(action) > 2:\n",
        "                action = action[:2]  # Tomar solo los primeros 2 elementos\n",
        "        else:\n",
        "            # Multidimensional, aplanar y tomar primeros 2\n",
        "            action_flat = action.flatten()\n",
        "            if len(action_flat) >= 2:\n",
        "                action = action_flat[:2]\n",
        "            else:\n",
        "                action = np.array([action_flat[0], action_flat[0]], dtype=np.float32)\n",
        "        \n",
        "        actions.append(action)\n",
        "        \n",
        "        # step() devuelve (obs, reward, done, info) o (obs, reward, terminated, truncated, info)\n",
        "        step_result = env.step(action)\n",
        "        obs = step_result[0]\n",
        "    \n",
        "    actions = np.array(actions)\n",
        "    mean_action = actions.mean(axis=0)\n",
        "    temperature = float(np.clip(mean_action[0], 0.1, 1.0))\n",
        "    top_p = float(np.clip(mean_action[1], 0.1, 1.0))\n",
        "    return temperature, top_p\n",
        "\n",
        "# Ajustar n_samples para reducir tiempo (por defecto 64)\n",
        "# Menos muestras = m\u00e1s r\u00e1pido pero menos preciso\n",
        "# M\u00e1s muestras = m\u00e1s lento pero m\u00e1s preciso\n",
        "N_SAMPLES = 32 \n",
        "\n",
        "print(f\"Calculando par\u00e1metros \u00f3ptimos con {N_SAMPLES} muestras por modelo...\")\n",
        "print(\"Esto puede tomar 10-30 minutos dependiendo de tu GPU...\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "temp_base, top_p_base = get_mean_action(td3_base, env_base, n_samples=N_SAMPLES)\n",
        "print(f\"\u2713 Base completado: temperature={temp_base:.3f}, top_p={top_p_base:.3f}\")\n",
        "\n",
        "temp_lora, top_p_lora = get_mean_action(td3_lora, env_lora, n_samples=N_SAMPLES)\n",
        "print(f\"\u2713 LoRA completado: temperature={temp_lora:.3f}, top_p={top_p_lora:.3f}\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(\"=\"*80)\n",
        "print(f\"Tiempo total: {elapsed_time/60:.1f} minutos ({elapsed_time:.0f} segundos)\")\n",
        "print(f\"\\nPar\u00e1metros TD3 - BASE: temperature={temp_base:.3f}, top_p={top_p_base:.3f}\")\n",
        "print(f\"Par\u00e1metros TD3 - LORA: temperature={temp_lora:.3f}, top_p={top_p_lora:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Tabla de resultados formateada: BASE + TD3 vs LORA + TD3\n",
        "# ==============================================================\n",
        "import pandas as pd\n",
        "\n",
        "# Verificar que las m\u00e9tricas TD3 est\u00e9n definidas\n",
        "if 'BASE_TD3_METRICS' not in globals() or 'LORA_TD3_METRICS' not in globals():\n",
        "    print(\"\u26a0\ufe0f ADVERTENCIA: BASE_TD3_METRICS o LORA_TD3_METRICS no est\u00e1n definidas.\")\n",
        "    print(\"Por favor, ejecuta primero la celda 17 que calcula estas m\u00e9tricas.\")\n",
        "    print(\"Esta celda requiere que se hayan evaluado los modelos con TD3.\")\n",
        "else:\n",
        "    # Crear tabla con los resultados\n",
        "    resultados_td3 = {\n",
        "        \"M\u00e9trica\": [\n",
        "            \"BERTScore F1\",\n",
        "            \"AlignScore (Factualidad)\",\n",
        "            \"Flesch Reading Ease\"\n",
        "        ],\n",
        "        \"BASE + TD3\": [\n",
        "            BASE_TD3_METRICS[\"bertscore_f1\"],\n",
        "            BASE_TD3_METRICS[\"factuality\"],\n",
        "            BASE_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "        ],\n",
        "        \"LORA + TD3\": [\n",
        "            LORA_TD3_METRICS[\"bertscore_f1\"],\n",
        "            LORA_TD3_METRICS[\"factuality\"],\n",
        "            LORA_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    df_td3 = pd.DataFrame(resultados_td3)\n",
        "    df_td3.set_index(\"M\u00e9trica\", inplace=True)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TABLA DE RESULTADOS: BASE + TD3 vs LORA + TD3\")\n",
        "    print(\"=\"*80)\n",
        "    print(df_td3.to_string())\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Calcular diferencias\n",
        "    print(\"\\nDiferencias (LORA + TD3 - BASE + TD3):\")\n",
        "    print(f\"\u0394 BERTScore F1:        {LORA_TD3_METRICS['bertscore_f1'] - BASE_TD3_METRICS['bertscore_f1']:+.4f}\")\n",
        "    print(f\"\u0394 AlignScore (Fact.):  {LORA_TD3_METRICS['factuality'] - BASE_TD3_METRICS['factuality']:+.4f}\")\n",
        "    print(f\"\u0394 Flesch Reading Ease: {LORA_TD3_METRICS['flesch_reading_ease'] - BASE_TD3_METRICS['flesch_reading_ease']:+.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1E157qY96Ek"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 7.6 Evaluar modelos con TD3 vs sin TD3\n",
        "# ==============================================================\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def evaluate_model_with_params(model, tokenizer, pairs, sample_size=30,\n",
        "                               temperature=0.7, top_p=0.9, max_new_tokens=256):\n",
        "    sample = pairs[:sample_size]\n",
        "    results = []\n",
        "    for pair in tqdm(sample, desc=\"Evaluating with TD3 params\"):\n",
        "        medical = pair.medical\n",
        "        reference = pair.plain\n",
        "        source = pair.medical\n",
        "\n",
        "        generated = generate_with_params(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            medical,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "\n",
        "        metrics = evaluator.evaluate(generated, reference, source)\n",
        "        results.append(metrics)\n",
        "\n",
        "    # Resumimos igualmente que en summarize_metrics\n",
        "    bert_f1 = np.mean([m[\"relevance\"][\"f1\"] for m in results])\n",
        "    fact = np.mean([m[\"factuality\"][\"score\"] for m in results])\n",
        "    fre = np.mean([m[\"readability\"][\"flesch_reading_ease\"] for m in results])\n",
        "\n",
        "    summary = {\n",
        "        \"bertscore_f1\": float(bert_f1),\n",
        "        \"factuality\": float(fact),\n",
        "        \"flesch_reading_ease\": float(fre),\n",
        "    }\n",
        "    \n",
        "    # Devolver tanto el resumen como los resultados individuales\n",
        "    return summary, results\n",
        "# Usamos el mismo eval_pairs y sample_size que antes\n",
        "sample_size = min(30, len(eval_pairs))\n",
        "\n",
        "print(\"\\n=== Evaluando BASE + TD3 ===\")\n",
        "BASE_TD3_METRICS, base_td3_results = evaluate_model_with_params(\n",
        "    base_model,\n",
        "    base_tokenizer,\n",
        "    eval_pairs,\n",
        "    sample_size=sample_size,\n",
        "    temperature=temp_base,\n",
        "    top_p=top_p_base,\n",
        ")\n",
        "\n",
        "print(\"\\n=== Evaluando LORA + TD3 ===\")\n",
        "LORA_TD3_METRICS, lora_td3_results = evaluate_model_with_params(\n",
        "    model_lora,\n",
        "    base_tokenizer,\n",
        "    eval_pairs,\n",
        "    sample_size=sample_size,\n",
        "    temperature=temp_lora,\n",
        "    top_p=top_p_lora,\n",
        ")\n",
        "\n",
        "print(\"BASE + TD3:\", BASE_TD3_METRICS)\n",
        "print(\"LORA + TD3:\", LORA_TD3_METRICS)\n",
        "print(f\"\\n\u2713 Resultados individuales guardados: base_td3_results ({len(base_td3_results)} muestras)\")\n",
        "print(f\"\u2713 Resultados individuales guardados: lora_td3_results ({len(lora_td3_results)} muestras)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CulacNt098sC"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 7.7 Tabla comparativa final: Base vs LoRA vs TD3\n",
        "# ==============================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "metrics_table = {\n",
        "    \"BERTScore F1\": {\n",
        "        \"Base\": BASE_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        \"Base + TD3\": BASE_TD3_METRICS[\"bertscore_f1\"],\n",
        "        \"LoRA\": FINETUNED_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        \"LoRA + TD3\": LORA_TD3_METRICS[\"bertscore_f1\"],\n",
        "    },\n",
        "    \"AlignScore (Factualidad)\": {\n",
        "        \"Base\": BASE_MODEL_METRICS[\"factuality\"],\n",
        "        \"Base + TD3\": BASE_TD3_METRICS[\"factuality\"],\n",
        "        \"LoRA\": FINETUNED_MODEL_METRICS[\"factuality\"],\n",
        "        \"LoRA + TD3\": LORA_TD3_METRICS[\"factuality\"],\n",
        "    },\n",
        "    \"Flesch Reading Ease\": {\n",
        "        \"Base\": BASE_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        \"Base + TD3\": BASE_TD3_METRICS[\"flesch_reading_ease\"],\n",
        "        \"LoRA\": FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        \"LoRA + TD3\": LORA_TD3_METRICS[\"flesch_reading_ease\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(metrics_table).T\n",
        "print(\"\\n=== TABLA COMPARATIVA FINAL ===\")\n",
        "print(df)\n",
        "\n",
        "print(\"\\nDiferencias (modelo - Base):\")\n",
        "for name, row in df.iterrows():\n",
        "    print(f\"\\n{name}:\")\n",
        "    for col in df.columns:\n",
        "        if col == \"Base\":\n",
        "            continue\n",
        "        delta = row[col] - row[\"Base\"]\n",
        "        print(f\"  {col} - Base: {delta:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# SECCI\u00d3N 8: EVALUACI\u00d3N DE APIs COMERCIALES\n",
        "# ==============================================================\n",
        "\n",
        "# Instalar librer\u00edas necesarias para APIs comerciales\n",
        "!pip install openai anthropic google-generativeai -q\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from typing import Optional\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ==============================================================\n",
        "# CONFIGURAR API KEYS\n",
        "# ==============================================================\n",
        "# Buscar API keys primero en secretos de Colab, luego en variables de entorno\n",
        "API_KEY_ANTHROPIC = None\n",
        "API_KEY_GEMINI = None\n",
        "API_KEY_OPENAI = None\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    # userdata.get() en la nueva versi\u00f3n solo acepta un argumento\n",
        "    try:\n",
        "        API_KEY_ANTHROPIC = userdata.get(\"API_KEY_ANTHROPIC\")\n",
        "    except (KeyError, Exception):\n",
        "        pass\n",
        "    try:\n",
        "        API_KEY_GEMINI = userdata.get(\"API_KEY_GEMINI\")\n",
        "    except (KeyError, Exception):\n",
        "        pass\n",
        "    try:\n",
        "        API_KEY_OPENAI = userdata.get(\"API_KEY_OPENAI\")\n",
        "    except (KeyError, Exception):\n",
        "        pass\n",
        "except ImportError:\n",
        "    # No estamos en Colab, usar variables de entorno\n",
        "    pass\n",
        "\n",
        "# Si no se encontraron en secretos, buscar en variables de entorno\n",
        "if not API_KEY_ANTHROPIC:\n",
        "    API_KEY_ANTHROPIC = os.getenv(\"API_KEY_ANTHROPIC\")\n",
        "if not API_KEY_GEMINI:\n",
        "    API_KEY_GEMINI = os.getenv(\"API_KEY_GEMINI\")\n",
        "if not API_KEY_OPENAI:\n",
        "    API_KEY_OPENAI = os.getenv(\"API_KEY_OPENAI\")\n",
        "\n",
        "# Verificar que las API keys est\u00e9n configuradas\n",
        "if not API_KEY_ANTHROPIC:\n",
        "    print(\" ADVERTENCIA: API_KEY_ANTHROPIC no est\u00e1 configurada\")\n",
        "if not API_KEY_GEMINI:\n",
        "    print(\" ADVERTENCIA: API_KEY_GEMINI no est\u00e1 configurada\")\n",
        "if not API_KEY_OPENAI:\n",
        "    print(\" ADVERTENCIA: API_KEY_OPENAI no est\u00e1 configurada\")\n",
        "\n",
        "# Importar clientes de APIs\n",
        "try:\n",
        "    import anthropic\n",
        "    # Validar y limpiar API key antes de crear el cliente\n",
        "    if API_KEY_ANTHROPIC:\n",
        "        API_KEY_ANTHROPIC = API_KEY_ANTHROPIC.strip()  # Eliminar espacios\n",
        "        if API_KEY_ANTHROPIC.startswith('sk-ant-'):\n",
        "            try:\n",
        "                client_anthropic = anthropic.Anthropic(api_key=API_KEY_ANTHROPIC)\n",
        "                print(f\" Anthropic configurado correctamente (key: {API_KEY_ANTHROPIC[:10]}...{API_KEY_ANTHROPIC[-5:]})\")\n",
        "            except Exception as e:\n",
        "                client_anthropic = None\n",
        "                print(f\" Error al inicializar Anthropic: {e}\")\n",
        "        else:\n",
        "            client_anthropic = None\n",
        "            print(f\" ADVERTENCIA: API_KEY_ANTHROPIC no tiene el formato correcto (debe empezar con 'sk-ant-'). Valor recibido: {API_KEY_ANTHROPIC[:20]}...\")\n",
        "    else:\n",
        "        client_anthropic = None\n",
        "except ImportError:\n",
        "    client_anthropic = None\n",
        "    print(\" No se pudo importar anthropic\")\n",
        "\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "    if API_KEY_GEMINI:\n",
        "        genai.configure(api_key=API_KEY_GEMINI)\n",
        "        model_gemini = genai.GenerativeModel('gemini-2.5-flash')\n",
        "    else:\n",
        "        model_gemini = None\n",
        "except ImportError:\n",
        "    model_gemini = None\n",
        "    print(\" No se pudo importar google.generativeai\")\n",
        "\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    client_openai = OpenAI(api_key=API_KEY_OPENAI) if API_KEY_OPENAI else None\n",
        "except ImportError:\n",
        "    client_openai = None\n",
        "    print(\" No se pudo importar openai\")\n",
        "\n",
        "# Funci\u00f3n para construir el prompt (mismo que se usa localmente)\n",
        "def build_api_prompt(medical_text: str) -> str:\n",
        "    \"\"\"Construye el prompt para las APIs comerciales (mismo formato que localmente).\"\"\"\n",
        "    return (\n",
        "        \"You are a specialist in healthcare communication. \"\n",
        "        \"Use the context to transform the following medical text into a clear, \"\n",
        "        \"concise, and easy-to-understand summary for a patient and their family. \"\n",
        "        \"Retain all relevant clinical data, but explain technical terms using simple \"\n",
        "        \"language and short sentences.\\n\\n\"\n",
        "        \"### Medical text:\\n\"\n",
        "        f\"{medical_text}\\n\\n\"\n",
        "        \"### Simplified summary:\\n\"\n",
        "    )\n",
        "\n",
        "# Funciones para generar res\u00famenes con cada API\n",
        "def generate_anthropic(medical_text: str, max_retries: int = 3) -> Optional[str]:\n",
        "    \"\"\"Genera resumen usando Anthropic Claude.\"\"\"\n",
        "    if not client_anthropic:\n",
        "        return None\n",
        "    \n",
        "    prompt = build_api_prompt(medical_text)\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            message = client_anthropic.messages.create(\n",
        "                model=\"claude-sonnet-4-20250514\",\n",
        "                max_tokens=1024,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            response = message.content[0].text\n",
        "            # Extraer solo el resumen si hay marcadores\n",
        "            if \"### Simplified summary:\" in response:\n",
        "                return response.split(\"### Simplified summary:\")[-1].strip()\n",
        "            return response.strip()\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)  # Backoff exponencial\n",
        "                continue\n",
        "            print(f\"Error en Anthropic: {e}\")\n",
        "            return None\n",
        "\n",
        "def generate_gemini(medical_text: str, max_retries: int = 3) -> Optional[str]:\n",
        "    \"\"\"Genera resumen usando Google Gemini.\"\"\"\n",
        "    if not model_gemini:\n",
        "        return None\n",
        "    \n",
        "    prompt = build_api_prompt(medical_text)\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = model_gemini.generate_content(\n",
        "                prompt,\n",
        "                generation_config=genai.types.GenerationConfig(\n",
        "                    max_output_tokens=8192,\n",
        "                    temperature=0.7,\n",
        "                )\n",
        "            )\n",
        "            \n",
        "            # Verificar finish_reason antes de acceder a response.text\n",
        "            if response.candidates and len(response.candidates) > 0:\n",
        "                candidate = response.candidates[0]\n",
        "                finish_reason = candidate.finish_reason\n",
        "                \n",
        "                # finish_reason 2 = MAX_TOKENS, 3 = SAFETY, 4 = RECITATION\n",
        "                if finish_reason in [2, 3, 4]:\n",
        "                    if finish_reason == 2:\n",
        "                        print(f\"Warning: Gemini alcanz\u00f3 el l\u00edmite de tokens (finish_reason=2). Usando texto parcial generado.\")\n",
        "                    elif finish_reason == 3:\n",
        "                        print(f\"Warning: Gemini bloque\u00f3 contenido por seguridad (finish_reason=3)\")\n",
        "                    elif finish_reason == 4:\n",
        "                        print(f\"Warning: Gemini bloque\u00f3 contenido por recitaci\u00f3n (finish_reason=4)\")\n",
        "                    # Intentar obtener el texto parcial si existe\n",
        "                    if candidate.content and candidate.content.parts:\n",
        "                        text = candidate.content.parts[0].text\n",
        "                        if text and len(text.strip()) > 10:\n",
        "                            if \"### Simplified summary:\" in text:\n",
        "                                return text.split(\"### Simplified summary:\")[-1].strip()\n",
        "                            return text.strip()\n",
        "                    return None\n",
        "                \n",
        "                # finish_reason 1 = STOP (\u00e9xito)\n",
        "                if finish_reason == 1 and candidate.content and candidate.content.parts:\n",
        "                    text = candidate.content.parts[0].text\n",
        "                    if text:\n",
        "                        if \"### Simplified summary:\" in text:\n",
        "                            return text.split(\"### Simplified summary:\")[-1].strip()\n",
        "                        return text.strip()\n",
        "            \n",
        "            # Fallback: intentar response.text si est\u00e1 disponible\n",
        "            try:\n",
        "                text = response.text\n",
        "                if text:\n",
        "                    if \"### Simplified summary:\" in text:\n",
        "                        return text.split(\"### Simplified summary:\")[-1].strip()\n",
        "                    return text.strip()\n",
        "            except Exception:\n",
        "                pass\n",
        "            \n",
        "            return None\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "                continue\n",
        "            print(f\"Error en Gemini: {e}\")\n",
        "            return None\n",
        "\n",
        "def generate_openai(medical_text: str, max_retries: int = 3) -> Optional[str]:\n",
        "    \"\"\"Genera resumen usando OpenAI GPT.\"\"\"\n",
        "    if not client_openai:\n",
        "        return None\n",
        "    \n",
        "    prompt = build_api_prompt(medical_text)\n",
        "    \n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client_openai.chat.completions.create(\n",
        "                model=\"gpt-4o\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a specialist in healthcare communication.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                max_tokens=1024,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "            text = response.choices[0].message.content\n",
        "            # Extraer solo el resumen si hay marcadores\n",
        "            if \"### Simplified summary:\" in text:\n",
        "                return text.split(\"### Simplified summary:\")[-1].strip()\n",
        "            return text.strip()\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)\n",
        "                continue\n",
        "            print(f\"Error en OpenAI: {e}\")\n",
        "            return None\n",
        "\n",
        "# Funci\u00f3n para evaluar una API\n",
        "def evaluate_api(generate_fn, api_name: str, pairs, sample_size: int = 30):\n",
        "    \"\"\"Eval\u00faa una API comercial con las mismas m\u00e9tricas.\"\"\"\n",
        "    sample = pairs[:sample_size]\n",
        "    results = []\n",
        "    errors = 0\n",
        "    \n",
        "    print(f\"\\nEvaluando {api_name}...\")\n",
        "    for i, pair in enumerate(tqdm(sample, desc=f\"Evaluating {api_name}\")):\n",
        "        medical = pair.medical\n",
        "        reference = pair.plain\n",
        "        source = pair.medical\n",
        "        \n",
        "        generated = generate_fn(medical)\n",
        "        \n",
        "        if generated is None or len(generated.strip()) < 10:\n",
        "            errors += 1\n",
        "            continue\n",
        "        \n",
        "        try:\n",
        "            metrics = evaluator.evaluate(generated, reference, source)\n",
        "            results.append(metrics)\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluando muestra {i}: {e}\")\n",
        "            errors += 1\n",
        "            continue\n",
        "    \n",
        "    if not results:\n",
        "        print(f\" No se pudieron evaluar muestras para {api_name}\")\n",
        "        return None\n",
        "    \n",
        "    if errors > 0:\n",
        "        print(f\" {errors} errores durante la evaluaci\u00f3n de {api_name}\")\n",
        "    \n",
        "    # Resumir m\u00e9tricas\n",
        "    bert_f1 = np.mean([m[\"relevance\"][\"f1\"] for m in results])\n",
        "    fact = np.mean([m[\"factuality\"][\"score\"] for m in results])\n",
        "    fre = np.mean([m[\"readability\"][\"flesch_reading_ease\"] for m in results])\n",
        "    \n",
        "    return {\n",
        "        \"bertscore_f1\": float(bert_f1),\n",
        "        \"factuality\": float(fact),\n",
        "        \"flesch_reading_ease\": float(fre),\n",
        "        \"n_samples\": len(results),\n",
        "        \"errors\": errors,\n",
        "    }\n",
        "\n",
        "# Evaluar todas las APIs comerciales\n",
        "sample_size = min(30, len(eval_pairs))\n",
        "print(\"=\"*80)\n",
        "print(\"EVALUANDO APIs COMERCIALES\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Tama\u00f1o de muestra: {sample_size}\")\n",
        "\n",
        "ANTHROPIC_METRICS = None\n",
        "GEMINI_METRICS = None\n",
        "OPENAI_METRICS = None\n",
        "\n",
        "if client_anthropic:\n",
        "    ANTHROPIC_METRICS = evaluate_api(generate_anthropic, \"Anthropic Claude\", eval_pairs, sample_size)\n",
        "    if ANTHROPIC_METRICS:\n",
        "        print(f\"Anthropic Claude: {ANTHROPIC_METRICS}\")\n",
        "else:\n",
        "    print(\" Anthropic no disponible (API key no configurada)\")\n",
        "\n",
        "if model_gemini:\n",
        "    GEMINI_METRICS = evaluate_api(generate_gemini, \"Google Gemini\", eval_pairs, sample_size)\n",
        "    if GEMINI_METRICS:\n",
        "        print(f\"Google Gemini: {GEMINI_METRICS}\")\n",
        "else:\n",
        "    print(\" Gemini no disponible (API key no configurada)\")\n",
        "\n",
        "if client_openai:\n",
        "    OPENAI_METRICS = evaluate_api(generate_openai, \"OpenAI GPT-4o\", eval_pairs, sample_size)\n",
        "    if OPENAI_METRICS:\n",
        "        print(f\"OpenAI GPT-4o: {OPENAI_METRICS}\")\n",
        "else:\n",
        "    print(\" OpenAI no disponible (API key no configurada)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 8.1 Tabla comparativa final: Modelos locales vs APIs comerciales\n",
        "# ==============================================================\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TABLA COMPARATIVA FINAL: MODELOS LOCALES vs APIs COMERCIALES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Construir tabla de m\u00e9tricas\n",
        "metrics_table = {\n",
        "    \"BERTScore F1\": {},\n",
        "    \"AlignScore (Factualidad)\": {},\n",
        "    \"Flesch Reading Ease\": {},\n",
        "}\n",
        "\n",
        "# Agregar modelos locales (verificar que existan antes de agregar)\n",
        "if 'BASE_MODEL_METRICS' in globals():\n",
        "    metrics_table[\"BERTScore F1\"][\"Base\"] = BASE_MODEL_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"Base\"] = BASE_MODEL_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"Base\"] = BASE_MODEL_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if 'BASE_TD3_METRICS' in globals():\n",
        "    metrics_table[\"BERTScore F1\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if 'FINETUNED_MODEL_METRICS' in globals():\n",
        "    metrics_table[\"BERTScore F1\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if 'LORA_TD3_METRICS' in globals():\n",
        "    metrics_table[\"BERTScore F1\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "# Agregar APIs comerciales si est\u00e1n disponibles\n",
        "if ANTHROPIC_METRICS:\n",
        "    metrics_table[\"BERTScore F1\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if GEMINI_METRICS:\n",
        "    metrics_table[\"BERTScore F1\"][\"Google Gemini\"] = GEMINI_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"Google Gemini\"] = GEMINI_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"Google Gemini\"] = GEMINI_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if OPENAI_METRICS:\n",
        "    metrics_table[\"BERTScore F1\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"bertscore_f1\"]\n",
        "    metrics_table[\"AlignScore (Factualidad)\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"factuality\"]\n",
        "    metrics_table[\"Flesch Reading Ease\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "# Crear DataFrame y mostrar\n",
        "df_final = pd.DataFrame(metrics_table).T\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(df_final.to_string())\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Mostrar diferencias respecto al modelo base\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DIFERENCIAS RESPECTO AL MODELO BASE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for name, row in df_final.iterrows():\n",
        "    print(f\"\\n{name}:\")\n",
        "    base_value = row.get(\"Base\", None)\n",
        "    if base_value is None:\n",
        "        continue\n",
        "    for col in df_final.columns:\n",
        "        if col == \"Base\":\n",
        "            continue\n",
        "        if col in row and pd.notna(row[col]):\n",
        "            delta = row[col] - base_value\n",
        "            print(f\"  {col:25s}: {row[col]:.4f} (\u0394 {delta:+.4f})\")\n",
        "\n",
        "# Resumen de mejores modelos\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MEJORES MODELOS POR M\u00c9TRICA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for metric in df_final.index:\n",
        "    row = df_final.loc[metric]\n",
        "    # Excluir NaN y encontrar el m\u00e1ximo\n",
        "    valid_values = row.dropna()\n",
        "    if len(valid_values) > 0:\n",
        "        best_model = valid_values.idxmax()\n",
        "        best_value = valid_values.max()\n",
        "        print(f\"{metric:30s}: {best_model:25s} ({best_value:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 8.2 Gr\u00e1ficas comparativas: Modelos locales vs APIs comerciales\n",
        "# ==============================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# ==============================================================\n",
        "# SELECCIONAR MODELO LOCAL A COMPARAR\n",
        "# ==============================================================\n",
        "# Descomenta el modelo local que quieras comparar con las APIs comerciales\n",
        "# Opciones: \"Base\", \"Base + TD3\", \"LoRA\", \"LoRA + TD3\"\n",
        "MODELO_LOCAL_SELECCIONADO = \"LoRA\"  # Cambia este valor para elegir otro modelo\n",
        "\n",
        "# Preparar datos para las gr\u00e1ficas\n",
        "metrics_data = {\n",
        "    \"BERTScore F1\": {\n",
        "        # \"Base\": BASE_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        # \"Base + TD3\": BASE_TD3_METRICS[\"bertscore_f1\"],\n",
        "        # \"LoRA\": FINETUNED_MODEL_METRICS[\"bertscore_f1\"],\n",
        "        # \"LoRA + TD3\": LORA_TD3_METRICS[\"bertscore_f1\"],\n",
        "    },\n",
        "    \"AlignScore (Factualidad)\": {\n",
        "        # \"Base\": BASE_MODEL_METRICS[\"factuality\"],\n",
        "        # \"Base + TD3\": BASE_TD3_METRICS[\"factuality\"],\n",
        "        # \"LoRA\": FINETUNED_MODEL_METRICS[\"factuality\"],\n",
        "        # \"LoRA + TD3\": LORA_TD3_METRICS[\"factuality\"],\n",
        "    },\n",
        "    \"Flesch Reading Ease\": {\n",
        "        # \"Base\": BASE_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        # \"Base + TD3\": BASE_TD3_METRICS[\"flesch_reading_ease\"],\n",
        "        # \"LoRA\": FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"],\n",
        "        # \"LoRA + TD3\": LORA_TD3_METRICS[\"flesch_reading_ease\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# Agregar el modelo local seleccionado\n",
        "if MODELO_LOCAL_SELECCIONADO == \"Base\":\n",
        "    metrics_data[\"BERTScore F1\"][\"Base\"] = BASE_MODEL_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"Base\"] = BASE_MODEL_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"Base\"] = BASE_MODEL_METRICS[\"flesch_reading_ease\"]\n",
        "elif MODELO_LOCAL_SELECCIONADO == \"Base + TD3\":\n",
        "    metrics_data[\"BERTScore F1\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"Base + TD3\"] = BASE_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "elif MODELO_LOCAL_SELECCIONADO == \"LoRA\":\n",
        "    metrics_data[\"BERTScore F1\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"LoRA\"] = FINETUNED_MODEL_METRICS[\"flesch_reading_ease\"]\n",
        "elif MODELO_LOCAL_SELECCIONADO == \"LoRA + TD3\":\n",
        "    metrics_data[\"BERTScore F1\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"LoRA + TD3\"] = LORA_TD3_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "# Agregar APIs comerciales si est\u00e1n disponibles\n",
        "if ANTHROPIC_METRICS:\n",
        "    metrics_data[\"BERTScore F1\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"Anthropic Claude\"] = ANTHROPIC_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if GEMINI_METRICS:\n",
        "    metrics_data[\"BERTScore F1\"][\"Google Gemini\"] = GEMINI_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"Google Gemini\"] = GEMINI_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"Google Gemini\"] = GEMINI_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "if OPENAI_METRICS:\n",
        "    metrics_data[\"BERTScore F1\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"bertscore_f1\"]\n",
        "    metrics_data[\"AlignScore (Factualidad)\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"factuality\"]\n",
        "    metrics_data[\"Flesch Reading Ease\"][\"OpenAI GPT-4o\"] = OPENAI_METRICS[\"flesch_reading_ease\"]\n",
        "\n",
        "# Definir colores para cada modelo\n",
        "model_colors = {\n",
        "    \"Base\": \"#8B7355\",  # Beige/Cream\n",
        "    \"Base + TD3\": \"#4A90E2\",  # Blue\n",
        "    \"LoRA\": \"#D3D3D3\",  # Light Gray\n",
        "    \"LoRA + TD3\": \"#FF6B35\",  # Red/Orange gradient\n",
        "    \"Anthropic Claude\": \"#2E7D32\",  # Green\n",
        "    \"Google Gemini\": \"#4285F4\",  # Google Blue\n",
        "    \"OpenAI GPT-4o\": \"#10A37F\",  # OpenAI Green\n",
        "}\n",
        "\n",
        "# Orden de modelos para la gr\u00e1fica (solo el modelo local seleccionado + APIs comerciales)\n",
        "model_order = [MODELO_LOCAL_SELECCIONADO]\n",
        "if ANTHROPIC_METRICS:\n",
        "    model_order.append(\"Anthropic Claude\")\n",
        "if GEMINI_METRICS:\n",
        "    model_order.append(\"Google Gemini\")\n",
        "if OPENAI_METRICS:\n",
        "    model_order.append(\"OpenAI GPT-4o\")\n",
        "\n",
        "# Crear figura con subplots\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "fig.suptitle(f\"Comparaci\u00f3n de M\u00e9tricas: {MODELO_LOCAL_SELECCIONADO} vs APIs Comerciales\",\n",
        "             fontsize=16, fontweight='bold', y=1.02)\n",
        "\n",
        "# Colores para las barras\n",
        "colors = [model_colors.get(model, \"#808080\") for model in model_order]\n",
        "\n",
        "# Graficar cada m\u00e9trica\n",
        "for idx, (metric_name, metric_values) in enumerate(metrics_data.items()):\n",
        "    ax = axes[idx]\n",
        "\n",
        "    # Obtener valores en el orden correcto\n",
        "    values = [metric_values.get(model, np.nan) for model in model_order]\n",
        "\n",
        "    # Crear barras\n",
        "    bars = ax.bar(range(len(model_order)), values, color=colors, alpha=0.8, edgecolor='black', linewidth=1.2)\n",
        "\n",
        "    # Agregar valores en las barras\n",
        "    for i, (bar, val) in enumerate(zip(bars, values)):\n",
        "        if not np.isnan(val):\n",
        "            height = bar.get_height()\n",
        "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                   f'{val:.2f}',\n",
        "                   ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
        "\n",
        "    # Configurar ejes\n",
        "    ax.set_xticks(range(len(model_order)))\n",
        "    ax.set_xticklabels(model_order, rotation=45, ha='right', fontsize=10)\n",
        "    ax.set_ylabel('Score', fontsize=11, fontweight='bold')\n",
        "    ax.set_title(metric_name, fontsize=12, fontweight='bold', pad=10)\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax.set_ylim(0, max([v for v in values if not np.isnan(v)]) * 1.15 if any(not np.isnan(v) for v in values) else 100)\n",
        "\n",
        "    # Agregar l\u00ednea de referencia en el valor del modelo Base\n",
        "    if \"Base\" in metric_values and not np.isnan(metric_values[\"Base\"]):\n",
        "        ax.axhline(y=metric_values[\"Base\"], color='gray', linestyle=':', linewidth=1.5, alpha=0.7, label='Base')\n",
        "        if idx == 0:  # Solo mostrar leyenda en el primer subplot\n",
        "            ax.legend(loc='upper right', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Gr\u00e1fica combinada (todas las m\u00e9tricas en una sola)\n",
        "fig2, ax2 = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "x = np.arange(len(model_order))\n",
        "width = 0.25  # Ancho de las barras\n",
        "\n",
        "# Posiciones de las barras para cada m\u00e9trica\n",
        "x1 = x - width\n",
        "x2 = x\n",
        "x3 = x + width\n",
        "\n",
        "# Valores para cada m\u00e9trica\n",
        "y1 = [metrics_data[\"BERTScore F1\"].get(model, np.nan) for model in model_order]\n",
        "y2 = [metrics_data[\"AlignScore (Factualidad)\"].get(model, np.nan) for model in model_order]\n",
        "y3 = [metrics_data[\"Flesch Reading Ease\"].get(model, np.nan) for model in model_order]\n",
        "\n",
        "# Normalizar Flesch Reading Ease para mejor visualizaci\u00f3n (dividir por 10)\n",
        "y3_norm = [v / 10 if not np.isnan(v) else np.nan for v in y3]\n",
        "\n",
        "# Crear barras\n",
        "bars1 = ax2.bar(x1, y1, width, label='BERTScore F1', color='#FF6B35', alpha=0.8, edgecolor='black', linewidth=1)\n",
        "bars2 = ax2.bar(x2, y2, width, label='AlignScore (Factualidad)', color='#4A90E2', alpha=0.8, edgecolor='black', linewidth=1)\n",
        "bars3 = ax2.bar(x3, y3_norm, width, label='Flesch Reading Ease (\u00f710)', color='#2E7D32', alpha=0.8, edgecolor='black', linewidth=1)\n",
        "\n",
        "# Agregar valores en las barras\n",
        "for bars, values, norm_factor in [(bars1, y1, 1), (bars2, y2, 1), (bars3, y3, 10)]:\n",
        "    for bar, val in zip(bars, values):\n",
        "        if not np.isnan(val):\n",
        "            height = bar.get_height()\n",
        "            ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{val:.2f}',\n",
        "                    ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "# Configurar ejes\n",
        "ax2.set_xlabel('Modelos', fontsize=12, fontweight='bold')\n",
        "ax2.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
        "ax2.set_title(f'Comparaci\u00f3n Completa de M\u00e9tricas: {MODELO_LOCAL_SELECCIONADO} vs APIs Comerciales',\n",
        "              fontsize=14, fontweight='bold', pad=15)\n",
        "ax2.set_xticks(x)\n",
        "ax2.set_xticklabels(model_order, rotation=45, ha='right', fontsize=10)\n",
        "ax2.legend(loc='upper left', fontsize=10)\n",
        "ax2.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GR\u00c1FICAS GENERADAS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\u2713 Gr\u00e1fica 1: Comparaci\u00f3n por m\u00e9trica (3 subplots)\")\n",
        "print(\"\u2713 Gr\u00e1fica 2: Comparaci\u00f3n combinada (todas las m\u00e9tricas)\")\n",
        "print(\"\\nNota: Flesch Reading Ease est\u00e1 normalizado (\u00f710) en la gr\u00e1fica combinada para mejor visualizaci\u00f3n.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# 9. DESCARGAR MODELOS DESDE COLAB\n",
        "# ==============================================================\n",
        "# Esta celda permite descargar los modelos entrenados:\n",
        "# 1. Modelo LoRA (finetuning)\n",
        "# 2. Agente TD3 Base\n",
        "# 3. Agente TD3 LoRA\n",
        "\n",
        "import zipfile\n",
        "import shutil\n",
        "from google.colab import files, drive\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DESCARGAR MODELOS DESDE COLAB\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Definir rutas (deben coincidir con las usadas en el notebook)\n",
        "ADAPTER_DIR = \"/content/deepseek-r1-distill-qwen-1.5b-pls\"\n",
        "TD3_BASE_DIR = \"/content/td3_base_agent\"\n",
        "TD3_LORA_DIR = \"/content/td3_lora_agent\"\n",
        "\n",
        "# Verificar qu\u00e9 modelos existen\n",
        "modelos_disponibles = []\n",
        "\n",
        "if os.path.exists(ADAPTER_DIR):\n",
        "    modelos_disponibles.append((\"LoRA (Finetuning)\", ADAPTER_DIR))\n",
        "    print(f\"\u2713 Modelo LoRA encontrado en: {ADAPTER_DIR}\")\n",
        "else:\n",
        "    print(f\"\u2717 Modelo LoRA NO encontrado en: {ADAPTER_DIR}\")\n",
        "\n",
        "if os.path.exists(TD3_BASE_DIR):\n",
        "    modelos_disponibles.append((\"TD3 Base\", TD3_BASE_DIR))\n",
        "    print(f\"\u2713 Agente TD3 Base encontrado en: {TD3_BASE_DIR}\")\n",
        "else:\n",
        "    print(f\"\u2717 Agente TD3 Base NO encontrado en: {TD3_BASE_DIR}\")\n",
        "\n",
        "if os.path.exists(TD3_LORA_DIR):\n",
        "    modelos_disponibles.append((\"TD3 LoRA\", TD3_LORA_DIR))\n",
        "    print(f\"\u2713 Agente TD3 LoRA encontrado en: {TD3_LORA_DIR}\")\n",
        "else:\n",
        "    print(f\"\u2717 Agente TD3 LoRA NO encontrado en: {TD3_LORA_DIR}\")\n",
        "\n",
        "if not modelos_disponibles:\n",
        "    print(\"\\n No se encontraron modelos para descargar.\")\n",
        "    print(\"Aseg\u00farate de haber ejecutado las celdas de entrenamiento primero.\")\n",
        "else:\n",
        "    print(f\"\\n Se encontraron {len(modelos_disponibles)} modelo(s) para descargar.\")\n",
        "    \n",
        "    # ==============================================================\n",
        "    # OPCI\u00d3N 1: Descargar como ZIP (recomendado para archivos grandes)\n",
        "    # ==============================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPCI\u00d3N 1: DESCARGAR COMO ARCHIVO ZIP\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    ZIP_OUTPUT = \"/content/modelos_deepseek-r1-distill-qwen-1.5b.zip\"\n",
        "    \n",
        "    # Crear ZIP con todos los modelos\n",
        "    with zipfile.ZipFile(ZIP_OUTPUT, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for nombre, ruta in modelos_disponibles:\n",
        "            if os.path.exists(ruta):\n",
        "                print(f\"Agregando {nombre}...\")\n",
        "                # Agregar todo el directorio al ZIP\n",
        "                for root, dirs, filenames in os.walk(ruta):\n",
        "                    for filename in filenames:\n",
        "                        file_path = os.path.join(root, filename)\n",
        "                        # Mantener estructura de directorios en el ZIP\n",
        "                        arcname = os.path.relpath(file_path, os.path.dirname(ruta))\n",
        "                        arcname = os.path.join(os.path.basename(ruta), arcname)\n",
        "                        zipf.write(file_path, arcname)\n",
        "    \n",
        "    # Obtener tama\u00f1o del archivo\n",
        "    zip_size_mb = os.path.getsize(ZIP_OUTPUT) / (1024 * 1024)\n",
        "    print(f\"\\n\u2713 ZIP creado: {ZIP_OUTPUT}\")\n",
        "    print(f\"  Tama\u00f1o: {zip_size_mb:.2f} MB\")\n",
        "    \n",
        "    # Descargar el ZIP\n",
        "    print(\"\\n\u2b07 Descargando ZIP...\")\n",
        "    files.download(ZIP_OUTPUT)\n",
        "    print(\"\u2713 Descarga iniciada. El archivo se descargar\u00e1 autom\u00e1ticamente.\")\n",
        "    \n",
        "    # ==============================================================\n",
        "    # OPCI\u00d3N 2: Subir a Google Drive (para archivos muy grandes)\n",
        "    # ==============================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPCI\u00d3N 2: SUBIR A GOOGLE DRIVE (Opcional)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Si el archivo ZIP es muy grande, puedes subirlo a Google Drive.\")\n",
        "    print(\"Descomenta las siguientes l\u00edneas para usar esta opci\u00f3n:\\n\")\n",
        "    \n",
        "    print(\"# Montar Google Drive\")\n",
        "    print(\"# drive.mount('/content/drive')\")\n",
        "    print(\"#\")\n",
        "    print(\"# Copiar ZIP a Drive\")\n",
        "    print(\"# drive_dest = '/content/drive/MyDrive/modelos_deepseek-r1-distill-qwen-1.5b.zip'\")\n",
        "    print(\"# shutil.copy(ZIP_OUTPUT, drive_dest)\")\n",
        "    print(\"# print(f'\u2713 Archivo copiado a: {drive_dest}')\")\n",
        "    \n",
        "    # ==============================================================\n",
        "    # OPCI\u00d3N 3: Descargar modelos individuales (para archivos peque\u00f1os)\n",
        "    # ==============================================================\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"OPCI\u00d3N 3: DESCARGAR MODELOS INDIVIDUALES (Solo para archivos peque\u00f1os)\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"Nota: Esta opci\u00f3n puede fallar si los archivos son muy grandes.\")\n",
        "    print(\"Recomendamos usar la OPCI\u00d3N 1 (ZIP) en su lugar.\\n\")\n",
        "    \n",
        "    # Descomentar para descargar modelos individuales:\n",
        "    # for nombre, ruta in modelos_disponibles:\n",
        "    #     if os.path.exists(ruta):\n",
        "    #         print(f\"Descargando {nombre}...\")\n",
        "    #         # Crear ZIP individual\n",
        "    #         zip_individual = f\"/content/{os.path.basename(ruta)}.zip\"\n",
        "    #         shutil.make_archive(zip_individual.replace('.zip', ''), 'zip', ruta)\n",
        "    #         files.download(zip_individual)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESUMEN\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\u2713 ZIP creado con {len(modelos_disponibles)} modelo(s)\")\n",
        "    print(f\"\u2713 Ubicaci\u00f3n: {ZIP_OUTPUT}\")\n",
        "    print(f\"\u2713 Tama\u00f1o: {zip_size_mb:.2f} MB\")\n",
        "    print(\"\\n Para usar los modelos en otro lugar:\")\n",
        "    print(\"   1. Descarga el archivo ZIP\")\n",
        "    print(\"   2. Extrae el contenido\")\n",
        "    print(\"   3. Carga los modelos usando:\")\n",
        "    print(\"      - LoRA: model_lora = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\")\n",
        "    print(\"      - TD3: td3_agent = TD3.load(TD3_DIR, env=env)\")\n",
        "    print(\"\\n\" + \"=\"*80)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}